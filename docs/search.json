[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Psychologists",
    "section": "",
    "text": "This book is an introduction to Data Science for Psychologists (and others in the social sciences). Primarily using the R programming language, you will learn a general workflow that helps you to process, plot, analyse, and present data to your audience. It is expected that readers are familiar with introductory statistics typically taught at the Undergraduate level in Psychology programmes across the UK.\nThroughout, concepts will be taught using examples from real and simulated data from studies in Psychology. R will be taught using a tidyverse-first approach, using a suite of packages that are designed to make programming quick, easy, and highly readable."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.” Journal of Open Source\nSoftware 4 (43): 1686."
  },
  {
    "objectID": "01_getting-started.html",
    "href": "01_getting-started.html",
    "title": "1  Getting Started",
    "section": "",
    "text": "2 References"
  },
  {
    "objectID": "10_understanding-null-results.html",
    "href": "10_understanding-null-results.html",
    "title": "10  Understanding Null Results",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "02_creating-graphs.html",
    "href": "02_creating-graphs.html",
    "title": "2  Creating Graphs",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "03_transforming-data.html",
    "href": "03_transforming-data.html",
    "title": "3  Transforming Data",
    "section": "",
    "text": "4 Reading Data\nPrior to this chapter, we’ve used inbuilt data sets or created data within R to show off the functionality of R. However, in a real research context you’ll often be required to work with data from an external file. Thankfully, several packages allow us to read in and write to a wide range of files.\nOne of the glaring issues with this data set is that we have one column, enjoymentFOLLOW which encodes two variables, whether people enjoyed the study and if they’d be contactable at a follow up, and time that encodes the start and end times for participants in the study. We can use the separate() function to split a column into two or more columns, depending on how many separate sources of data the function detects or is instructed to detect.\nseparate() looks for separators between data points, e.g. _ or -, and splits columns there. So, if we have many separators within our data, but we only want to split the columns at one of them, we need to be explicit in where to split the data.\nLet’s fix the messy data, starting with splitting columns that contain multiple variables. First, we’ll start with enjoymentFOLLOW which has only one separator, _ between the values in the column. Using the function without specifying the separator works fine in this instance. We just need to specify the names of the new columns.\nWe now have separate columns for each variable. Next, we need to fix time. What happens if we don’t specify a separator?\nWe have start and end times, but these don’t look correct. We also have a warning saying that due to our names separate() expected two rows but created (and dropped) 25! That’s because datetimes are made up of many separators, e.g. “2018-03-22 23:06:11”. In this instance, the start and end times are separated by an underscore, e.g. “2018-03-22 23:06:11_2018-03-23 00:25:51”. We can use this to tell separate() to only split the data at this separator.\nNow we have the expected output.\nTo put all of this together, we can chain multiple separation calls together with the pipe. Notice that unless we assign the result back to the variable we haven’t actually changed our data set. So, we’ll do this now.\nOften your data set has lots of columns you won’t use. This can make working with the data more difficult and takes up memory on our computer which can slow things down. We can choose which columns to keep by listing them by name or by position within the select() function.\nIf we want to create or change a column, we have two options: mutate() and transmute():\nThroughout this book we’ll mainly use mutate() as it’s rare to only want to work with columns you’ve just created or changed. mutate() can consist of a simple operation on one column, using many columns to create another, or doing conditional operations within a column.\nHere we create a new column by adding together values from columns 1 and y.\nLet’s look at working out the birth years of the participants in the data set. The study concluded in 2022 (despite what the start_date and end_date columns tell us), so we can take the year 2022 and subtract participant ages to find their birth year.\nWe can also create columns by combining operations across multiple existing columns. Here, we create a total score for the De Jong Gierveld scale by adding each of the scores on the individual items.\nOften, however, we might want to update columns by transforming values within them.\nIn our data set we have the problem that date times are stored as character vectors rather than date times. This means that we can’t perform any mathematical operations on the values in these columns.\nIf we try to get the total time participants took in the study like so, it won’t work.\nThis would also be the case if we wanted to perform a mathematical operation using a numeric and non-numeric column. To fix this, we change the data type.\nLet’s update the start_time and end_time columns, converting them to date time data types. We’ll use the lubridate package for this which has the function ymd_hms() which parses character vectors in the format of year-month-day_hours-minutes-seconds to a proper date time data type.\nNotice that we can also create a new column, total_time by subtracting start_time from end_time within the same mutate() call that we use to convert their data types. This column goes to the end of our data set.\nWe can subset our data sets by filtering it out to only contain rows where specific conditions are met using the filter() function. In the example below we filter the data set down to rows where x is greater than 1.\nIn our data set, we might subset it to only those who completed the study. Due to inconsistent coding, we have two values for having completed the study in the progress column, FINISH and END. We want to filter the data to people who have progress as FINISH OR END (they can’t be both), so we use the | OR operator.\nIf we have many conditions we want to meet, many OR (|) statements are verbose and can be unwieldy. We can instead use %in% to define the values in progess that we want to keep. This is TRUE if an observation is in a value that you provide.\nWe can combine these with criteria on other columns to do additional subsetting in one filter() call.\nWhat if we have conditions we’d like to exclude from our data set rather than keep? We can do the inverse of these operations using !. Here we ask for those whose progress ISN’T FINISH or END but who are over 50.\nOne thing to bear in mind If you ask R if a value is equal to an NA (an unknown value) it is very literal in that it tells us it can’t know. So, this doesn’t work:\nInstead, we have to use is.na():\nIf we want values that AREN’T NAs, then we can combine is.na() and the NOT (!) operator.\nWhile arguable one of the less important aspects of data transformation, you will sometimes want or need to arrange your data in a specific order. To do this, we can arrange rows sorted by value in a column using arrange().\nThis defaults to an ascending order, but we can use desc() to enforce a descending order. Compare the default and desc() versions below.\nOne very important aspect of data analysis is presenting descriptive statistics, or summaries of your data. Often this will be counts, measures of central tendency (such as the mean, median, or mode) and measures of dispersion (such as standard deviation, interquartile range, etc.). To produce these summaries we can use the summarise() function.\nHere, we create a new table with a new column which is the result of a mathematical operation on every value in the original table of data.\nLet’s look at getting an idea of some very basic descriptive statistics for the demographics in our data set. We’ll create a new table which contains the mean of ages in our data set.\nHowever, by default this results in an error. That’s because mean() can’t compute a mean of numeric values and something that doesn’t exist NA. Instead, we can tell mean() to explicitly ignore NAs in the computation. We can do this as follows.\nWe’ve seen already how we can chain many functions together using the pipe. We can combine all functions we’ve covered to do all our data transformation. We can even pipe the result of these chains into ggplot() calls. In the Section 13.1 we’ll do this, fixing all of the problems with our data before we produce any summaries or plots using the functions you’ve learned about in this chapter.\nFinally, once you’re done cleaning your data and making summaries you can save it using the write_*() family of functions. There are the same number of write_*() functions corresponding to the read_*() functions listed above for each package. I suggest saving to a .csv file with write_csv() as any program can open .csv files. As mentioned earlier, it’s a good idea to keep your raw and cleaned data apart from one another, even in separate sub-folders. Here we save our data to the “cleaned_data” sub-folder. We should probably do this on actually cleaned data, but this is just an example. You will do this properly in Section 13.1."
  },
  {
    "objectID": "04_tidying-and-merging-data.html",
    "href": "04_tidying-and-merging-data.html",
    "title": "4  Tidying and Merging Data",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "05_core-statistics.html",
    "href": "05_core-statistics.html",
    "title": "5  Core Statistics",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "06_open-science-practices.html",
    "href": "06_open-science-practices.html",
    "title": "6  Open Science Practices",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "07_multilevel-modelling.html",
    "href": "07_multilevel-modelling.html",
    "title": "7  Multilevel Modelling",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "08_data-simulation-and-power.html",
    "href": "08_data-simulation-and-power.html",
    "title": "8  Data Simulation and Power",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "09_bayesian-estimation-and-model-comparison.html",
    "href": "09_bayesian-estimation-and-model-comparison.html",
    "title": "9  Bayesian Estimation and Model Comparison",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "99_installing-r.html",
    "href": "99_installing-r.html",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "",
    "text": "B Installing R\nTo install R, go to the Comprehensive R Archive Network (CRAN). In the heading Download and Install R click the download link for your operating system.\nSelect the correct distribution for your operating system."
  },
  {
    "objectID": "99_installing-r.html#downloading-r-for-mac",
    "href": "99_installing-r.html#downloading-r-for-mac",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "B.1 Downloading R for Mac",
    "text": "B.1 Downloading R for Mac\nFor Mac users, click Download R for macOS. Always download the latest release of R. At the time of writing this is R version 4.2.2. Click the link to the left of the release notes to download R to your system.\n\n\n\nDownloading R for Mac"
  },
  {
    "objectID": "99_installing-r.html#downloading-r-for-windows",
    "href": "99_installing-r.html#downloading-r-for-windows",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "B.2 Downloading R for Windows",
    "text": "B.2 Downloading R for Windows\nFor Windows users, click Install R for the first time.\n\n\n\nDownloading R for Windows\n\n\nThis will take you to another web page. At the top is your download link in the format Download R-[version number] for windows. At the time of writing this is Download R-4.2.2 for Windows.\nIf you have a 64 bit system, install the 64 bit version of R as you'll be able to take advantage of having more than 4Gb RAM. (This is useful in instances where you're working with very large data sets.)"
  },
  {
    "objectID": "99_installing-r.html#downloading-r-for-linux",
    "href": "99_installing-r.html#downloading-r-for-linux",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "B.3 Downloading R for Linux",
    "text": "B.3 Downloading R for Linux\nFor Linux users, detailed instructions are given for different distributions. Follow the relevant links and details there to download and install R."
  },
  {
    "objectID": "99_installing-r.html#installing-r",
    "href": "99_installing-r.html#installing-r",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "A.1 Installing R",
    "text": "A.1 Installing R\nTo install R, go to the Comprehensive R Archive Network (CRAN). In the heading Download and Install R click the download link for your operating system.\nSelect the correct distribution for your operating system.\n\n\n\nDownloading R from CRAN\n\n\n\nA.1.1 Downloading R for Mac\nFor Mac users, click Download R for macOS. Always download the latest release of R. At the time of writing this is R version 4.2.2. Click the link to the left of the release notes to download R to your system.\n\n\n\nDownloading R for Mac\n\n\n\n\nA.1.2 Downloading R for Windows\nFor Windows users, click Install R for the first time.\n\n\n\nDownloading R for Windows\n\n\nThis will take you to another web page. At the top is your download link in the format Download R-[version number] for windows. At the time of writing this is Download R-4.2.2 for Windows.\nIf you have a 64 bit system, install the 64 bit version of R as you'll be able to take advantage of having more than 4Gb RAM. (This is useful in instances where you're working with very large data sets.)\n\n\nA.1.3 Downloading R for Linux\nFor Linux users, detailed instructions are given for different distributions. Follow the relevant links and details there to download and install R."
  },
  {
    "objectID": "99_installing-r.html#installing-rstudio",
    "href": "99_installing-r.html#installing-rstudio",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "A.2 Installing RStudio",
    "text": "A.2 Installing RStudio"
  },
  {
    "objectID": "99_installing-r.html#installing-quarto",
    "href": "99_installing-r.html#installing-quarto",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "A.3 Installing Quarto",
    "text": "A.3 Installing Quarto"
  },
  {
    "objectID": "99a_installing-r.html#downloading-r-for-mac",
    "href": "99a_installing-r.html#downloading-r-for-mac",
    "title": "Appendix A — Installing R",
    "section": "A.1 Downloading R for Mac",
    "text": "A.1 Downloading R for Mac\nFor Mac users, click Download R for macOS. Always download the latest release of R. At the time of writing this is R version 4.2.2. Click the link to the left of the release notes to download R to your system.\n\n\n\nDownloading R for Mac\n\n\nIf you have a Mac with an M1 chip, ensure that you download and install the arm64 build. Otherwise, for Macs with an Intel chip, download and install the Intel 64-bit build."
  },
  {
    "objectID": "99a_installing-r.html#downloading-r-for-windows",
    "href": "99a_installing-r.html#downloading-r-for-windows",
    "title": "Appendix A — Installing R",
    "section": "A.2 Downloading R for Windows",
    "text": "A.2 Downloading R for Windows\nFor Windows users, click Install R for the first time.\n\n\n\nDownloading R for Windows\n\n\nThis will take you to another web page. At the top is your download link in the format Download R-[version number] for windows. At the time of writing this is Download R-4.2.2 for Windows.\nIf you have a 64 bit system, install the 64 bit version of R as you’ll be able to take advantage of having more than 4Gb RAM. (This is useful in instances where you’re working with very large data sets.)"
  },
  {
    "objectID": "99a_installing-r.html#downloading-r-for-linux",
    "href": "99a_installing-r.html#downloading-r-for-linux",
    "title": "Appendix A — Installing R",
    "section": "A.3 Downloading R for Linux",
    "text": "A.3 Downloading R for Linux\nFor Linux users, detailed instructions are given for different distributions. Follow the relevant links and details there to download and install R."
  },
  {
    "objectID": "99a_installing-r.html#installing-rstudio",
    "href": "99a_installing-r.html#installing-rstudio",
    "title": "Appendix A — Installing R",
    "section": "A.4 Installing RStudio",
    "text": "A.4 Installing RStudio"
  },
  {
    "objectID": "99a_installing-r.html#installing-quarto",
    "href": "99a_installing-r.html#installing-quarto",
    "title": "Appendix A — Installing R",
    "section": "A.5 Installing Quarto",
    "text": "A.5 Installing Quarto"
  },
  {
    "objectID": "99d_installing-bayesian-software.html#mac",
    "href": "99d_installing-bayesian-software.html#mac",
    "title": "Appendix D — Installing Bayesian Software",
    "section": "D.1 Mac",
    "text": "D.1 Mac\n\nD.1.1 Configure the C++ Toolchain\nIf you have previously used R on your Mac, you may have the files ~/.R/Makevars and/or ~/.Renviron. If you have any important settings in here that you’ve defined personally, make a backup of these files in another location. Then, delete the originals. If you can’t find these files on your system, go to Finder, click on Home (i.e. the icon with the house), and type Shift + CMD + . to show the hidden files on your system. After completing the installation instructions you can go back to the new Makevars file in this location and add back in any settings you previously had.\nThere are two steps to configuring the C++ toolchain on Mac:\n\nInstalling the Xcode Command Line Tools\nInstalling gfortran.\n\nFirst, install the Xcode Command Line Tools by opening the terminal (use the spotlight search and type Terminal). In the terminal type the following and then press Enter:\nxcode-select --install.\nInstalling the Xcode Command Line Tools may take a while. Once done install gfortran. The version you install will differ whether you have a Mac with Apple or Intel chips.\nInstall the latest version of gfortran for your operating system at https://github.com/fxcoudert/gfortran-for-macOS/releases.\nCheck your macOS by clicking the Apple logo in the dock and selecting About this Mac. On the releases page, select the appropriate release for your operating system. Please install the latest version of gfortran for your system. Specific instructions for Apple Silicon and Intel Silicon Macs are provided below.\n\nD.1.1.1 Macs with Apple Silicon (i.e. M1 or M2 chips)\nAt the time of writing for up to date Macs with Apple Silicon this is gfortran 12.2 for Ventura (macOS 13). Click gfortran-ARM-12.2-Ventura.dmg to download the software and install this on your system.\n\n\nD.1.1.2 Macs with Intel Silicon\nAt the time of writing for up to date Macs with Apple Silicon this is gfortran 12.1 for Monterey (macOS 12). Click gfortran-ARM-12.1-Monterey.dmg to download the software and install this on your system.\n\n\n\nD.1.2 Installing RStan\nIn case you previously tried to install RStan and it didn’t work, copy and paste the following code into the RStudio console and press Enter. This will clean up R to remove any failed installations.\n\nremove.packages(\"rstan\")\nif (file.exists(\".RData\")) file.remove(\".RData\")\n\nRestart R by clicking Session, Restart R.\nCopy and paste the following code into the console and press Enter.\n\ninstall.packages(\"rstan\")\n\nFinally, we’ll check everything works by again copying and pasting the following code into the console and pressing Enter.\n\nexample(stan_model, package = \"rstan\", run.dontrun = TRUE)\n\nThis might take some time to run, but if you see a big wall of text and it ends by sampling a model then your install works.\n\n\n\nA working installation of RStan\n\n\nFinally, install brms by typing the following into the RStudio console:\n\ninstall.packages(\"brms\")\n\nYou’re now ready to work with Bayesian models in R!"
  },
  {
    "objectID": "99d_installing-bayesian-software.html#windows",
    "href": "99d_installing-bayesian-software.html#windows",
    "title": "Appendix D — Installing Bayesian Software",
    "section": "D.2 Windows",
    "text": "D.2 Windows\n\nD.2.1 Configure the C++ Toolchain\nFollow the relevant instructions for your R version here: https://github.com/stan-dev/rstan/wiki/Configuring-C—Toolchain-for-Windows. It is strongly advised that you use the latest version of R for this. At the time of writing this is R version 4.2. This necessitates installing RTools42. To do this, follow this link: https://cran.r-project.org/bin/windows/Rtools/rtools42/rtools.html and download the Rtools42 installer. Click the executable and follow the instructions to install RTools.\nAs the current version of RStan on CRAN isn’t compatible with R 4.2, install the preview versions of StanHeaders and rstan by copying and pasting the following code into your RStudio console and pressing Enter:\n\ninstall.packages(\"StanHeaders\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\ninstall.packages(\"rstan\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\n\nEnsure that your installation works by running the following in the RStudio console:\n\nexample(stan_model, package = \"rstan\", run.dontrun = TRUE)\n\nIf this runs and samples (see the example above in the Mac instructions) then your RStan installation works.\nFinally, install brms by typing the following into the RStudio console:\n\ninstall.packages(\"brms\")\n\nYou’re now ready to work with Bayesian models in R!"
  },
  {
    "objectID": "99d_installing-bayesian-software.html#linux",
    "href": "99d_installing-bayesian-software.html#linux",
    "title": "Appendix D — Installing Bayesian Software",
    "section": "D.3 Linux",
    "text": "D.3 Linux\n\nD.3.1 Configure the C++ Toolchain\nFollow the instructions at https://github.com/stan-dev/rstan/wiki/Configuring-C-Toolchain-for-Linux to install a pre-built RStan binary.\nFinally, install brms by typing the following into the RStudio console:\n\ninstall.packages(\"brms\")\n\nYou’re now ready to work with Bayesian models in R!"
  },
  {
    "objectID": "99b_installing-rstudio.html",
    "href": "99b_installing-rstudio.html",
    "title": "Appendix B — Installing RStudio",
    "section": "",
    "text": "A popular way to work with R is to use an Integrated Development Environment (IDE). This IDE makes it easier to work with R when integrating version control, producing codebooks, and creating project files.\nThe most popular IDE for R is RStudio by Posit, which you can download from the Posit website. We will use this IDE in this course.\nClick Download RStudio in the top right of the screen. On the new page ensure you are on the tab for RStudio Desktop and click the Download RStudio button.\n\n\n\nDownloading RStudio\n\n\nYou should have already downloaded and installed R at this point. If you haven’t, follow the instructions on the Posit website or return to Appendix A. Next, download and install RStudio using the download link on the Posit website. Open the executable and follow the instructions to install R on your system."
  },
  {
    "objectID": "99c_installing-quarto.html",
    "href": "99c_installing-quarto.html",
    "title": "Appendix C — Installing Quarto",
    "section": "",
    "text": "Quarto is a new open-source program for publishing technical documents. This allows you to create articles, presentations, websites, books, and various other outputs. Crucially, Quarto allows you to dynamically create content within these outputs using R, Python, Julia, or Observable. (In fact, this book was written in Quarto!)\nWe will use Quarto to create documents and to complete exercises for this course. On the Quarto web page, click Get Started.\n\n\n\nA summary of Quarto’s capabilities\n\n\nTo download Quarto, click Get Started. This will take you to the download page. Click the button labelled Download Quarto CLI. Next, run the executable and install this on your system. You should now be able to create and view Quarto documents in RStudio."
  },
  {
    "objectID": "99e_installing-r-packages.html",
    "href": "99e_installing-r-packages.html",
    "title": "Appendix E — Installing R Packages",
    "section": "",
    "text": "For this course, we will use various packages which can be downloaded directly from CRAN or via GitHub within R. Packages are just functions that authors have created and shared with other R users to make certain processes easier. These can range from rather hard core data processing, plotting, and analysis functions to a set of beautiful palettes to make your plots more attractive.\nPlease copy and paste the following code into your RStudio Console to install these packages in R. In each session we will load several of these packages up to make working with our data easier.\n\ninstall.packages(\"tidyverse\") # various data wrangling and plotting packages bundled\ninstall.packages(\"here\") # working with file paths\ninstall.packages(\"easystats\") # various stats packages bundled\ninstall.packages(\"emmeans\") # calculating marginal means and pairwise tests\ninstall.packages(\"lme4\") # mixed effects modelling\ninstall.packages(\"afex\") # ANOVA and improvements on lme4\ninstall.packages(\"brms\") # bayesian regression models using Stan\ninstall.packages(\"tidybayes\") # working with draws from bayesian models\ninstall.packages(\"ggdist\") # visualising distributions and uncertainty"
  },
  {
    "objectID": "01_getting-started.html#understanding-rstudio",
    "href": "01_getting-started.html#understanding-rstudio",
    "title": "1  Getting Started",
    "section": "1.1 Understanding RStudio",
    "text": "1.1 Understanding RStudio\nNow that you have R installed, you could jump straight into opening the R Graphical User Interface (GUI). But, as you’ll see in Figure 1.1, Funny-Looking Kid isn’t just the name of this R distribution, but perfectly captures the look of this interface.\n\n\n\nFigure 1.1: The R GUI\n\n\nYou can still get a lot done in the R GUI, but there’s a lot of quality of life improvements we can get from RStudio, including access to syntax highlighting, code completion, a graphical git interface, RStudio Projects, templates for Quarto documents, and a rich markdown editor. None of these things should mean anything to you at this point, but you’ll see how they can be very helpful later. For this reason, we’ll start with RStudio from the beginning.\nThe RStudio pane can be broken down into a few different sections, as shown in Figure 1.2.\n\n\n\nFigure 1.2: The RStudio GUI\n\n\n\nThe editor: Type, edit, and save your R code in R files or Quarto documents.\nThe console: Execute your R code here by typing or copying your code here and pressing Enter. You can also highlight sections of code in the editor and press Shift + Enter to execute that code in the console.\nThe environment and history: View objects stored in memory for this working session (e.g. values, objects, and user-defined functions etc.). You can also see a history of your commands in the History tab and use the Git interface to save records of your code using the Git version control system.\nThe viewer: view any files in your working directory, see your last plot from this session, view installed packages on your machine (you can also load them here), view the help documentation for any R commands, and view plots and documents you’ve created in the viewer.\n\nYou should always aim to write your code in the editor because you can save it, edit it, and reuse it later. Only write your code in the console if you’re happy for it to be lost as soon as you type it out.\nBefore we get started writing code in RStudio, please take the time to change some of the defaults in RStudio.\nRStudio defaults to saving your workspace to an .RData file when you exit RStudio and to restoring your workspace once you reopen this. This means anything you’ve created will be restored when you start it back up. This is a bad idea because it impedes reproducibility: Imagine you mess around in the console and create or edit an object that changes the results of your analyses. This change will still be there when you restart RStudio, but you’d have no record of it. Instead, it’s better to ensure your code works from scratch in case you move it to a new computer or share it with others.\nRemove these defaults by going to Tools, Global Options and deselecting Restore .RData into workspace at startup and from the drop-down menu on Save workspace to .RData on exit to Never."
  },
  {
    "objectID": "01_getting-started.html#packages",
    "href": "01_getting-started.html#packages",
    "title": "1  Getting Started",
    "section": "1.4 Packages",
    "text": "1.4 Packages\nWhile you can get a lot done in R out of the box, many developers have created packages that add additional functionality to R (e.g. new analytical techniques) or make working with R more convenient.\nOne of the most successful packages is the tidyverse Wickham et al. (2019) suite of packages which bundles together several packages containing functions that make working with your data much easier and make your code more readable. We will primarily focus on using functions from the tidyverse in this course.\nTo install a package on your computer, simply type install.packages() in your console with the package name in quotes within this function call. Press Enter, and your package will be installed directly in R. You only need to do this once per computer.\nTry installing the tidyverse on your computer now.\n\ninstall.packages(\"tidyverse\")\n\nEvery time you open RStudio, be sure to load up the packages you need for your code. Do this now with the tidyverse. Here, you’re asking R to load the package library.\n\nlibrary(\"tidyverse\")"
  },
  {
    "objectID": "01_getting-started.html#file-systems-and-projects",
    "href": "01_getting-started.html#file-systems-and-projects",
    "title": "1  Getting Started",
    "section": "1.4 File Systems and Projects",
    "text": "1.4 File Systems and Projects"
  },
  {
    "objectID": "01_getting-started.html#objects-and-functions",
    "href": "01_getting-started.html#objects-and-functions",
    "title": "1  Getting Started",
    "section": "1.5 Objects and Functions",
    "text": "1.5 Objects and Functions\nYou can work directly with data in R, for example using it like a calculator.\n\n4 + 2\n\n[1] 6\n\n\nR will be patient and wait for you to finish an expression before executing code. So, if your line of text ends with a mathematical operator, R waits to receive the next number. This can be useful for splitting long expressions across multiple lines:\n\n1 + 2 + 3 + 4 + 5 + 6 + 7 +\n  8 + 9\n\n[1] 45\n\n\nTry typing this out in the console. You’ll notice the > that is normally there when you type changed to a +, indicating that R is waiting for more code.\nR also parses text if included in quotes.\n\n\"Hello World!\"\n\n[1] \"Hello World!\"\n\n\nThe same rule applies about finishing expressions here; if you don’t close your quote, then R will wait for you to do so. This means you can spread your text over several lines (by pressing Enter) and R will parse that as one expression. Note with our output we get \\n which indicates that a new line follows the comma.\n\n\"Hello world, isn't this book taking longer to write than Glenn expected\ndespite Glenn having alredy done this before and \nswearing he learned to not ovedo it?\"\n\n[1] \"Hello world, isn't this book taking longer to write than Glenn expected\\ndespite Glenn having alredy done this before and \\nswearing he learned to not ovedo it?\"\n\n\nBut, repeatedly typing data out or referring back to this data is going to be very tedious if we can’t use a shorthand to refer to it. This is where objects come in. Objects are used to store information in R. Crucially, we can perform operations on them by simply using the name of the object.\nWe assign values to a object using the assignment operator <-. Using this, we give the object its values.\n\nsummed_numbers <- 4 + 2\n\nBy default, R will not return the result of this operation from summed_numbers unless you ask it to do so. To get the result, simply type the name of the object.\n\nsummed_numbers\n\n[1] 6\n\n\nWe can perform operations on these objects after they’ve been created.\n\nsummed_numbers * 5\n\n[1] 30\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can’t start objects with a number, you can’t use special characters (e.g. %!*), and you can’t include spaces in your object name.\nAlso, capitalisation matters, so Summed_numbers is not summed_numbers.\n\n\nR has many mathematical operations built in.\n\nMathematical Operations in R\n\n\n\n\n\n\nOperation\nCode Example\n\n\n\n\nAdd\nx + y\n\n\nSubtract\nx - y\n\n\nMultiply\nx * y\n\n\nDivide\nx / y\n\n\nExponentiate\nx ^ y\n\n\nModulus\nx %% y (e.g. 5 mod 2 = 1, the remainder of how many times 2 goes into 5.)\n\n\nInteger Division\nx %/% y (e.g. 5 int div 2 = 2)\n\n\nMatrix Multiplication\n%*%\n\n\n\nIt also has many logical operations built in.\n\nLogical Operations in R\n\n\nOperation\nCode Example\n\n\n\n\nLess than\nx < y\n\n\nLess than or equal to\nx <= y\n\n\nGreater than\nx > y\n\n\nGreater than or equal to\nx >= y\n\n\nExactly equal to\n==\n\n\nNot equal to\nx != y\n\n\nNot x\n!x\n\n\nx OR y\nx | y\n\n\nx AND y\nx & y\n\n\nIs x TRUE?\nisTRUE(x)\n\n\n\nThese come in pretty handy for performing most operations on our data. If you’re unfamiliar with these, don’t worry. We’ll cover how you might use some of these in a staggered format as you progress through this course. Nicely, R also has a number of functions built in.\nFunctions in R always end in parentheses, indicating that they take an argument. For example, one of the most basic and important functions in R is c() for concatenate. This allows you to combine many values into a vector or list of values.\nWhen we combine values into an object, this object is stored in our global environment. This means that we can perform operations on the object later on, without the worry of typing our the values again. This is particularly useful if you want to store values from one function (say a statistical test) that you cannot pre-define but that you want to use later on.\nLet’s see how this works.\n\nmy_values <- c(1, 10, 4, 5)\nmy_values\n\n[1]  1 10  4  5\n\n\nWe now have a vector of values stored in one object.\nR has other convenient functions built in. For example, we can sum this vector, or get its mean.\n\nsum(my_values)\n\n[1] 20\n\nmean(my_values)\n\n[1] 5\n\n\nNotice how you also don’t have to output things one at a time. R remembers the order of operations.\nFunctions can have default values or not (requiring you to specify the argument). Above, we passed the the values in my_values to each function as an argument. Later, we’ll look at functions that ask for arguments from separate data types (e.g. numbers and characters) or even multiple arguments.\nIf you’re unsure what an argument does, you can always ask R what it does, how it does it, and what to pass to it by using ?, e.g. ?mean(). This will bring up a document in the Help window of RStudio.\nUsing objects allows our code to be flexible, as we can write a script that performs operations on objects that can take any range of values. This, to me, is one of the nicest things about doing your analyses in R. While you may spend more time getting your script up and running in the first place when compared to using point-and-click methods (e.g. in SPSS), if you gain new data or run a new experiment, it’s likely that your script can simply be re-run with no (or few) changes at very little cost to your time.\nNow, this part is pretty important but may only be obvious if you’ve programmed in other languages. R is a vectorised language, which means that, as with the sum() function above, R can perform operations on the entire object. So, if you want to increment all values in your object by 1, you can simply tell R to do so in one line of code, without the need for loops or other complex methods.\n\nmy_values + 1\n\n[1]  2 11  5  6\n\n\n\n1.5.1 Namespace Conflicts\nMost of the time, you won’t have any trouble using functions from a loaded package. However, there can be cases when you have two packages installed that use the same function name. To tell R exactly which version of a function to use, we can specify both the package and function name in the form package::function_name(). For example, we can use the group_by() function from the package dplyr by typing dplyr::group_by(). You won’t come across this in this course, as we’ll be using packages that have functions with unique names, but it’s worth bearing in mind if you come across problems with functions you know should work in the future.\n\n\n1.5.2 Data Types\nWhile we’ve seen that we can create objects containing integers (whole numbers), we can also create objects of other data types. There are 4 main data types that you’ll come across regularly in R:\n\nCharacters: Strings of text, e.g. \"My cats, Bear and Penny\"\nNumeric: Numbers stored as floats (decimals), e.g. 1, 1.5, 1.576.\nInteger: Numbers stored explicitly as whole numbers using L notation, e.g. 1L, 2L\nLogical: Boolean operators, i.e. TRUE and FALSE. (Avoid T and F as these can be overwritten.)\n\nOnly values of the same data type can be stored together in an object. If you try to concatenate values of different data types you get type coercion. Let’s see the difference in output when concatenating two numbers versus a number and a character.\n\nc(2, 2)\n\n[1] 2 2\n\nc(2, \"Cat\")\n\n[1] \"2\"   \"Cat\"\n\n\nNotice that in the first example the two values are unquoted. In the second, the two values are quoted. That’s because since you can only store values of a single data type within an object, when you try to combine a number with a character, R converts all values to character.\nSimilarly, certain operations only work on specific data types. For example, if you try to perform mathematical operations on invalid data types (e.g. trying to add two characters), R will give you an error.\n\n2 + \"Cat\"\n2 + \"2\"\n\nIf you try to run this code, you get a similar error to this: Error in 2 + \"2\" : non-numeric argument to binary operator. This basically says you can’t add a character to a number.\n\n\n1.5.3 Data Structures\n\n1.5.3.1 Vectors\nWe’ve seen already how we can combine values of different data types into one object. In R, these objects are called vectors. We’ve seen already how vectors can be useful in that operations can be applied to every element in the vector using simple mathematical operations. For example, when we want to add 1 to every element of the vector my_numbers, we just type my_numbers + 1. In other, non-vectorised languages, we need to have a way to apply this addition to every element of the vector. This brings us nicely to the idea of indexing values in a vector. How do we get an value back from a vector at a specific location?\nHere, we’ll create a vector with the values 3 through 7, and extract the third value from the vector. We can use the : operator to get values between 3 and 7 without explicitly writing them out. We can then extract a value at a specific place in our vector using []. Since R is a 1 indexed language, when we want the third value from a vector, we make this [3]\n\nmy_numbers <- 3:7 # values are: 3, 4, 5, 6, 7\nmy_numbers[3]\n\n[1] 5\n\n\nR returns the value in the third position, 5.\nWhat if we want to change the value in position 2 to 189? We use indexing to access this value, [2], and assignment to make the value at that position 189, <- 189.\n\nmy_numbers[2] <- 189\nmy_numbers\n\n[1]   3 189   5   6   7\n\n\nFinally, we can create vectors from a range of complex in-built (and additional) functions. Let’s look at creating scores from a sequence of numbers, sampling from a set of numbers, and even drawing scores from a normal distribution. These functions all take on multiple named arguments. Remember, you can find out about these functions by using ?, e.g. ?seq().\nFor this, imagine we want to perform a quick simulation of what IQ looks like for cat and dog owners (assuming people only have one or the other).\nLet’s first create some participant IDs ranging from 1 to 100.\n\nparticipant_id <- seq(from = 1, to = 100, by = 1)\nparticipant_id\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\n\nBefore we get with sampling further data, we’ll set the random seed in R. Since computers are deterministic, nothing is ever actually random with them. Instead, processes that we imagine are random (avoiding heavy philosophical issues here) are created by pseudorandom number generators that take a seed. This is a number used to get the pseudorandom number generator started. We often use these in our code to ensure computational reproducibility. Though the process itself is hopefully random, you might still want to recreate my exact random numbers. Let’s do this.\n\nset.seed(1892)\n\nThen lets make some pet ownership codes and sample from this 100 times with replacement.\n\npets <- c(\"cat\", \"dog\")\npet_owned <- sample(pets, size = 100, replace = TRUE)\npet_owned\n\n  [1] \"dog\" \"cat\" \"dog\" \"cat\" \"dog\" \"cat\" \"cat\" \"dog\" \"cat\" \"dog\" \"dog\" \"cat\"\n [13] \"cat\" \"dog\" \"dog\" \"cat\" \"dog\" \"dog\" \"cat\" \"dog\" \"dog\" \"cat\" \"cat\" \"cat\"\n [25] \"cat\" \"dog\" \"cat\" \"cat\" \"cat\" \"cat\" \"cat\" \"cat\" \"dog\" \"dog\" \"cat\" \"dog\"\n [37] \"cat\" \"cat\" \"cat\" \"dog\" \"dog\" \"dog\" \"cat\" \"dog\" \"cat\" \"cat\" \"dog\" \"dog\"\n [49] \"cat\" \"dog\" \"cat\" \"dog\" \"cat\" \"dog\" \"dog\" \"dog\" \"dog\" \"cat\" \"cat\" \"cat\"\n [61] \"cat\" \"dog\" \"dog\" \"dog\" \"dog\" \"cat\" \"dog\" \"cat\" \"dog\" \"cat\" \"dog\" \"dog\"\n [73] \"dog\" \"cat\" \"dog\" \"dog\" \"dog\" \"cat\" \"cat\" \"dog\" \"cat\" \"dog\" \"dog\" \"cat\"\n [85] \"dog\" \"dog\" \"cat\" \"cat\" \"dog\" \"cat\" \"cat\" \"dog\" \"cat\" \"dog\" \"cat\" \"dog\"\n [97] \"cat\" \"cat\" \"cat\" \"dog\"\n\n\nFinally, let’s create the IQ scores, sampling from a normal distribution with mean 150 and with a standard deviation of 15.\n\niq_score <- rnorm(n = 100, mean = 100, sd = 15)\niq_score\n\n  [1] 119.90419 100.31550 106.28567 100.32896  99.40381 105.60851  69.25938\n  [8]  88.32001 101.35022 108.49565  79.78903 106.13353 102.67474  97.29907\n [15]  88.10597 109.67761 109.34948 102.41409 115.91469 126.64919  80.55541\n [22]  93.51404 103.97598  75.78122  97.44319 107.84348  91.55643  81.11721\n [29] 108.46863 113.81963  96.25558  94.15728 122.85215  86.35401 109.98973\n [36] 104.70575  95.83989  62.79169  90.88445 100.07760  82.95541  95.33197\n [43] 114.02527 112.32369  81.45885 111.64807  87.57530  94.64206  99.15243\n [50]  61.11503 106.62990  79.72260 100.15081  83.41594 114.31351  99.14609\n [57]  84.50973  81.42513 114.89340 124.02161  89.06398  93.27161  94.83063\n [64] 106.89741 111.33804  84.21109 126.75998 106.26074 120.37403  77.79786\n [71] 128.42155 112.19440  92.08103 104.15516 127.38645 105.28515  90.39646\n [78]  96.97260  99.75440 124.81151 107.16399 106.19192  70.66848  78.42079\n [85]  94.77819  92.42836  96.64234  91.71500 133.46106  84.11078  88.70980\n [92]  80.65210  94.50641  93.65194  87.23135 109.73530 128.65171  91.49707\n [99]  76.43452 108.62265\n\n\nNow, we could index these values, change them, or perform operations on them to our heart’s content. Let’s see some useful things we can do if we had data stored in this way.\nFirst, we might want to know how many participants are in our sample. Assume we don’t already know it’s 100, we could do this by asking for the length of the participant ID vector.\n\nlength(participant_id)\n\n[1] 100\n\n\nWe have 100 IDs! But what if someone took the study again? How would we find out the number of unique people in the sample? First, assign participant 100 again to the object, in the 101st place. Then we’ll get the length of this to see that we have 101 values. If we want the unique values, we’ll take the length of the unique values.\n\nparticipant_id[101] <- 100\nlength(participant_id)\n\n[1] 101\n\nlength(unique(participant_id))\n\n[1] 100\n\n\nNotice that by chaining together two functions we were able to get the length of the unique participants in the object. This function chaining is an important concept in any programming work.\n\n\n1.5.3.2 Lists\nLists are a way to store multiple vectors of different data types together in one object. Think of it as nesting vectors within vectors (very meta).\nYou can make these from existing objects or from scratch by defining values within the list. Lists can either have named elements, where we explicitly state the name of each object to be stored in the list, or they can be unnamed. Here, we’ll make a named list from our simulated data.\n\nsimulated_data <- list(\n  participant = participant_id,\n  pet = pet_owned,\n  iq = iq_score\n)\n\nAs before, we could print out each element of this list, but it’d produce a lot of output for the console. Instead, let’s check out a new list based on data we create within the list. Let’s look at the qualities of people named Glenn and not Glenn.\n\nperson_quality <- list(\n  glenn = c(\"handsome\", \"smart\", \"modest\"),\n  not_glenn = c(\"less_handsome\", \"less_smart\", \"less_modest\")\n)\nperson_quality\n\n$glenn\n[1] \"handsome\" \"smart\"    \"modest\"  \n\n$not_glenn\n[1] \"less_handsome\" \"less_smart\"    \"less_modest\"  \n\n\nIf we want just Glenn (which most people do, I’m sure) along with the name of the vector, use the same notation as before to access the element in the first location.\n\nperson_quality[1]\n\n$glenn\n[1] \"handsome\" \"smart\"    \"modest\"  \n\n\nOr we could access it by name:\n\nperson_quality[\"glenn\"]\n\n$glenn\n[1] \"handsome\" \"smart\"    \"modest\"  \n\n\nIf we just want the values in this element (which we often do), we need to use the double bracket notation, [[]].\n\nperson_quality[[\"glenn\"]]\n\n[1] \"handsome\" \"smart\"    \"modest\"  \n\n\nIn doing this we can edit values at specific locations or add elements to the vector just as we did before. However, this requires indexing the values of the correct element in the list, and then accessing the position of the correct value. This requires using a combination of double and single bracket notation.\n\nperson_quality[[\"glenn\"]][4] <- \"liar\"\nperson_quality[[\"glenn\"]]\n\n[1] \"handsome\" \"smart\"    \"modest\"   \"liar\"    \n\n\nAs you’ll notice in adding a fourth element to this entry in the list, the data needn’t be square. There are 4 elements in one of the entries of the list, but only 3 in the other.\n\nperson_quality\n\n$glenn\n[1] \"handsome\" \"smart\"    \"modest\"   \"liar\"    \n\n$not_glenn\n[1] \"less_handsome\" \"less_smart\"    \"less_modest\"  \n\n\nThis isn’t the case for the more commonly encountered data structure you’ll use in the course, data frames. We’ll often work with data frames because they’re easy to manage and follow a logical structure that’s analogous to working with a spreadsheet.\n\n\n1.5.3.3 Data Frames (and Tibbles)\nIn the real world, if you tested IQs you’d typically have this data stored in a table somewhere prior to reading it into R. So let’s pair the data together into a table in R. One way to do this is to create a data frame. However, if you use the tidyverse set of packages, which we do here, you have access to tibbles. These are just data frames with some sensible defaults like ensuring that R doesn’t convert vectors to different data structures when you subset your table.\nLet’s make a tibble from our IQ data from earlier. Notice that we have to subset the participant_id object to be the same length (100 items) as the other objects.\n\niq_data <- tibble(\n  participant = participant_id[1:100],\n  pet = pet_owned,\n  iq = iq_score\n)\niq_data\n\n\n\n  \n\n\n\nTibbles are nice to use in that they show you the data type of each column, and by default print the first 10 rows of data only when you print the table. if you want more rows of data, you can ask for it explicitly. Here, n defines the number of rows, while width defines the number of columns. We can set this to Inf or infinity, to ensure all columns are printed.\n\nprint(iq_data, n = 12, width = Inf)\n\n# A tibble: 100 × 3\n   participant pet      iq\n         <dbl> <chr> <dbl>\n 1           1 dog   120. \n 2           2 cat   100. \n 3           3 dog   106. \n 4           4 cat   100. \n 5           5 dog    99.4\n 6           6 cat   106. \n 7           7 cat    69.3\n 8           8 dog    88.3\n 9           9 cat   101. \n10          10 dog   108. \n11          11 dog    79.8\n12          12 cat   106. \n# … with 88 more rows\n\n\nUnfortunately, some older functions in R won’t allow you to use a tibble. If this is the case, simply convert your tibble to a data.frame using the as.data.frame() function. Note, we use head() to see the head of our data frame, or the first 6 values. This is necessary here to avoid printing out each row, as we’re not in using a tibble any more. Notice that We’ve assigned the data.frame version of our IQ data to a new object, rather than overwriting the previous object. This is good practice when testing your code, as you never know what might break, resulting in data loss. (Although this wasn’t strictly necessary here.)\n\niq_data_df <- as.data.frame(iq_data)\nhead(iq_data_df)\n\n\n\n  \n\n\n\nThere are multiple ways to access data from a tibble or data frame.\n\n1.5.3.3.1 Working with Columns\nWe can access columns through dollar indexing for a object, or by name or position as we did with lists. (In fact, this works because tibbles and data frames are just square lists!)\n\niq_data$iq\niq_data[[\"iq\"]]\niq_data[[3]]\n\n\n\n  [1] 119.90419 100.31550 106.28567 100.32896  99.40381 105.60851  69.25938\n  [8]  88.32001 101.35022 108.49565  79.78903 106.13353 102.67474  97.29907\n [15]  88.10597 109.67761 109.34948 102.41409 115.91469 126.64919  80.55541\n [22]  93.51404 103.97598  75.78122  97.44319 107.84348  91.55643  81.11721\n [29] 108.46863 113.81963  96.25558  94.15728 122.85215  86.35401 109.98973\n [36] 104.70575  95.83989  62.79169  90.88445 100.07760  82.95541  95.33197\n [43] 114.02527 112.32369  81.45885 111.64807  87.57530  94.64206  99.15243\n [50]  61.11503 106.62990  79.72260 100.15081  83.41594 114.31351  99.14609\n [57]  84.50973  81.42513 114.89340 124.02161  89.06398  93.27161  94.83063\n [64] 106.89741 111.33804  84.21109 126.75998 106.26074 120.37403  77.79786\n [71] 128.42155 112.19440  92.08103 104.15516 127.38645 105.28515  90.39646\n [78]  96.97260  99.75440 124.81151 107.16399 106.19192  70.66848  78.42079\n [85]  94.77819  92.42836  96.64234  91.71500 133.46106  84.11078  88.70980\n [92]  80.65210  94.50641  93.65194  87.23135 109.73530 128.65171  91.49707\n [99]  76.43452 108.62265\n\n\nAll three methods pull out every value from the IQ column as a basic vector. Just like with lists, by using the double bracket method you get just the values, and not the name of the object.\n\n1.5.3.3.1.1 Adding or Removing Columns\nTo add a row to a data frame, we simply need to specify what we want to add and assign it a new name. Let’s say that we want to add a column that indicates the operating system used by each participant.\nWe may have this because we made assumptions that people who use Windows, macOS, or the Linux families of operating systems differ in their IQ. This is a silly example for several reasons, not only because you can use more than one system; but we’ll stick with this for now.\nImagine we already have a sample of operating systems to draw from. You don’t need to understand how this works, but briefly I’ve used the inbuilt sample() function to pick from the three names with replacement, skewing the probabilities to select Windows most often, followed by Mac, then Linux. All that matters is that we’re assigning 100 names to an object.\n\nset.seed(1892) # set the random seed\n\noperating_system <- sample(\n  c(\"windows\", \"mac\", \"linux\"), \n  size = 100, \n  replace = TRUE,\n  prob = c(0.5, 0.3, 0.2)\n)\n\nIn the iq_data data set, we can add the new column using the usual assignment operator.\n\niq_data$operating_system <- operating_system # add new column\niq_data\n\n\n\n  \n\n\n\nNote that you can rename the column to anything you like. But, for consistency, I like to keep the same name as the object which acts as the data source.\nFinally, we can remove the new column (and any column) by setting the entire column to nothing (NULL), like so:\n\niq_data$operating_system <- NULL # remove the column\niq_data\n\n\n\n  \n\n\n\nNow the data is back to its original format.\n\n\n\n1.5.3.3.2 Working with Rows\nWe can access rows again using the name or position indexing as above. However, since we’re accessing multiple columns we won’t be able to pull them out as a single vector. This means we can’t use the double bracket notation. Instead, we use single bracket notation and use a comma to specify what we want from rows, and what we want from columns. Remember, rows first, then columns.\nLet’s get the first two rows from the pet column. There’s a few ways we could do this:\n\niq_data[1:2, \"pet\"]\niq_data[c(1, 2), \"pet\"]\niq_data[1:2, 2]\n\n\n\n\n\n  \n\n\n\nYou’ll notice that R returns a tibble even when we subset this time. That’s because we’ve asked for specific rows from the entire table. If we want just these rows from a column, we can combine the notation with list-style subsetting as we used before.\n\niq_data[1:2, \"pet\"]$pet\niq_data[1:2, \"pet\"][[\"pet\"]]\niq_data[1:2, \"pet\"][[1]]\n\n\n\n[1] \"dog\" \"cat\"\n\n\nWe can include multiple columns in the first column index to get specific rows for a subset of columns.\n\niq_data[1:2, c(\"pet\", \"iq\")]\n\n\n\n  \n\n\n\nOnce you know how to index these values, assigning new values to them is just as easy as before. Simply use the assignment operator <-.\nThis can all be a little unintuitive, so in future chapters we’ll look at how to use the tidyverse functions to subset rows and columns more easily.\n\n1.5.3.3.2.1 Adding or Removing Rows\nWhat if we want to add a new row to our data? This may be less common than adding a new column for data processing purposes, but it’s good to know anyway.\nFirst, we need to know what should go in each cell. Remember that we have to keep the data square, so you can’t have missing values when you add a row. If you don’t have any data, you can just put NA (with no quotations) to keep the data square but to show that you don’t have any value for a given cell.\nLet’s assume we want to add a new participant, 101, who has a dog but an unknown IQ. We must define a list of data where we assign values to the columns that match up with our IQ data column headings.\nHere, we have to define all our values to be added in parentheses, using the list() function:\n\nparticipant number is 101\npet_id is “dog”\niq is NA (i.e. unknown)\n\nThen we assign this list of values to the data frame in the 101st row.\nWe do this like so:\n\niq_data[101, ] <- list(\n  participant = 101, \n  pet = \"dog\",\n  iq = NA\n)\n\niq_data\n\n\n\n  \n\n\n\nRemember that data frames and tibbles have to be square (i.e. with data in every column). This means if we just assign a participant ID to a column, all remaining rows are completed with NA.\n\niq_data[102, \"participant\"] <- 102\n\ntail(iq_data)\n\n\n\n  \n\n\n\n\n\n\n\n1.5.3.4 Matrices\nMatrices work very similarly to data frames and tibbles, but they’re even stricter. They can only contain the same data type throughout, so we can’t mix columns containing characters and numbers without converting them all to the same data type. Here’s how you’d make a matrix. However, we won’t go into any other details here. You’ll mainly come across matrices only when doing more advanced statistics by hand or developing your own statistical packages. For most of your data work, this isn’t necessary.\n\nmatrix_example <- matrix(\n  rep(1:25),\n  nrow = 5,\n  ncol = 5\n)\nmatrix_example\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    6   11   16   21\n[2,]    2    7   12   17   22\n[3,]    3    8   13   18   23\n[4,]    4    9   14   19   24\n[5,]    5   10   15   20   25"
  },
  {
    "objectID": "01_getting-started.html#quarto-documents",
    "href": "01_getting-started.html#quarto-documents",
    "title": "1  Getting Started",
    "section": "1.3 Quarto Documents",
    "text": "1.3 Quarto Documents\nBefore we start coding, it’s worthwhile explaining how we can create scripts or documents for our code that allow us to perform our data processing tasks.\nTraditionally, R users often wrote their code in R Scripts. You can try this now by going to File > New File > R Script. This will open a pane in your editor where you can write your R code. Crucially, you can save this file to your computer, allowing you to return to your work at a later date or to rerun your code. Notice that at the moment the pane is labelled Untitled1. That’s because you haven’t saved your work yet. Go to File, Save or click the floppy disk icon just under the tab for this pane to save your work.\n\n1.3.1 Literate Programming\nScripts are a great way to work with R, but they can be difficult to manage (especially for beginners) and even if these scripts produce files or graphs, you’re still left with the prospect of putting together your scientific outputs in a separate word processor, which often involves a lot of copying and pasting.\nInstead, one option is to produce your outputs with your R code embedded within them. This has the advantage of cutting down on transcription errors and time tweaking the output of your documents every time you update something. Let’s imagine you write a paper based on a project with several analyses and containing several plots. Unfortunately, you didn’t realise that one participant should have been excluded from the analyses all along (we all make mistakes). If you made your report manually, you’ve got a lot of manual edits to make. If you let R populate your outputs in the document, it’s as simple as updating a single line of code to remove that participant and rerunning the analyses by pressing a big play button. This should cut down on further human error.\nFor these reasons, I’m a fan of Quarto: open-source scientific and technical publishing system that allows you to create dynamic documents with content using R, Python, Julia, or Observable code. The advantage of Quarto over other literate programming systems in R (such as RMarkdown) is that if you ever change your programming language to one of the other 3, your workflow stays the same. This reduces overheads to being multilingual.\n\n\n1.3.2 Creating a Quarto Document\nCreate a Quarto document by going to File > New File > Quarto Document. You can set the name and author of the document here or define those later on. Keep the default options for the output type and engine for rendering and click Create. This creates a Quarto document with some boilerplate code and text to show you how it works.\n\n\n\nThe default Quarto template\n\n\nSave this document somewhere where you’ll have access to it again if you’d like to keep a record of the code used in the remaining sections of this chapter.\n\n\n1.3.3 Understanding Quarto Documents\nBy default, RStudio opens the document using the visual editor. This allows you to type text directly into RStudio in a similar way to how you would with other word processing software. Notice the headings below the tab that allow you to apply different styles to the text, to create lists, links, and include images and tables.\n\n1.3.3.1 YAML Headers\nAt the top of the document is the YAML header. This stands for YAML Ain’t Markup Language (illuminating, I know). Essentially, this is a highly-readable format for configuring your Quarto document. Options are presented as bare text with a colon and can define the content, look, or behaviour of your document. Here, if we change title: \"Untitled\" to title: \"My First Quarto Document\" when you render the file your title will be updated. That’s all we need to know about the heading for now.\n\n\n1.3.3.2 Markdown\nText is authored using Markdown. (Specifically, the Pandoc flavour of Markdown.) This is a plain text syntax and tool for converting your text to HTML without having to know much, if any, HTML. The idea behind this system is that you can write your content and use some simple syntax to control the formatting of the text. The exact look of it is then handled by a template, several of which come with Quarto. This is different to a WYSIWIG (What You See Is What You Get) editor such as Word where you must define the content, formatting, and presentation at the same time. One advantage of Markdown is that you can create a document based on e.g. the APA manuscript template and then immediately change it to a specific journal’s formatting by swapping out your template. No more manual edits.\nFor an introduction to Markdown formatting, see https://quarto.org/docs/authoring/markdown-basics.html. But, since we’re using the visual editor this won’t be necessary right now.\n\n\n1.3.3.3 Embedding Code\nFinally, you’ll notice that the Quarto boilerplate includes some code chunks. These can be inserted by clicking the green +C in the header of RStudio, by Cmd + Option + I on a Mac or Ctrl + Alt + I on PC, or by typing:\n```{r}\n```\nYour code then goes in the empty space between the backticks.\nCode chunks allow you to write long expressions in R that span multiple lines. The output of that code is then presented in your document immediately below the code that produced it (unless you set options for your code to not show up). Press play on the code chunk or highlight the code and press Cmd + Enter or Ctrl + Enter to run it.\nFinally, you can embed R code within text by using surrounding your code with backticks with the inclusion of the language (R) at the header of this. Here’s an example: `r 1 + 1 `. This is useful when including in-text statistics in a document.\n\n\n1.3.3.4 Rendering Your Document\nTo create an output file which renders your Markdown and code chunks into a nicely formatted output (like this ebook you’re viewing now), simply click the Render button at the top of your editor. This will create a rendered document in the same location as your Quarto file, rendered in the format you specify in the YAML header. By default, this is an html file. Figure 1.3 shows how the source code and rendered html file compare.\n\n\n\n\n\n\n\n(a) Quarto source code\n\n\n\n\n\n\n\n(b) Quarto rendered html output\n\n\n\n\nFigure 1.3: Rendering Quarto documents"
  },
  {
    "objectID": "01_getting-started.html#section",
    "href": "01_getting-started.html#section",
    "title": "1  Getting Started",
    "section": "1.6 ",
    "text": "1.6 \n\n\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686."
  },
  {
    "objectID": "01_getting-started.html#project-management",
    "href": "01_getting-started.html#project-management",
    "title": "1  Getting Started",
    "section": "1.2 Project Management",
    "text": "1.2 Project Management\nA large part of creating an effective workflow for your data processing needs revolves around having an effective system for managing the inputs, processing scripts, and outputs associated with your project. By using a logical folder structure and consistent naming conventions you can make working with and managing updates/changes to your project much easier.\n\n1.2.1 File Systems\nWhen working with R, its important to know where the working directory of your project is based. The working directory is essentially the home base for R: where it looks by default when you try to read into R or save data from R to file.\nBy default in RStudio this depends on how you open RStudio:\n\nBy opening RStudio: the working directory is wherever you installed R on your computer.\nBy opening a .R file: the working directory is the location of the file.\nBy opening an RStudio Project: the working directory is the location of the project. More on this later!\n\nTo find out where your working directory is in RStudio right now, type in the console getwd() and press Enter to get the working directory. (Henceforth, if you see code like this, try it out by either copying and pasting into the console or typing it out and pressing Enter.)\nAfter using getwd() you should see something like \"/Users/glenn\", with glenn replaced with whatever your home folder is. This means that by default if I want to read some data into R or write it to a file then R will look in this folder to do so. But, you probably want to have a better organisational system than having many files floating around in your home directory. One way around this is to create a specific folder for your project and set your working directory to that folder. Imagine I have a folder in my home directory called “DS-Psych”. I might set the working directory to this location by using setwd(\"/Users/glenn/DS-Psych\"). Now, R will by default read/write files at this location.\nBut, if you share this with script with someone (or even yourself on say, a computer at work), it won’t work on their machine because you can guarantee they won’t have the same file structure as you. For example, they might have a home directory called rachel or george. Now using setwd(\"/Users/glenn/DS-Psych\") will result in an error.\nSo, how do we solve the problem of (a) knowing exactly where our working directory is, and (b) ensuring that directory works across different machines? RStudio Projects are the answer.\n\n\n1.2.2 RStudio Projects\nAn RStudio Project is a file that can sit in a folder which allows you to open an instance of RStudio in the root (top level) of the folder.\nYou can create the folder from scratch within RStudio with an associated Project file, or add the Project file to an existing folder within RStudio. To do this click File > New Project and select the relevant option for your use case.\n\n\n\nCreating an RStudio Project\n\n\nNow, whenever you want to start up RStudio and ensure that your working directory is in the project folder, you simply need to open the RStudio project file that sits in the folder. This will work across different computers for different people, meaning that simply sharing the folder with other people is enough to ensure that R looks in the correct place for files when they run your scripts.\nTo further ensure that your script runs when, for example, reading in or writing data to file, its good practice to ensure that you way you direct R to files avoids specifying locations that are only present on your computer.\n\n\n1.2.3 Absolute and Relative File Paths\nImagine you have a folder called analysis on your desktop containing the following items:\n\ninputs/my_data.csv\noutputs/\nmy_script.R\nanalysis.RProj\n\nThe data you want to analyse, my_data.csv, sits in a sub-folder called inputs. You’d like to write a script that reads this data in, creates a graph, and saves that graph in the outputs folder.\nYou open up RStudio by double clicking on the analysis.RProj file. This ensures that R knows the working directory is set to the root of the analysis folder (i.e. wherever the .RProj file sits). You can read the data into R in one of two ways, using:\n\nAbsolute file paths: You specify the exact location of the file on your computer using the full location from your root directory. For me, this would be /Users/glenn/Desktop/analysis/inputs/my_data.csv\nRelative file paths: You specify the location of the file relative to your working directory. For all of us, this would be: /inputs/my_data.csv\n\nNot only are relative file paths shorter and easier to manage, but they will work on anyone’s computer as long as they have your project folder. This makes reproducibility and collaboration much easier.\n\n\n1.2.4 Naming Conventions\nSince you’ll often work with files and folders, it’s a good idea to establish a consistent naming convention. Jenny Bryan has a great presentation on why this matters, based around 3 principles. Make names:\n\nMachine readable: File names should be easily read by computers. Avoid spaces, punctuation, accented characters, and case sensitivity.\nHuman readable: File names should be easily read by humans. Include a slug to define what a file is or does (e.g. 01_read-files.R). Separate words with a dash to meet the machine readable and human readable criteria. Use underscores to separate slugs (e.g. concepts).\nPlay well with default ordering: Numerics come first. Left pad numbers to ensure proper numeric ordering. Use the ISO 8601 standard (YYYY-MM-DD) for dates.\n\nExamples of poor and good naming conventions are provided below:\n\nExamples of poor and good file naming conventions\n\n\nPoor Naming Conventions\nGood Naming Conventions\n\n\n\n\n26012022_rp1.csv\n2022-01-26_reading-data_participant-01.csv\n\n\n26012023_wp1.csv\n2023-01-27_writing-data_participant-02.csv\n\n\n10.R\n01_read-data.R\n\n\n1.R\n02_plot-data.R\n\n\n…\n…\n\n\n2.R\n10_fit-models.R\n\n\n\nNotice that with poor naming conventions not using the correct date changes the order of the files for both the .csv files (based on date) and by not left-padding the R files? Also, it’s clear that using too many abbreviations or simply naming files with inscrutable titles will make working with this a nightmare."
  },
  {
    "objectID": "01_getting-started.html#some-final-tips",
    "href": "01_getting-started.html#some-final-tips",
    "title": "1  Getting Started",
    "section": "1.6 Some Final Tips",
    "text": "1.6 Some Final Tips\nFinally, a few tips on checking your data before you manipulate your data:\n\nIf you’re unsure what objects you’ve created in a session, either check the environment pane in RStudio or type ls() to list everything in the global environment.\nIf you want to know the class of data for some object, use the class() function (e.g. class(iq_data)).\nIf you want to know the structure (including object classes) for some object, use the str() function (e.g. str(iq_data). Nicely, str() also tells you how many arrays are in the object, and how many observations you have in total.\n\nI strongly recommend that you choose a style guide and stick to it throughout when you write your R code. This will make it easier to notice any errors in your code, and increases readability for you and others. Consistency is key here. Since we’re using a tidyverse first approach to teaching R in this course, I recommend the following one by Hadley Wickham, a core developer of the tidyverse.\n\n1.6.1 R Style Guide by Hadley Wickham\nThe important things to take home are that:\n\nUse sensible object names: if a column shows, e.g. participant weight, call it participant_weight.\nUse verbs to describe user-defined functions: if you write a function to make all the descriptive statistics you could ever want, call it something like make_descriptives().\nUse a consistent style, like snake_case, or even camelCase, but don’t mix_snake_and_camelCase.\nComment your code with descriptions of why you’ve done something using #: you can often work out how you did it by following your code, but the why is easily lost.\n\n\n\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686."
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Data Science for Psychologists",
    "section": "Course Content",
    "text": "Course Content\nTo download the course content, including workshop slides, videos, and exercise workbooks, including instructions for using and downloading the content, please follow this link: https://github.com/gpwilliams/ds-psych_course"
  },
  {
    "objectID": "00a_part-core.html",
    "href": "00a_part-core.html",
    "title": "Core",
    "section": "",
    "text": "This section is dedicated to getting you from no experience with R or programming whatsoever to being able to use R for your entire data processing pipeline.\nAt the end of this section you will be able to perform complex data processing operations, including combining and cleaning multiple data sets, create beautiful, publication-ready graphs, compute descriptive and inferential statistics with a deep understanding of the general linear model family of analyses, create beautiful dynamic reports that can generate content from code, and do all of this while adhering to best practices in terms of open science practices.\nOnce you understand the core content, you can optionally move on to the Advanced topics. This section consists of modular chapters that introduce you to advanced topics in R for which R excels."
  },
  {
    "objectID": "00b_part-advanced.html",
    "href": "00b_part-advanced.html",
    "title": "Advanced",
    "section": "",
    "text": "This section consists of modular chapters that introduce you to advanced topics in R for which R excels. Here, the focus is on introducing the conceptual and philosophical basis for these methods, learning how to implement them, but also crucially on how to communicate these results and tackle any potential issues encountered when executing these methods.\nIt is expected that you are at least comfortable with the Core topics before attempting these chapters. The structure of this section is such that you can read each chapter in any order you like, so feel free to focus on chapters that are only useful to your research."
  },
  {
    "objectID": "index.html#core-content",
    "href": "index.html#core-content",
    "title": "Data Science for Psychologists",
    "section": "Core Content",
    "text": "Core Content\nThe first section of the book is written for those with no experience with R or programming in general. The focus here is on using R across the data processing and analysis pipeline so that you can automate data processing. This has the benefit of documenting your work, automating tedious manual processes, and avoiding user error. Along the way you will learn about best practices in terms of project structure and workflows, version control to track and manage updates to your code, and how to share your work online with the broader scientific community. By the end of this section you should be able to do all your data analysis work with R."
  },
  {
    "objectID": "index.html#advanced-content",
    "href": "index.html#advanced-content",
    "title": "Data Science for Psychologists",
    "section": "Advanced Content",
    "text": "Advanced Content\nThe second section of the book is written for those with a background in R who want to take advantage of the advanced data analysis methods available to this language. This section has a heavier focus on theory with the goal of understanding not just how to use advanced methods, but also how these methods work."
  },
  {
    "objectID": "index.html#approach-to-coding",
    "href": "index.html#approach-to-coding",
    "title": "Data Science for Psychologists",
    "section": "Approach to Coding",
    "text": "Approach to Coding\nThroughout, concepts will be taught using examples from real and simulated data from studies in Psychology. R will be taught using a tidyverse-first approach, using a suite of packages that are designed to make programming quick, easy, and highly readable."
  },
  {
    "objectID": "index.html#sections",
    "href": "index.html#sections",
    "title": "Data Science for Psychologists",
    "section": "Sections",
    "text": "Sections\n\nCore\nThe first section of the book is written for those with no experience with R or programming in general. The focus here is on using R across the data processing and analysis pipeline so that you can automate data processing. This has the benefit of documenting your work, automating tedious manual processes, and avoiding user error. Along the way, you will learn about best practices in terms of project structure and workflows, version control to track and manage updates to your code, and how to share your work online with the broader scientific community. By the end of this section you should be able to do all your data analysis work with R.\n\n\nAdvanced\nThe second section of the book is written for those with a background in R or who have completed the core content who want to take advantage of the advanced data analysis methods available to this language. This section has a heavier focus on theory with the goal of understanding not just how to use advanced methods, but also how these methods work, how to diagnose and solve problems, and how to communicate your findings."
  },
  {
    "objectID": "02_creating-graphs.html#the-grammar-of-graphics",
    "href": "02_creating-graphs.html#the-grammar-of-graphics",
    "title": "2  Creating Graphs",
    "section": "2.1 The Grammar of Graphics",
    "text": "2.1 The Grammar of Graphics\nIn R, you can build plots using the base plotting system or lattice graphics. The former is very low level, giving you a lot of granular control over plotting at the cost to specifying everything manually. The latter is more high level but makes customisation difficult. Thankfully, ggplot2, a package in the tidyverse strikes a good balance between the two. Most notably, however, it relies on a grammar of graphics to make creating bespoke plots consistent across many types of visualisations.\nBroadly, a grammar of graphics is a set of rules we can use to describe the components of a graphic. In the context of creating plots in R, the approach moves away from focusing on specific types of plots (e.g. a scatter plot, a box plot, a bar plot), instead focusing on the elements of the plot that can be build up to create different visualisations (Wickham 2010).\nIn ggplot2, this means every plot must define a data set from which to map onto a canvas. From here, we add layers to the plot. Some key concepts that form the basis of every plot are:\n\ngeoms: these are geometric objects that you map your data to. For example, points on a scatter plot.\naes: these are the aesthetics you define which map certain parts of your data to the plot. For example, the which parts of your data should be mapped onto the x and y coordinates of a scatter plot.\nscale: the scale you want to have for your axes. This is often calculated using sensible defaults in ggplot2, but sometimes you’d like control over this.\nannotations: Labels used for axes, titles, captions, etc. to help with communicating your results. The functions lab() and annotate() are key here.\n\nIn addition to these key ideas, ggplot2 allows you to create faceted plots, where we can produce many panels of a plot across variables, allowing you to create many sub plots with a simple, one-line command.\nBeyond these basic ways to define your plot, ggplot2 comes with various themes that set sensible defaults for many parameters that define the look of the plot. We will explore these and how to customise them."
  },
  {
    "objectID": "02_creating-graphs.html#building-a-plot-layer-by-layer",
    "href": "02_creating-graphs.html#building-a-plot-layer-by-layer",
    "title": "2  Creating Graphs",
    "section": "2.2 Building a Plot, Layer by Layer",
    "text": "2.2 Building a Plot, Layer by Layer\nTo get started, we’ll need a data set. R itself comes with a few data sets build in that we can use for examples. However, there are more fun data sets to use in the packages we’ve already installed. We’ll use the starwars data set from dplyr.\nAs always, we’ll first load the tidyverse.\n\nlibrary(tidyverse)\n\nOnce loaded, we can load up the starwars data set from the dplyr library (one of the libraries loaded up when using library(tidyverse). Let’s take a look at it.\n\nstarwars\n\n\n\n  \n\n\n\nWe have 87 rows of data with 14 columns containing information about different characters from Star Wars.\nLet’s build our plot up, one layer at a time. Every plot made in ggplot2 must define the dataset within the ggplot() function.\n\nggplot(data = starwars)\n\n\n\n\nThat hasn’t done much. All we did was create a canvas for our data. But, this is the first step to any plot.\nNext, we’ll add our geometric objects to the plot. We’ll make a scatter plot of the mass and height of each character. To do this, we use the geom_point() function. But, we’re not done yet. The following code won’t run because we’re missing some key information. Try this yourself to see what’s wrong.\n\nggplot(data = starwars) +\n  geom_point()\n\nWe have to specify how ggplot2 should map the data we have onto the aesthetics of the plot to create the points. To do this, we need to tell it which parts of our data set should be mapped onto the relevant aesthetics of the plot. For simple points, this means we must define the key aesthetics of a point, the x and y values on the axis and where to get these in our data set.\nTry this code below, which should now work properly.\n\nggplot(data = starwars) +\n  geom_point(mapping = aes(x = mass, y = height))\n\nWarning: Removed 28 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWe have a plot, but we also got a warning stating that we have 28 rows in our data set that contain missing values. These rows with missing data are thus missing from the plot. ggplot() tells us about this missing data because it’s generally good practice to explicitly remove values with missing data before you try to plot them. Take this as a reminder that either (1) you need to remove these points prior to plotting, or (2) you can confirm that the number of missing values ggplot() tells you about matches what you’d expect from your understanding of the data.\n\n\n\n\n\n\nNote\n\n\n\nWarnings in R are there to tell you that the output of your code may not contain what you wanted it to contain. In the previous plot, ggplot dropped those with missing heights and masses, even though we didn’t explicitly tell it to do so. If you remove these ahead of plotting you won’t get a warning.\nWarnings are different to errors in that your code will still work, but you need to check out whether it did what you wanted it to do. On the other hand, errors indicate you did something wrong and your code will fail to run.\n\n\nA further thing you might notice here is the big outlier. One character has a middling height but a huge mass. We might choose to highlight this or do further investigations later. Regardless, this is a good reminder that the best thing you can do to understand your data prior to analysis is plot it.\n\n2.2.1 Aesthetics\nWhile for geom_point() the mandatory aesthetics are x and y values for plotting the points, you can also define additional aesthetics based on variables within your dataset. Let’s pick out the gender of each character.\n\n2.2.1.1 Colour\n\nggplot(data = starwars) +\n  geom_point(mapping = aes(x = mass, y = height, colour = gender))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may have noticed my plots have stopped outputting a warning. I’ve suppressed this in Quarto by using the code chunk options. You can set many options for chunks. Here, I’ve set messages such as warnings to not display. You do this by inserting your options using the hash-pipe (not the Weezer song) as follows:\n#| warning: false\n\n\n\n\n2.2.1.2 Coordinates\nWhile we’ve now picked out each dot with the colour representing gender, it’s difficult to see any real trends here due to the outlier. We can tackle this in two ways:\n\nFilter out the outlier prior to plotting the data. We’ll cover this in Transforming Data.\nChange the coordinates of the plot to only include a limited range.\n\nWe’ll go with the latter option now to show the capabilities of ggplot2 and to highlight how we can build plots up layer by layer by just adding more commands.\nTo limit the x-axis to a restricted range we need to provide two concatenated values, the start and end of the axis, to the xlim argument in the coord_cartesian() function. This function also takes ylim arguments if you’d like to limit that too.\n\nggplot(data = starwars) +\n  geom_point(mapping = aes(x = mass, y = height, colour = gender)) +\n  coord_cartesian(xlim = c(0, 180))\n\n\n\n\nNow we have a better understanding of our data and any general trends across those within a more restricted range of masses.\n\n\n\n\n\n\nWarning\n\n\n\nWe can alternatively remove data from our plot by setting limits on the x-axis using scale_x_continuous(limits = c(0, 180)). This may seem more intuitive than coord_cartesian(), but it throws out the data points outside the limits prior to plotting. This isn’t a problem in this instance, but if we want to, for example, draw a line of best fit through our data, scale_x_continuous() will ignore the outlier, while coord_catesian() will still include it. Of course, the option to include or exclude this in your estimates is a decision you have to make informed by your domain knowledge. But, being aware of what is going on in each option is crucial to make sure you make the correct inferences.\n\n\nWhat if we want to change the look of our points, but we want to apply this change to every point (i.e. not making it vary by some column in the data set). We simply have to specify some options outside the aes() call.\n\nggplot(data = starwars) +\n  geom_point(\n    mapping = \n      aes(\n        x = mass, \n        y = height, \n        colour = gender\n      ),\n    alpha = 0.7, # opacity\n    shape = \"triangle\", # triangles\n    size = 4 # bigger points\n  ) +\n  coord_cartesian(xlim = c(0, 180))\n\n\n\n\nNow we’ve set fixed values for some aesthetics, specifically setting the alpha (opacity), shape, and size of the points. Here’s a good list of the aesthetic specifications which provides a good cheat sheet to all the aesthetics you can change and their options.\nSetting an aesthetic within aes() and mapping it to a column in your data allows it to vary. Alternatively, you can define the colour to be set outside of the aesthetic to make it consistent across all data points. If you set both at the same time, the fixed aesthetic takes precedence. Give it a go.\n\nggplot(data = starwars) +\n  geom_point(\n    mapping = \n      aes(\n        x = mass, \n        y = height, \n        colour = gender\n      ),\n    alpha = 0.7, # opacity\n    shape = \"triangle\", # triangles\n    size = 4, # bigger points\n    colour = \"red\"\n  ) +\n  coord_cartesian(xlim = c(0, 180))\n\n\n\n\n\n\n2.2.1.3 Fill\nA final thing to bear in mind is that colour and fill are different properties within ggplot2. Below, we’ll change the triangles to filled circles (i.e. a with a border) and we’ll map gender to the fill aesthetic instead of colour.\n\nggplot(data = starwars) +\n  geom_point(\n    mapping = \n      aes(\n        x = mass, \n        y = height, \n        fill = gender\n      ),\n    alpha = 0.7, # opacity\n    shape = \"circle filled\", # triangles\n    size = 4, # bigger points\n    colour = \"red\"\n  ) +\n  coord_cartesian(xlim = c(0, 180))\n\n\n\n\nNotice how the colour of the circle varies, but the fill of the border is fixed?\nFinally, we can see how smart ggplot2 can be in terms of setting legends. If we use a continuous, rather than a categorical variable to define the colour of points you’ll notice that we get a gradient of colours.\n\nggplot(data = starwars) +\n  geom_point(\n    mapping = \n      aes(\n        x = mass, \n        y = height, \n        colour = birth_year\n      )\n  ) +\n  coord_cartesian(xlim = c(0, 180))\n\n\n\n\nNotice the very short and light character highlighted in light blue, indicating they are very old? Who could that be?"
  },
  {
    "objectID": "02_creating-graphs.html#geoms",
    "href": "02_creating-graphs.html#geoms",
    "title": "2  Creating Graphs",
    "section": "2.3 Geoms",
    "text": "2.3 Geoms"
  },
  {
    "objectID": "02_creating-graphs.html#facets",
    "href": "02_creating-graphs.html#facets",
    "title": "2  Creating Graphs",
    "section": "2.7 Facets",
    "text": "2.7 Facets\nAnother useful part of plotting in ggplot2 is that you can make facets of plots, or subplots. This is a good way to display your data if you have multiple categorical variables. Essentially, you’ll get a plot for each category in your data. There are two approaches to faceting with ggplot2:\n\nfacet_wrap(): Let ggplot figure out the rows and columns for you, wrapping your plot around to make the most use of the plotting area.\nfacet_grid(): Manually define variables which should be mapped onto the columns or rows of the plot.\n\nIn each case, you have some control over the number of rows and columns you’d like in the plot. Of course, with facet_wrap() this can override some of the behaviour that maximises the use of the plotting space.\n\n2.7.1 Facet Wrap\nFor facet_wrap(), we define the variables that are mapped onto columns using R’s formula notation, like so: facet_grid(. ~ variable). The dot signifies that we’re not plotting anything onto separate rows, while the variable name to the right of the ~ (read: tilde) denotes the variable you’d like to split your plots by, plotting onto separate columns.\n\nggplot(data = starwars, mapping = aes(x = height)) +\n  geom_histogram(fill = \"white\", colour = \"black\") +\n  facet_wrap(. ~ gender)\n\n\n\n\n\n\n2.7.2 Facet Grid\nFor facet_grid(), we define the variables that are mapped onto columns or rows using R’s formula notation, like so: facet_grid(rows ~ columns).\n\nggplot(data = starwars, mapping = aes(x = height)) +\n  geom_histogram(fill = \"white\", colour = \"black\") +\n  facet_grid(eye_color ~ gender)\n\n\n\n\nIn this case, we have limited data for each combination of the variables, so many panels are empty. However, this still gives you a good idea of both the data set and the functionality of facet_grid().\nBy default ggplot2 sets a consistent y-axis range across all facets. However, if you’d like the scales to vary within each facet you have some flexibility:\n\nscales = \"fixed\": Both x and y axis have fixed scales across all facets.\nscales = \"free\": Both x and y axis have different scales defined by the range of the data in each facet.\nscales = \"free_x\" or scales = \"free_y\": Either the x or y axis have free scales respectively, with the other scale set to fixed."
  },
  {
    "objectID": "02_creating-graphs.html#customisation",
    "href": "02_creating-graphs.html#customisation",
    "title": "2  Creating Graphs",
    "section": "2.6 Customisation",
    "text": "2.6 Customisation\nOnce we’ve created the elements of the plot using the default settings, we often want to customise it to better present some elements of the plot, or simply to make the plot our own.\n\n2.6.1 Labels\nThe most basic way to do this is to first change the labels. To do this, we use the labs() function where we can specify various labels such as:\n\nx: the label for the x-axis.\ny: the label for the y-axis.\ntitle: the title of the plot.\ncaption: a caption in the bottom right of the plot.\ncolour: the heading for the legend if picked out by colour.\n\n\nggplot(data = starwars, mapping = aes(x = gender, fill = sex)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    x = \"Gender\",\n    y = \"Count\",\n    title = \"Counts of Each Gender\",\n    caption = \"A little customisation.\"\n  )\n\n\n\n\n\n\n2.6.2 Scales and Breaks\nWhile ggplot2 attempts to find sensible scales for the plot, sometimes you might want to restrict this or set a different number of breaks. For continuous scales, we can set this using scale_x_continuous() and scale_y_continuous(). For discrete scales, it’s unsurprisingly, scale_x_discrete() and scale_y_discrete().\nWithin these functions we define both where breaks occurs and what the limits of the axes are. Let’s set a wider range on the scale for our previous plot and add additional breaks. We can make these using the seq() function that will give us a sequence of numbers counting by whatever we want to use. We can then set the upper and lower limit of the plotting region.\n\nggplot(data = starwars, mapping = aes(x = gender, fill = sex)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    x = \"Gender\",\n    y = \"Count\",\n    title = \"Counts of Each Gender\",\n    caption = \"A little customisation.\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 80, by = 10), limits = c(0, 80))\n\n\n\n\n\n\n2.6.3 Colours\nOften the default colours in ggplot2 are a little garish and not particularly colour-blind friendly. Thankfully, there are a number of packages such as RColorBrewer and even MetBrewer (themes based on art in the Metropolitan Museum of Art in New York which are seriously good) which can help us. Alternatively, we can specify colours using hex codes. We’ll use these here as it means we don’t need an additional package to manage. In this case, we take advantage of scale_fill_manual() to specify a fill for each value we have for the levels assigned to the colour variable in our data set. In this case, that’s 5 colours.\n\nggplot(data = starwars, mapping = aes(x = gender, fill = sex)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    x = \"Gender\",\n    y = \"Count\",\n    title = \"Counts of Each Gender\",\n    caption = \"A little customisation.\"\n  ) +\n  scale_fill_manual(values = c(\n    \"#efc86e\", \n    \"#97c684\", \n    \"#6f9969\", \n    \"#808fe1\", \n    \"#5c66a8\"\n  ))\n\n\n\n\nIf we instead used colour to pick out the sexes here, we would have to change the function to scale_colour_manual().\n\n\n2.6.4 Themes\nThe ggplot2 theme is iconic and easy to recognise. But if you want your plots to stand out, you’ll often want to use a different theme or even tweak and existing theme. There are many themes that come with ggplot2. We’ll use a pretty basic one here, theme_bw(). As with all ggplot2 functions, we just add this to our chain of functions.\n\nggplot(data = starwars, mapping = aes(x = gender, fill = sex)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    x = \"Gender\",\n    y = \"Count\",\n    title = \"Counts of Each Gender\",\n    caption = \"A little customisation.\"\n  ) +\n  scale_fill_manual(values = c(\n    \"#efc86e\", \n    \"#97c684\", \n    \"#6f9969\", \n    \"#808fe1\", \n    \"#5c66a8\"\n  )) +\n  theme_bw()\n\n\n\n\nWe might change or remove additional elements by editing the theme further beyond this by using the theme() function. Note that this must come after you choose a pre-made theme or the pre-made theme’s settings will overwrite your own.\n\nggplot(data = starwars, mapping = aes(x = gender, fill = sex)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    x = \"Gender\",\n    y = \"Count\",\n    title = \"Counts of Each Gender\",\n    caption = \"A little customisation.\"\n  ) +\n  scale_fill_manual(values = c(\n    \"#efc86e\", \n    \"#97c684\", \n    \"#6f9969\", \n    \"#808fe1\", \n    \"#5c66a8\"\n  )) +\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.text = element_text(size = 16)\n  )\n\n\n\n\nNotice here that we removed elements from the theme using element_blank() and we changed text elements by first using element_text() within which we can define the parameters we want to change, such as the font face or size.\n\n2.6.4.1 Legend Position\nFinally, the legend can be put in either absolute positions on the plot or in a relative position (e.g. “top” or “bottom”). We can set this again within the theme options.\nFirst, by defining the position within the plot:\n\nggplot(data = starwars, mapping = aes(x = gender, fill = sex)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    x = \"Gender\",\n    y = \"Count\",\n    title = \"Counts of Each Gender\",\n    caption = \"A little customisation.\"\n  ) +\n  scale_fill_manual(values = c(\n    \"#efc86e\", \n    \"#97c684\", \n    \"#6f9969\", \n    \"#808fe1\", \n    \"#5c66a8\"\n  )) +\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.text = element_text(size = 16),\n    legend.position = c(.90, .70)\n  )\n\n\n\n\nSecond, by defining its relative position:\n\nggplot(data = starwars, mapping = aes(x = gender, fill = sex)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    x = \"Gender\",\n    y = \"Count\",\n    title = \"Counts of Each Gender\",\n    caption = \"A little customisation.\"\n  ) +\n  scale_fill_manual(values = c(\n    \"#efc86e\", \n    \"#97c684\", \n    \"#6f9969\", \n    \"#808fe1\", \n    \"#5c66a8\"\n  )) +\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.text = element_text(size = 16),\n    legend.position = \"top\"\n  )\n\n\n\n\nClearly, these plots still have a long way to go, but you get the idea about how much control we have."
  },
  {
    "objectID": "02_creating-graphs.html#combining-plots",
    "href": "02_creating-graphs.html#combining-plots",
    "title": "2  Creating Graphs",
    "section": "2.8 Combining Plots",
    "text": "2.8 Combining Plots\nOften, you might several plots that serve different data visualisation needs, but you want to display these together in one place. If using Quatro, you can choose to save your plots externally (e.g. to a .png file) and then load them back into Quarto, using the options to display subfigures to display plots side by side. However, you may want to just display these plots in your document from R code without saving the plots (but still retaining this option if you choose). For this, we can use the patchwork package. To use this package, we have to load it up like any other package prior to using it.\n\nlibrary(patchwork)\n\nWe can assign our plots to variables just like any other object in R. Let’s create two plots, my_histogram and my_dotplot using the following code:\n\nmy_histogram <- ggplot(data = starwars, mapping = aes(x = height)) +\n  geom_histogram(fill = \"white\", colour = \"black\")\n\nmy_dotplot <- ggplot(data = starwars, mapping = aes(x = height, y = mass)) +\n  geom_point()\n\nThen we can simply add the plots together to display them side by side.\n\nmy_histogram + my_dotplot\n\n\n\n\nWe can make relatively complicated layouts by nesting plots together using () and defining separate rows using the /. First, let’s make one final before we make a 3 plot output with 2 rows.\n\nmy_boxplot <- ggplot(\n    data = starwars, \n    mapping = aes(x = gender, y = mass, colour = gender)\n  ) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 200))\n\nNow, let’s add all of this together:\n\n(my_histogram | my_dotplot) / my_boxplot"
  },
  {
    "objectID": "02_creating-graphs.html#saving-plots",
    "href": "02_creating-graphs.html#saving-plots",
    "title": "2  Creating Graphs",
    "section": "2.9 Saving Plots",
    "text": "2.9 Saving Plots\nFinally, once you’ve created these plots you might want to save them for further use beyond R and Quarto documents. To do this, we use the ggsave() function which expects us to define a file name (optionally set at a specific path). Note that you need to include the file type. Often, you will want to save your plot as a .png file as these can be small, lightweight, allow transparency, and compress relatively well with little artifacting (cf. .jpegs). Alternatively, you can use a vectorised format (e.g. .svg or .pdf) that scales up and down to different sizes while maintaining the integrity of the plot even better than .png files.\nNext, ggsave() expects the name of the object you saved your plot to, or the last plot you created. I suggest you’re always explicit with this: Once happy with a plot, assign it to an object and save it by specifying the name of the plot. That way, you know you’ll have saved the correct version of your plot.\n\nggsave(\n  filename = \"my_plot.png\",\n  plot = my_histogram\n)"
  },
  {
    "objectID": "02_creating-graphs.html#advanced-functionality",
    "href": "02_creating-graphs.html#advanced-functionality",
    "title": "2  Creating Graphs",
    "section": "2.10 Advanced Functionality",
    "text": "2.10 Advanced Functionality\nThere’s a lot more we can do in ggplot beyond what we’ve covered here. But these basics cover the most important use cases you’ll come across. You now understand how to build plots, what options there are for different aesthetics and geometric shapes, how to change positioning of elements, how to create subplots, and how to combine and save your plots.\nBeyond this, you might want to explore using the stat_summary() functions in ggplot, which allow you to aggregate data and create statistics such as standard errors and 95% confidence intervals from your data. However, in later sessions we will explore how to get these values from fitted model objects and create plots directly from these values, obviating the need for calculating these statistics in ggplot.\nOnce you understand that, you might want to check out modern plotting approaches such as creating raincloud plots to show the raw data, density of data, central tendency and confidence intervals together in one attractive package.\n\n\n\n\nCorrell, Michael, and Michael Gleicher. 2014. “Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error.” IEEE Transactions on Visualization and Computer Graphics 20 (12): 2142–51. https://doi.org/10.1109/TVCG.2014.2346298.\n\n\nKang, Hyunmin, Jeayeong Ji, Yeji Yun, and Kwanghee Han. 2021. “Estimating Bar Graph Averages: Overcoming Within-the-Bar Bias.” I-Perception 12 (1): 204166952098725. https://doi.org/10.1177/2041669520987254.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n\n\nXiong, Cindy, Cristina R. Ceja, Casimir J. H. Ludwig, and Steven Franconeri. 2020. “Biased Average Position Estimates in Line and Bar Graphs: Underestimation, Overestimation, and Perceptual Pull.” IEEE Transactions on Visualization and Computer Graphics 26 (1): 301–10. https://doi.org/10.1109/TVCG.2019.2934400."
  },
  {
    "objectID": "02_creating-graphs.html#exploring-geoms",
    "href": "02_creating-graphs.html#exploring-geoms",
    "title": "2  Creating Graphs",
    "section": "2.3 Exploring Geoms",
    "text": "2.3 Exploring Geoms\nThere are many geoms that you can use in ggplot2. Some common ones you might use are bars (geom_bar()), boxes (geom_boxplot()), violins (geom_violin()), densities (geom_density()), and histograms (geom_histogram()). Let’s briefly see how we might use each geom.\n\n2.3.1 Bars\nBars are best used to indicate counts. They are often used in Psychology along with error bars to indicate means and 95% confidence intervals or standard errors. However, these aren’t the best way to display continuous data as it (1) hides any information about the distribution of data (e.g. groupings) that may be masked by a bar plot with error bars, and (2) anchor people to the top of the bar such that under or overestimate the magnitude of effects and the potential for dispersion (Kang et al. 2021; Xiong et al. 2020; Correll and Gleicher 2014). So, let’s just use them for counts!\n\nggplot(data = starwars) +\n  geom_bar(mapping = aes(x = gender))\n\n\n\n\n\n\n2.3.2 Boxplots\nBoxplots do a nice job of visualising central tendency (specifically the median) and dispersion (specifically the interquartile range), so they are a good choice for looking at continuous outcomes grouped by a categorical measure. Let’s make a boxplot for each gender looking at heights.\n\nggplot(data = starwars) +\n  geom_boxplot(mapping = aes(x = gender, y = height))\n\n\n\n\nHow should you interpret this?\n\nThe middle line represents the median\nThe upper white section of the box the upper quartile: 75% of scores fall below this.\nThe lower white section the lower quartile: 25% of scores fall above this.\nTogether the quartiles represent the interquartile range: The middle 50% of scores.\nThe limits of the whiskers (black lines) for the upper and lower parts of the graph represent the smallest and largest observations that are equal to the upper or lower quartiles minus or plus 1.5 times the interquartile range. Effectively, this is most of the rest of the data, apart from outliers.\nThe dots represent outliers (i.e. those values outside of the whiskers).\n\n\n\n2.3.3 Violins\nViolin plots show you the density of the scores. The wider the section of the violin, the more scores around that area. We set trim to FALSE within the violin plot so that we see the full tails of the data. If we set this to TRUE, then the tails are trimmed to the range of the data.\nIt can be useful to draw quantiles on the violin plot so they can communicate similar information as a box plot. To do this, set draw_quantiles to concatenated values indicating the quartiles you’re interested in. Here, we chose the upper and lower 25% along with the median.\n\nggplot(data = starwars) +\n  geom_violin(\n    mapping = aes(x = gender, y = height),\n    trim = FALSE,\n    draw_quantiles = c(0.25, 0.5, 0.75)\n  )\n\n\n\n\n\n\n2.3.4 Histograms\nOften, you want to get a general idea of the distribution of your data with the aim of looking at skewness and kurtosis. One way to do this is with a histogram. This bins values into a specific range on your x-axis and looks at counts for observations within each bin. By default, ggplot2 selects sensible bins for us, in this case bins incrementing by 30cm in height. But, you can change this by setting binwidth to whatever value you like.\n\nggplot(data = starwars) +\n  geom_histogram(\n    mapping = aes(x = height), \n    fill = \"white\",\n    colour = \"black\"\n  )\n\n\n\n\nFrom the histogram we can see that the more common heights lie between 150 and 200cm, but we have a fair spread of scores with a few characters much shorter and larger than this.\n\n\n2.3.5 Density Plots\nDensity plots work like histograms but apply kernel smoothing to create a density line that approximates the shape of the histogram. This can often be easier to interpret than a histogram as it smooths out the noise in the histogram to make looking at general trends a little easier.\nAs with other geoms in ggplot2 we can customise our plot by picking out categories in different colours, and even changing the opacity of the plots if we’re concerned with overplotting. Let’s look at the densities of the heights by each gender.\n\nggplot(data = starwars) +\n  geom_density(\n    mapping = aes(x = height, fill = gender), \n    alpha = 0.5\n  )\n\n\n\n\nWe can see that the distribution of heights is different between genders. Due to the small number of observations where gender is NA (i.e. droids in this data set) we can see this density is very narrow with a large peak.\n\n\n2.3.6 Smooths\nIf we want to add a line of best fit to a plot, we can either define this line manually, perhaps from some values from a fitted model object, or allow ggplot to do this for us using one of many methods. This relies on ggplot calculating statistics for us based on our data and adding a line of best fit to the plot. To do this, we use the geom_smooth() function. The following methods are available to us:\n\n\"lm\": the linear model fitted using stats::lm(), fitting a straight line between the values on the x and y axis to find the line of best fit.\n\"loess\": locally estimated scatterplot smoothing fitted using stats::loess(), a nonparametric method to find the line of best fit in non-linear data by fitting many local regressions within the series of data.\n\"gam\": generalised additive model fitted using mgcv::gam(), a nonparametric method to find the line of best fit in non-linear data by using a series of basis splines.\n\nFor our purposes, we’ll just rely on “lm” to draw a line of best fit through our data.\nNote, that we used scale_x_continuous() with a restricted range to remove Jabba the Hutt from our data set as an outlier. Remember, unlike coord_cartesian(), scale_x_continuous() removed the data point entirely, meaning our estimate of the line of best fit will be adjusted.\n\nggplot(data = starwars) +\n  scale_x_continuous(limits = c(0, 200)) +\n  geom_smooth(mapping = aes(x = mass, y = height), method = \"lm\")\n\n\n\n\nggplot2 will attempt to use a sensible default for the formula used to fit our model. Here, it set the formula to y ~ x as in the values on the y-axis are predicted by values on the x-axis. You can, however, specify an alternative formula by including a string to the formula argument. For example, if we wanted to predict the log of values on the y-axis, we might use formula = log(y) ~ x.\nIf none of this made sense, then don’t worry. We will turn to model fitting and estimation in later chapters."
  },
  {
    "objectID": "02_creating-graphs.html#combining-geoms",
    "href": "02_creating-graphs.html#combining-geoms",
    "title": "2  Creating Graphs",
    "section": "2.4 Combining Geoms",
    "text": "2.4 Combining Geoms\nWe can easily combine geoms in ggplot2 by simply adding another layer to our plot. Let’s create a dotplot and add the line of best fit to the data.\n\nggplot(data = starwars) +\n  scale_x_continuous(limits = c(0, 200)) +\n  geom_point(mapping = aes(x = mass, y = height)) +\n  geom_smooth(mapping = aes(x = mass, y = height), method = \"lm\")\n\n\n\n\nNote that there’s a bit of repetition here. We’ve specified the x and y elements in each geom, but they’re just the same. If this happens, we can specify them during the initial ggplot() call and allow these to be inherited by the geoms.\n\nggplot(data = starwars, mapping = aes(x = mass, y = height)) +\n  scale_x_continuous(limits = c(0, 200)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "02_creating-graphs.html#positioning",
    "href": "02_creating-graphs.html#positioning",
    "title": "2  Creating Graphs",
    "section": "2.5 Positioning",
    "text": "2.5 Positioning\nWhile ggplot2 has many sensible defaults for plotting data, often we want more granular control over the positioning of the elements of a plot. There are a few different positions we can specify. Most often, however, if you want to change the position of an element you want to jitter elements by some amount to avoid overplotting or stop bar plots from stacking and instead set them side by side. We’ll briefly explore each method here.\nOften, with scatter plots it’s a better idea to use opacity to help with overplotting as adding jitter clearly influences the interpretation of the plot. But sometimes this still doesn’t help. In these cases, adding some jitter (and making this clear to the reader) is appropriate. Compare the two plots below.\n\n2.5.1 Jitter\n\nggplot(data = starwars, mapping = aes(x = gender, y = height)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\nggplot(data = starwars, mapping = aes(x = gender, y = height)) +\n  geom_point(alpha = 0.5, position = \"jitter\")\n\n\n\n\n\n\n2.5.2 Dodge\nWith bar plots we often want to display more than just one variable. To do this, we may choose to pick out the bars on the x-axis in colour.\n\nggplot(data = starwars, mapping = aes(x = gender, fill = sex)) +\n  geom_bar()\n\n\n\n\nBy default, ggplot2 using the “identity” position, where bars are stacked on top of each other within categories. Sometimes this is what we want if we care about the breakdown within the groups on the x-axis. However, if we want to compare absolute values across all subgroups, we need to dodge the bars. We do this using the position = \"dodge\" argument.\n\nggplot(data = starwars, mapping = aes(x = gender, fill = sex)) +\n  geom_bar(position = \"dodge\")"
  },
  {
    "objectID": "03_transforming-data.html#read_-functions",
    "href": "03_transforming-data.html#read_-functions",
    "title": "3  Transforming Data",
    "section": "4.1 read_*() functions",
    "text": "4.1 read_*() functions\n\n\n\nThe readr package allows you to read rectangular data into R from delimited files (e.g. .csv, .tsv)\n\nread_csv(): comma-separated values (CSV)\nread_tsv(): tab-separated values (TSV)\nread_csv2(): semicolon-separated values with , as the decimal mark\nread_delim(): delimited files (CSV and TSV are important special cases)\nread_table(): whitespace-separated files\n\n…and others.\n\nIf you’re working proprietary data formats, use haven which has:\n\nread_sav():\nread_dta:\nread_sas():\n\nIf using Excel, use readxl which has the read_excel() function.\n\n\nAll of these functions take the format read_*() and expect the path to your data as input"
  },
  {
    "objectID": "03_transforming-data.html#reading-our-data",
    "href": "03_transforming-data.html#reading-our-data",
    "title": "3  Transforming Data",
    "section": "4.2 Reading Our Data",
    "text": "4.2 Reading Our Data\n\nFirst we need some data. I made a messy version of real data from Dunne et al. (2023): Uncovering the social determinants of brain injury rehabilitation.\nThis data is stored as a .csv file in https://github.com/gpwilliams/ds-psych_course\nTo follow along, download the repository from GitHub and open the exercises file in 03_transforming-data or make your own Quarto document within the folder."
  },
  {
    "objectID": "03_transforming-data.html#reading-our-data-1",
    "href": "03_transforming-data.html#reading-our-data-1",
    "title": "3  Transforming Data",
    "section": "4.3 Reading Our Data",
    "text": "4.3 Reading Our Data\nLoad the packages once per session.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(lubridate)\n\nAssuming you’re working from the ds-psych_course folder, read the data from the data subfolder.\n\nraw_data <- read_csv(here(\"data\", \"bi-loneliness.csv\"))\n\n\n\nRows: 29 Columns: 45\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (32): PID, time, progress, Gender, Region, Marital Status, Living Arrang...\ndbl (13): Age, BRS1, BRS2, BRS3, BRS4, BRS5, BRS6, DJG_1, DJG_2, DJG_3, DJG_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou will get a message telling you about the data types that have been detected for each column."
  },
  {
    "objectID": "03_transforming-data.html#inspecting-your-data",
    "href": "03_transforming-data.html#inspecting-your-data",
    "title": "3  Transforming Data",
    "section": "4.2 Inspecting Your Data",
    "text": "4.2 Inspecting Your Data\nWith any data set, it’s important to understand the columns. Either print it out by typing the name of the data set, here raw_data in the console, or print the transposed data set with glimpse()\n\nglimpse(raw_data)\n\nRows: 29\nColumns: 45\n$ PID                   <chr> \"26\", \"27\", \"28\", \"29\", \"30\", \"A2\", \"A3\", \"C\", \"…\n$ time                  <chr> \"2018-03-22 23:06:11_2018-03-23 00:25:51\", \"2018…\n$ progress              <chr> \"FINISH\", \"no\", \"END\", \"ethics\", \"ethics\", \"END\"…\n$ Gender                <chr> \"M\", \"M\", \"F\", \"M\", \"M\", \"male\", \"M\", \"M\", \"M\", …\n$ Age                   <dbl> 54, 54, 37, 51, 52, 39, 47, 53, 68, 50, 46, 55, …\n$ Region                <chr> \"East\", \"Scotland\", \"South East\", \"West Yorkshir…\n$ `Marital Status`      <chr> \"Married\", \"Married\", \"Married\", \"Single\", \"Marr…\n$ `Living Arrangements` <chr> \"With wife\", \"With wife\", \"With husband and todd…\n$ `Primary Carer`       <chr> \"None\", \"None\", \"None\", \"None\", \"Wife\", \"None\", …\n$ Employment            <chr> \"Medically Retired\", \"Medically Retired\", \"Long …\n$ `Time Since BI`       <chr> \"6 (2015)\", \"3 (2018)\", \"5 (2016)\", \"23 (1998)\",…\n$ `BI Severity`         <chr> \"Mild\", \"Severe\", \"Mild\", \"Moderate\", \"Severe\", …\n$ Vision                <chr> NA, NA, \"X\", NA, \"X\", \"x\", NA, NA, \"X\", NA, NA, …\n$ Speech                <chr> \"X\", \"X\", NA, NA, NA, \"x\", NA, NA, \"X\", \"X\", \"X\"…\n$ Motor                 <chr> NA, \"X\", \"X\", NA, NA, \"x\", \"X\", NA, \"X\", \"X\", \"X…\n$ Memory                <chr> NA, NA, NA, NA, \"X\", NA, \"X\", NA, \"X\", \"X\", NA, …\n$ Cognitive             <chr> \"X\", \"X\", NA, NA, \"X\", NA, NA, \"X\", NA, \"X\", NA,…\n$ Pain                  <chr> NA, NA, \"X\", NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ BRS1                  <dbl> 5, 4, 3, 4, 2, 5, 5, 4, 5, 5, 4, 4, 2, 4, NA, 4,…\n$ BRS2                  <dbl> 4, 4, 2, 4, 1, 5, 5, 1, 4, 4, 2, 2, 2, 1, NA, 2,…\n$ BRS3                  <dbl> 3, 4, 1, 4, 5, 2, 5, 4, 4, 5, 2, 2, 2, 2, NA, 5,…\n$ BRS4                  <dbl> 4, 4, 2, 4, 2, 5, 5, 5, 4, 4, 4, 4, 2, 4, NA, 4,…\n$ BRS5                  <dbl> 4, 2, 2, 3, 1, 4, 5, 4, 4, 3, 4, 4, 2, 2, NA, 4,…\n$ BRS6                  <dbl> 5, 2, 1, 3, 2, 4, 5, 5, 4, 4, 2, 5, 3, 5, NA, 4,…\n$ DJG_1                 <dbl> 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, NA, 1,…\n$ DJG_2                 <dbl> 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, NA, 1,…\n$ DJG_3                 <dbl> 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1,…\n$ DJG_4                 <dbl> 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, NA, 0,…\n$ DJG_5                 <dbl> 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, NA, 1,…\n$ DJG_6                 <dbl> 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, NA, 0,…\n$ `WEMWBS 1`            <chr> \"2 - Rarely\", \"3 - Some of the time\", \"3 - Some …\n$ `WEMWBS 2`            <chr> \"4 - Often\", \"2 - Rarely\", \"2 - Rarely\", \"2 - Ra…\n$ `WEMWBS 3`            <chr> \"4 - Often\", \"3 - Some of the time\", \"2 - Rarely…\n$ `WEMWBS 4`            <chr> \"3 - Some of the time\", \"4 - Often\", \"1 - None o…\n$ `WEMWBS 5`            <chr> \"1 - None of the time\", \"2 - Rarely\", \"1 - None …\n$ `WEMWBS 6`            <chr> \"5 - All of the time\", \"3 - Some of the time\", \"…\n$ `WEMWBS 7`            <chr> \"4 - Often\", \"3 - Some of the time\", \"4 - Often\"…\n$ `WEMWBS 8`            <chr> \"3 - Some of the time\", \"2 - Rarely\", \"2 - Rarel…\n$ `WEMWBS 9`            <chr> \"2 - Rarely\", \"3 - Some of the time\", \"3 - Some …\n$ `WEMWBS 10`           <chr> \"3 - Some of the time\", \"2 - Rarely\", \"2 - Rarel…\n$ `WEMWBS 11`           <chr> \"5 - All of the time\", \"3 - Some of the time\", \"…\n$ `WEMWBS 12`           <chr> \"3 - Some of the time\", \"3 - Some of the time\", …\n$ `WEMWBS 13`           <chr> \"1 - None of the time\", \"2 - Rarely\", \"1 - None …\n$ `WEMWBS 14`           <chr> \"3 - Some of the time\", \"2 - Rarely\", \"3 - Some …\n$ enjoymentFOLLOW       <chr> \"7-no\", \"6-yes\", \"4-yes\", \"0-no\", \"4-no\", \"5-no\"…\n\n\nThat’s a lot of columns! Broadly, this data set has one row of data for each participant and tracks some of the demographics of the participants. As the data set concerns brain injury survivors, we also have information on the traumatic brain injuries of the participants, including the time since the brain injury and severity of brain injury. Along with this we have columns coding for types of impairments or disturbances as a result of brain injury (i.e. visual, speech, motor, memory, or cognitive, pain). Finally, we have ratings for two questionnaires, the BRS (Brief Resilience Scale), DJG (De Jong Gierveld scale for emotional and social loneliness), and WEMWBS (The Warwick-Edinburgh Mental Wellbeing Scales). We also have a closing question in one column that encodes two variables: whether they enjoyed the study and if they would be contactable in a follow up. We need to fix a number of issues with this data.\nAt this stage it is a good idea to take some time to really understand our data. What are the unique values in each column? What are the issues with the presentation that might make working with the data difficult? Using plots is a good way to visually inspect each variable."
  },
  {
    "objectID": "03_transforming-data.html#a-note-on-pipes",
    "href": "03_transforming-data.html#a-note-on-pipes",
    "title": "3  Transforming Data",
    "section": "3.2 A Note on Pipes",
    "text": "3.2 A Note on Pipes\nIn R, we can combine several functions to produce a result using nested code. For example, to get get counts of unique participant IDs we can combine the unique() and length() functions as follows:\n\nlength(unique(raw_data$PID))\n\n[1] 26\n\n\nThis is fine, but can get complicated to read as you have to read in to out. The more functions we chain together the harder it can get to read the code. Instead, we can use the pipe. This allows us to write code and read it left to right. Here’s how we might use the pipe to complete the same operation as above:\n\nraw_data$PID |> \n  unique() |> \n  length()\n\n[1] 26\n\n\nThe pipe can be read as “and then”, so take our PID column from raw_data and then get the unique values and then get the length (or count) of them.\nWe will use pipes throughout this book as we now start to chain together many functions."
  },
  {
    "objectID": "03_transforming-data.html#separate",
    "href": "03_transforming-data.html#separate",
    "title": "3  Transforming Data",
    "section": "5.1 separate()",
    "text": "5.1 separate()\n\nLet’s fix the messy data, starting with splitting columns that contain multiple variables.\nWe now have start_time and end_time columns and enjoyment and follow_up columns.\n\n\nraw_data <- raw_data |> \n  separate(col = enjoymentFOLLOW, into = c(\"enjoyment\", \"follow_up\")) |> \n  separate(col = time, into = c(\"start_time\", \"end_time\"), sep = \"_\")\n\nglimpse(raw_data)\n\nRows: 29\nColumns: 47\n$ PID                   <chr> \"26\", \"27\", \"28\", \"29\", \"30\", \"A2\", \"A3\", \"C\", \"…\n$ start_time            <chr> \"2018-03-22 23:06:11\", \"2018-03-26 00:30:20\", \"2…\n$ end_time              <chr> \"2018-03-23 00:25:51\", \"2018-03-26 02:15:52\", \"2…\n$ progress              <chr> \"FINISH\", \"no\", \"END\", \"ethics\", \"ethics\", \"END\"…\n$ Gender                <chr> \"M\", \"M\", \"F\", \"M\", \"M\", \"male\", \"M\", \"M\", \"M\", …\n$ Age                   <dbl> 54, 54, 37, 51, 52, 39, 47, 53, 68, 50, 46, 55, …\n$ Region                <chr> \"East\", \"Scotland\", \"South East\", \"West Yorkshir…\n$ `Marital Status`      <chr> \"Married\", \"Married\", \"Married\", \"Single\", \"Marr…\n$ `Living Arrangements` <chr> \"With wife\", \"With wife\", \"With husband and todd…\n$ `Primary Carer`       <chr> \"None\", \"None\", \"None\", \"None\", \"Wife\", \"None\", …\n$ Employment            <chr> \"Medically Retired\", \"Medically Retired\", \"Long …\n$ `Time Since BI`       <chr> \"6 (2015)\", \"3 (2018)\", \"5 (2016)\", \"23 (1998)\",…\n$ `BI Severity`         <chr> \"Mild\", \"Severe\", \"Mild\", \"Moderate\", \"Severe\", …\n$ Vision                <chr> NA, NA, \"X\", NA, \"X\", \"x\", NA, NA, \"X\", NA, NA, …\n$ Speech                <chr> \"X\", \"X\", NA, NA, NA, \"x\", NA, NA, \"X\", \"X\", \"X\"…\n$ Motor                 <chr> NA, \"X\", \"X\", NA, NA, \"x\", \"X\", NA, \"X\", \"X\", \"X…\n$ Memory                <chr> NA, NA, NA, NA, \"X\", NA, \"X\", NA, \"X\", \"X\", NA, …\n$ Cognitive             <chr> \"X\", \"X\", NA, NA, \"X\", NA, NA, \"X\", NA, \"X\", NA,…\n$ Pain                  <chr> NA, NA, \"X\", NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ BRS1                  <dbl> 5, 4, 3, 4, 2, 5, 5, 4, 5, 5, 4, 4, 2, 4, NA, 4,…\n$ BRS2                  <dbl> 4, 4, 2, 4, 1, 5, 5, 1, 4, 4, 2, 2, 2, 1, NA, 2,…\n$ BRS3                  <dbl> 3, 4, 1, 4, 5, 2, 5, 4, 4, 5, 2, 2, 2, 2, NA, 5,…\n$ BRS4                  <dbl> 4, 4, 2, 4, 2, 5, 5, 5, 4, 4, 4, 4, 2, 4, NA, 4,…\n$ BRS5                  <dbl> 4, 2, 2, 3, 1, 4, 5, 4, 4, 3, 4, 4, 2, 2, NA, 4,…\n$ BRS6                  <dbl> 5, 2, 1, 3, 2, 4, 5, 5, 4, 4, 2, 5, 3, 5, NA, 4,…\n$ DJG_1                 <dbl> 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, NA, 1,…\n$ DJG_2                 <dbl> 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, NA, 1,…\n$ DJG_3                 <dbl> 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1,…\n$ DJG_4                 <dbl> 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, NA, 0,…\n$ DJG_5                 <dbl> 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, NA, 1,…\n$ DJG_6                 <dbl> 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, NA, 0,…\n$ `WEMWBS 1`            <chr> \"2 - Rarely\", \"3 - Some of the time\", \"3 - Some …\n$ `WEMWBS 2`            <chr> \"4 - Often\", \"2 - Rarely\", \"2 - Rarely\", \"2 - Ra…\n$ `WEMWBS 3`            <chr> \"4 - Often\", \"3 - Some of the time\", \"2 - Rarely…\n$ `WEMWBS 4`            <chr> \"3 - Some of the time\", \"4 - Often\", \"1 - None o…\n$ `WEMWBS 5`            <chr> \"1 - None of the time\", \"2 - Rarely\", \"1 - None …\n$ `WEMWBS 6`            <chr> \"5 - All of the time\", \"3 - Some of the time\", \"…\n$ `WEMWBS 7`            <chr> \"4 - Often\", \"3 - Some of the time\", \"4 - Often\"…\n$ `WEMWBS 8`            <chr> \"3 - Some of the time\", \"2 - Rarely\", \"2 - Rarel…\n$ `WEMWBS 9`            <chr> \"2 - Rarely\", \"3 - Some of the time\", \"3 - Some …\n$ `WEMWBS 10`           <chr> \"3 - Some of the time\", \"2 - Rarely\", \"2 - Rarel…\n$ `WEMWBS 11`           <chr> \"5 - All of the time\", \"3 - Some of the time\", \"…\n$ `WEMWBS 12`           <chr> \"3 - Some of the time\", \"3 - Some of the time\", …\n$ `WEMWBS 13`           <chr> \"1 - None of the time\", \"2 - Rarely\", \"1 - None …\n$ `WEMWBS 14`           <chr> \"3 - Some of the time\", \"2 - Rarely\", \"3 - Some …\n$ enjoyment             <chr> \"7\", \"6\", \"4\", \"0\", \"4\", \"5\", \"0\", \"6\", \"1\", \"4\"…\n$ follow_up             <chr> \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\"…"
  },
  {
    "objectID": "03_transforming-data.html#selecting-columns-to-keep",
    "href": "03_transforming-data.html#selecting-columns-to-keep",
    "title": "3  Transforming Data",
    "section": "6.1 Selecting Columns to Keep",
    "text": "6.1 Selecting Columns to Keep\n\nraw_data |> \n  select(\n    PID,\n    Gender,\n    Age\n  )\n\n# A tibble: 29 × 3\n   PID   Gender   Age\n   <chr> <chr>  <dbl>\n 1 26    M         54\n 2 27    M         54\n 3 28    F         37\n 4 29    M         51\n 5 30    M         52\n 6 A2    male      39\n 7 A3    M         47\n 8 C     M         53\n 9 D     M         68\n10 I     M         50\n# … with 19 more rows"
  },
  {
    "objectID": "03_transforming-data.html#selecting-columns-to-keep-1",
    "href": "03_transforming-data.html#selecting-columns-to-keep-1",
    "title": "3  Transforming Data",
    "section": "6.2 Selecting Columns to Keep",
    "text": "6.2 Selecting Columns to Keep\nOften your data set has lots of columns you won’t use. We can choose which columns to keep by listing them by index.\n\nraw_data |> \n  select(c(1, 5, 6))\n\n# A tibble: 29 × 3\n   PID   Gender   Age\n   <chr> <chr>  <dbl>\n 1 26    M         54\n 2 27    M         54\n 3 28    F         37\n 4 29    M         51\n 5 30    M         52\n 6 A2    male      39\n 7 A3    M         47\n 8 C     M         53\n 9 D     M         68\n10 I     M         50\n# … with 19 more rows"
  },
  {
    "objectID": "03_transforming-data.html#selecting-columns-to-remove",
    "href": "03_transforming-data.html#selecting-columns-to-remove",
    "title": "3  Transforming Data",
    "section": "6.3 Selecting Columns to Remove",
    "text": "6.3 Selecting Columns to Remove\nWe can select columns to remove by using - before the names or index. We can list these as between a range, e.g. 6:47 or c(Age:follow_up), or as a specific vector of indices, e.g. c(1, 2, 3) or c(PID, start_time, end_time)\n\nraw_data |> \n  select(-c(Age:follow_up))\n\n# A tibble: 29 × 5\n   PID   start_time          end_time            progress Gender\n   <chr> <chr>               <chr>               <chr>    <chr> \n 1 26    2018-03-22 23:06:11 2018-03-23 00:25:51 FINISH   M     \n 2 27    2018-03-26 00:30:20 2018-03-26 02:15:52 no       M     \n 3 28    2018-03-21 11:09:38 2018-03-21 12:16:28 END      F     \n 4 29    2018-03-25 13:03:58 2018-03-25 14:45:25 ethics   M     \n 5 30    2018-03-24 06:46:30 2018-03-24 08:17:29 ethics   M     \n 6 A2    2018-03-21 03:23:57 2018-03-21 04:28:01 END      male  \n 7 A3    2018-03-25 21:07:24 2018-03-25 22:51:44 ethics   M     \n 8 C     2018-03-24 18:03:12 2018-03-24 19:38:12 ethics   M     \n 9 D     2018-03-22 04:16:08 2018-03-22 05:29:05 <NA>     M     \n10 I     2018-03-23 02:48:32 2018-03-23 04:09:31 END      M     \n# … with 19 more rows"
  },
  {
    "objectID": "03_transforming-data.html#select-helpers",
    "href": "03_transforming-data.html#select-helpers",
    "title": "3  Transforming Data",
    "section": "6.4 Select Helpers",
    "text": "6.4 Select Helpers\nThere are a number of select() helpers to make working with it easier:\n\nstarts_with(): keep columns starting with a specific string.\nends_with(): keep columns ending with a specific string.\ncontains(): keep columns containing a specific string.\nmatches(): Keeps columns matching a regular expression. This is useful for complex matching.\nnum_range(): Keeps columns with a matching prefix and a following range of numbers."
  },
  {
    "objectID": "03_transforming-data.html#select-helpers-1",
    "href": "03_transforming-data.html#select-helpers-1",
    "title": "3  Transforming Data",
    "section": "6.5 Select Helpers",
    "text": "6.5 Select Helpers\n\n6.5.1 contains()\nLet’s try a few:\n\nraw_data |> \n  select(contains(\"DJG\"))\n\n# A tibble: 29 × 6\n   DJG_1 DJG_2 DJG_3 DJG_4 DJG_5 DJG_6\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     1     1     1     0     0     0\n 2     0     0     0     1     0     1\n 3     0     1     0     0     1     1\n 4     0     1     1     0     1     0\n 5     1     1     0     0     1     1\n 6     0     1     1     1     0     1\n 7     0     1     1     1     1     1\n 8     1     0     1     0     1     0\n 9     0     0     1     1     0     1\n10     1     1     1     1     0     1\n# … with 19 more rows"
  },
  {
    "objectID": "03_transforming-data.html#select-helpers-2",
    "href": "03_transforming-data.html#select-helpers-2",
    "title": "3  Transforming Data",
    "section": "6.6 Select Helpers",
    "text": "6.6 Select Helpers\n\n6.6.1 num_range()\nLet’s try a few:\n\nraw_data |> \n  select(num_range(\"DJG_\", 1:3))\n\n# A tibble: 29 × 3\n   DJG_1 DJG_2 DJG_3\n   <dbl> <dbl> <dbl>\n 1     1     1     1\n 2     0     0     0\n 3     0     1     0\n 4     0     1     1\n 5     1     1     0\n 6     0     1     1\n 7     0     1     1\n 8     1     0     1\n 9     0     0     1\n10     1     1     1\n# … with 19 more rows"
  },
  {
    "objectID": "03_transforming-data.html#select-everything-and-rename",
    "href": "03_transforming-data.html#select-everything-and-rename",
    "title": "3  Transforming Data",
    "section": "7.1 select(), everything(), and rename()",
    "text": "7.1 select(), everything(), and rename()\nYou can use select() to reorder columns. Either specify the exact locations or specify the first few and use everything() to select all remaining columns.\n\nraw_data |> \n  select(PID, Gender, Age, start_time, end_time, everything())\n\n# A tibble: 29 × 47\n   PID   Gender   Age start_time  end_t…¹ progr…² Region Marit…³ Livin…⁴ Prima…⁵\n   <chr> <chr>  <dbl> <chr>       <chr>   <chr>   <chr>  <chr>   <chr>   <chr>  \n 1 26    M         54 2018-03-22… 2018-0… FINISH  East   Married With w… None   \n 2 27    M         54 2018-03-26… 2018-0… no      Scotl… Married With w… None   \n 3 28    F         37 2018-03-21… 2018-0… END     South… Married With h… None   \n 4 29    M         51 2018-03-25… 2018-0… ethics  West … Single  Alone   None   \n 5 30    M         52 2018-03-24… 2018-0… ethics  Scotl… Married With w… Wife   \n 6 A2    male      39 2018-03-21… 2018-0… END     North… Married Wife w… None   \n 7 A3    M         47 2018-03-25… 2018-0… ethics  North… Married Wife w… Wife i…\n 8 C     M         53 2018-03-24… 2018-0… ethics  Norfo… With P… Partner None   \n 9 D     M         68 2018-03-22… 2018-0… <NA>    Scotl… Widower With B… Brother\n10 I     M         50 2018-03-23… 2018-0… END     Scotl… In rel… Live w… None   \n# … with 19 more rows, 37 more variables: Employment <chr>,\n#   `Time Since BI` <chr>, `BI Severity` <chr>, Vision <chr>, Speech <chr>,\n#   Motor <chr>, Memory <chr>, Cognitive <chr>, Pain <chr>, BRS1 <dbl>,\n#   BRS2 <dbl>, BRS3 <dbl>, BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>,\n#   DJG_2 <dbl>, DJG_3 <dbl>, DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>,\n#   `WEMWBS 1` <chr>, `WEMWBS 2` <chr>, `WEMWBS 3` <chr>, `WEMWBS 4` <chr>,\n#   `WEMWBS 5` <chr>, `WEMWBS 6` <chr>, `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, …"
  },
  {
    "objectID": "03_transforming-data.html#renaming-columns",
    "href": "03_transforming-data.html#renaming-columns",
    "title": "3  Transforming Data",
    "section": "7.1 Renaming Columns",
    "text": "7.1 Renaming Columns\nOften when reading data into R we might want to change the column names, either because they are specified so that working with them is more difficult (e.g. those containing spaces such as `WEMWBS 1` which requires us to use back ticks around names to access them, e.g. raw_data$`WEMWBS 1`) or because they are excessively long or uninformative.\nWe can use the rename() function to change the name of columns. This takes the format of new = old.\n\n\n\nRenaming columns\n\n\nHere we will make the PID column more informative by making it participant_id.\n\nraw_data |> \n  rename(participant_id = PID)\n\n# A tibble: 29 × 47\n   partici…¹ start…² end_t…³ progr…⁴ Gender   Age Region Marit…⁵ Livin…⁶ Prima…⁷\n   <chr>     <chr>   <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n 1 26        2018-0… 2018-0… FINISH  M         54 East   Married With w… None   \n 2 27        2018-0… 2018-0… no      M         54 Scotl… Married With w… None   \n 3 28        2018-0… 2018-0… END     F         37 South… Married With h… None   \n 4 29        2018-0… 2018-0… ethics  M         51 West … Single  Alone   None   \n 5 30        2018-0… 2018-0… ethics  M         52 Scotl… Married With w… Wife   \n 6 A2        2018-0… 2018-0… END     male      39 North… Married Wife w… None   \n 7 A3        2018-0… 2018-0… ethics  M         47 North… Married Wife w… Wife i…\n 8 C         2018-0… 2018-0… ethics  M         53 Norfo… With P… Partner None   \n 9 D         2018-0… 2018-0… <NA>    M         68 Scotl… Widower With B… Brother\n10 I         2018-0… 2018-0… END     M         50 Scotl… In rel… Live w… None   \n# … with 19 more rows, 37 more variables: Employment <chr>,\n#   `Time Since BI` <chr>, `BI Severity` <chr>, Vision <chr>, Speech <chr>,\n#   Motor <chr>, Memory <chr>, Cognitive <chr>, Pain <chr>, BRS1 <dbl>,\n#   BRS2 <dbl>, BRS3 <dbl>, BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>,\n#   DJG_2 <dbl>, DJG_3 <dbl>, DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>,\n#   `WEMWBS 1` <chr>, `WEMWBS 2` <chr>, `WEMWBS 3` <chr>, `WEMWBS 4` <chr>,\n#   `WEMWBS 5` <chr>, `WEMWBS 6` <chr>, `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, …\n\n\n\n7.1.1 Renaming Columns Automatically\nThe janitor package has a function, clean_names(), which fixes irregularities in names. Mainly, it’s good practice to keep one format such as snake_case, to avoid spaces in names, and to avoid numbers at the start of column names.\n\nraw_data |> \n  janitor::clean_names()\n\n# A tibble: 29 × 47\n   pid   start_time  end_t…¹ progr…² gender   age region marit…³ livin…⁴ prima…⁵\n   <chr> <chr>       <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n 1 26    2018-03-22… 2018-0… FINISH  M         54 East   Married With w… None   \n 2 27    2018-03-26… 2018-0… no      M         54 Scotl… Married With w… None   \n 3 28    2018-03-21… 2018-0… END     F         37 South… Married With h… None   \n 4 29    2018-03-25… 2018-0… ethics  M         51 West … Single  Alone   None   \n 5 30    2018-03-24… 2018-0… ethics  M         52 Scotl… Married With w… Wife   \n 6 A2    2018-03-21… 2018-0… END     male      39 North… Married Wife w… None   \n 7 A3    2018-03-25… 2018-0… ethics  M         47 North… Married Wife w… Wife i…\n 8 C     2018-03-24… 2018-0… ethics  M         53 Norfo… With P… Partner None   \n 9 D     2018-03-22… 2018-0… <NA>    M         68 Scotl… Widower With B… Brother\n10 I     2018-03-23… 2018-0… END     M         50 Scotl… In rel… Live w… None   \n# … with 19 more rows, 37 more variables: employment <chr>,\n#   time_since_bi <chr>, bi_severity <chr>, vision <chr>, speech <chr>,\n#   motor <chr>, memory <chr>, cognitive <chr>, pain <chr>, brs1 <dbl>,\n#   brs2 <dbl>, brs3 <dbl>, brs4 <dbl>, brs5 <dbl>, brs6 <dbl>, djg_1 <dbl>,\n#   djg_2 <dbl>, djg_3 <dbl>, djg_4 <dbl>, djg_5 <dbl>, djg_6 <dbl>,\n#   wemwbs_1 <chr>, wemwbs_2 <chr>, wemwbs_3 <chr>, wemwbs_4 <chr>,\n#   wemwbs_5 <chr>, wemwbs_6 <chr>, wemwbs_7 <chr>, wemwbs_8 <chr>, …\n\n\n\n\n7.1.2 Reordering Columns\nWe can also use select() to reorder columns. Either specify the names or indices of every column you’d like in the order you’d like, or you can use the everything() helper function to make this easier. To use this function, we simply state the order of columns we’d like in a specific order, then use everything() to keep every other column in the data set after those specified in the specific order.\n\nraw_data |> \n  select(PID, Gender, Age, start_time, end_time, everything())\n\n# A tibble: 29 × 47\n   PID   Gender   Age start_time  end_t…¹ progr…² Region Marit…³ Livin…⁴ Prima…⁵\n   <chr> <chr>  <dbl> <chr>       <chr>   <chr>   <chr>  <chr>   <chr>   <chr>  \n 1 26    M         54 2018-03-22… 2018-0… FINISH  East   Married With w… None   \n 2 27    M         54 2018-03-26… 2018-0… no      Scotl… Married With w… None   \n 3 28    F         37 2018-03-21… 2018-0… END     South… Married With h… None   \n 4 29    M         51 2018-03-25… 2018-0… ethics  West … Single  Alone   None   \n 5 30    M         52 2018-03-24… 2018-0… ethics  Scotl… Married With w… Wife   \n 6 A2    male      39 2018-03-21… 2018-0… END     North… Married Wife w… None   \n 7 A3    M         47 2018-03-25… 2018-0… ethics  North… Married Wife w… Wife i…\n 8 C     M         53 2018-03-24… 2018-0… ethics  Norfo… With P… Partner None   \n 9 D     M         68 2018-03-22… 2018-0… <NA>    Scotl… Widower With B… Brother\n10 I     M         50 2018-03-23… 2018-0… END     Scotl… In rel… Live w… None   \n# … with 19 more rows, 37 more variables: Employment <chr>,\n#   `Time Since BI` <chr>, `BI Severity` <chr>, Vision <chr>, Speech <chr>,\n#   Motor <chr>, Memory <chr>, Cognitive <chr>, Pain <chr>, BRS1 <dbl>,\n#   BRS2 <dbl>, BRS3 <dbl>, BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>,\n#   DJG_2 <dbl>, DJG_3 <dbl>, DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>,\n#   `WEMWBS 1` <chr>, `WEMWBS 2` <chr>, `WEMWBS 3` <chr>, `WEMWBS 4` <chr>,\n#   `WEMWBS 5` <chr>, `WEMWBS 6` <chr>, `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, …"
  },
  {
    "objectID": "03_transforming-data.html#renaming-columns-1",
    "href": "03_transforming-data.html#renaming-columns-1",
    "title": "3  Transforming Data",
    "section": "7.3 Renaming Columns",
    "text": "7.3 Renaming Columns\n\nraw_data |> \n  rename(participant_id = PID)\n\n# A tibble: 29 × 47\n   partici…¹ start…² end_t…³ progr…⁴ Gender   Age Region Marit…⁵ Livin…⁶ Prima…⁷\n   <chr>     <chr>   <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n 1 26        2018-0… 2018-0… FINISH  M         54 East   Married With w… None   \n 2 27        2018-0… 2018-0… no      M         54 Scotl… Married With w… None   \n 3 28        2018-0… 2018-0… END     F         37 South… Married With h… None   \n 4 29        2018-0… 2018-0… ethics  M         51 West … Single  Alone   None   \n 5 30        2018-0… 2018-0… ethics  M         52 Scotl… Married With w… Wife   \n 6 A2        2018-0… 2018-0… END     male      39 North… Married Wife w… None   \n 7 A3        2018-0… 2018-0… ethics  M         47 North… Married Wife w… Wife i…\n 8 C         2018-0… 2018-0… ethics  M         53 Norfo… With P… Partner None   \n 9 D         2018-0… 2018-0… <NA>    M         68 Scotl… Widower With B… Brother\n10 I         2018-0… 2018-0… END     M         50 Scotl… In rel… Live w… None   \n# … with 19 more rows, 37 more variables: Employment <chr>,\n#   `Time Since BI` <chr>, `BI Severity` <chr>, Vision <chr>, Speech <chr>,\n#   Motor <chr>, Memory <chr>, Cognitive <chr>, Pain <chr>, BRS1 <dbl>,\n#   BRS2 <dbl>, BRS3 <dbl>, BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>,\n#   DJG_2 <dbl>, DJG_3 <dbl>, DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>,\n#   `WEMWBS 1` <chr>, `WEMWBS 2` <chr>, `WEMWBS 3` <chr>, `WEMWBS 4` <chr>,\n#   `WEMWBS 5` <chr>, `WEMWBS 6` <chr>, `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, …"
  },
  {
    "objectID": "03_transforming-data.html#renaming-columns-automatically",
    "href": "03_transforming-data.html#renaming-columns-automatically",
    "title": "3  Transforming Data",
    "section": "7.4 Renaming Columns Automatically",
    "text": "7.4 Renaming Columns Automatically\nThe janitor package has a function, clean_names(), which fixes irregularities in names. Mainly, it’s good practice to keep one format such as snake_case, to avoid spaces in names, and to avoid numbers at the start of column names.\n\nraw_data |> \n  janitor::clean_names()\n\n# A tibble: 29 × 47\n   pid   start_time  end_t…¹ progr…² gender   age region marit…³ livin…⁴ prima…⁵\n   <chr> <chr>       <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n 1 26    2018-03-22… 2018-0… FINISH  M         54 East   Married With w… None   \n 2 27    2018-03-26… 2018-0… no      M         54 Scotl… Married With w… None   \n 3 28    2018-03-21… 2018-0… END     F         37 South… Married With h… None   \n 4 29    2018-03-25… 2018-0… ethics  M         51 West … Single  Alone   None   \n 5 30    2018-03-24… 2018-0… ethics  M         52 Scotl… Married With w… Wife   \n 6 A2    2018-03-21… 2018-0… END     male      39 North… Married Wife w… None   \n 7 A3    2018-03-25… 2018-0… ethics  M         47 North… Married Wife w… Wife i…\n 8 C     2018-03-24… 2018-0… ethics  M         53 Norfo… With P… Partner None   \n 9 D     2018-03-22… 2018-0… <NA>    M         68 Scotl… Widower With B… Brother\n10 I     2018-03-23… 2018-0… END     M         50 Scotl… In rel… Live w… None   \n# … with 19 more rows, 37 more variables: employment <chr>,\n#   time_since_bi <chr>, bi_severity <chr>, vision <chr>, speech <chr>,\n#   motor <chr>, memory <chr>, cognitive <chr>, pain <chr>, brs1 <dbl>,\n#   brs2 <dbl>, brs3 <dbl>, brs4 <dbl>, brs5 <dbl>, brs6 <dbl>, djg_1 <dbl>,\n#   djg_2 <dbl>, djg_3 <dbl>, djg_4 <dbl>, djg_5 <dbl>, djg_6 <dbl>,\n#   wemwbs_1 <chr>, wemwbs_2 <chr>, wemwbs_3 <chr>, wemwbs_4 <chr>,\n#   wemwbs_5 <chr>, wemwbs_6 <chr>, wemwbs_7 <chr>, wemwbs_8 <chr>, …"
  },
  {
    "objectID": "03_transforming-data.html#mutate-and-transmute",
    "href": "03_transforming-data.html#mutate-and-transmute",
    "title": "3  Transforming Data",
    "section": "8.1 mutate() and transmute()",
    "text": "8.1 mutate() and transmute()\nWe have two main functions for this:\n\nmutate(): changes or creates a column, keeping all existing columns.\ntransmute(): changes or creates a column, dropping columns not defined in transmute().\n\nWe’ll mainly use mutate().\n\nThis can be a simple operation on one column, using two columns to create another, or doing conditional operations within a column."
  },
  {
    "objectID": "03_transforming-data.html#mutate",
    "href": "03_transforming-data.html#mutate",
    "title": "3  Transforming Data",
    "section": "8.2 Mutate",
    "text": "8.2 Mutate\nHere we create a new column by adding together values from columns 1 and y.\n\n\n\nUsing mutate to produce a new column based on operations on existing columns"
  },
  {
    "objectID": "03_transforming-data.html#simple-operations",
    "href": "03_transforming-data.html#simple-operations",
    "title": "3  Transforming Data",
    "section": "8.3 Simple Operations",
    "text": "8.3 Simple Operations\n\nraw_data |> \n  mutate(\n    birth_year = 2023 - Age\n  ) |> \n  select(PID, birth_year)\n\n# A tibble: 29 × 2\n   PID   birth_year\n   <chr>      <dbl>\n 1 26          1969\n 2 27          1969\n 3 28          1986\n 4 29          1972\n 5 30          1971\n 6 A2          1984\n 7 A3          1976\n 8 C           1970\n 9 D           1955\n10 I           1973\n# … with 19 more rows"
  },
  {
    "objectID": "03_transforming-data.html#multi-column-operations",
    "href": "03_transforming-data.html#multi-column-operations",
    "title": "3  Transforming Data",
    "section": "8.4 Multi-column Operations",
    "text": "8.4 Multi-column Operations\n\nraw_data |> \n  mutate(\n    djg_total = DJG_1 + DJG_2 + DJG_3 + DJG_4 + DJG_5 + DJG_6\n  )\n\n# A tibble: 29 × 48\n   PID   start_time  end_t…¹ progr…² Gender   Age Region Marit…³ Livin…⁴ Prima…⁵\n   <chr> <chr>       <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n 1 26    2018-03-22… 2018-0… FINISH  M         54 East   Married With w… None   \n 2 27    2018-03-26… 2018-0… no      M         54 Scotl… Married With w… None   \n 3 28    2018-03-21… 2018-0… END     F         37 South… Married With h… None   \n 4 29    2018-03-25… 2018-0… ethics  M         51 West … Single  Alone   None   \n 5 30    2018-03-24… 2018-0… ethics  M         52 Scotl… Married With w… Wife   \n 6 A2    2018-03-21… 2018-0… END     male      39 North… Married Wife w… None   \n 7 A3    2018-03-25… 2018-0… ethics  M         47 North… Married Wife w… Wife i…\n 8 C     2018-03-24… 2018-0… ethics  M         53 Norfo… With P… Partner None   \n 9 D     2018-03-22… 2018-0… <NA>    M         68 Scotl… Widower With B… Brother\n10 I     2018-03-23… 2018-0… END     M         50 Scotl… In rel… Live w… None   \n# … with 19 more rows, 38 more variables: Employment <chr>,\n#   `Time Since BI` <chr>, `BI Severity` <chr>, Vision <chr>, Speech <chr>,\n#   Motor <chr>, Memory <chr>, Cognitive <chr>, Pain <chr>, BRS1 <dbl>,\n#   BRS2 <dbl>, BRS3 <dbl>, BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>,\n#   DJG_2 <dbl>, DJG_3 <dbl>, DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>,\n#   `WEMWBS 1` <chr>, `WEMWBS 2` <chr>, `WEMWBS 3` <chr>, `WEMWBS 4` <chr>,\n#   `WEMWBS 5` <chr>, `WEMWBS 6` <chr>, `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, …"
  },
  {
    "objectID": "03_transforming-data.html#overwriting-columns",
    "href": "03_transforming-data.html#overwriting-columns",
    "title": "3  Transforming Data",
    "section": "8.5 Overwriting Columns",
    "text": "8.5 Overwriting Columns\nWe can also update columns by transforming values within them.\n\n\n\nUsing mutate to overwrite or update values in a column"
  },
  {
    "objectID": "03_transforming-data.html#overwriting-columns-1",
    "href": "03_transforming-data.html#overwriting-columns-1",
    "title": "3  Transforming Data",
    "section": "8.6 Overwriting Columns",
    "text": "8.6 Overwriting Columns\n\n8.6.1 Fixing Type Problems\nIf we try to get the total time participants took in the study like so, it won’t work.\n\nraw_data |> \n  mutate(total_time = end_time - start_time)\n\n\nThat’s because R doesn’t know how to work with date times.\nThis also wouldn’t work if we had a character and numeric column.\nTo fix this, we change the data type."
  },
  {
    "objectID": "03_transforming-data.html#multi-column-operations-1",
    "href": "03_transforming-data.html#multi-column-operations-1",
    "title": "3  Transforming Data",
    "section": "8.7 Multi-Column Operations",
    "text": "8.7 Multi-Column Operations\n\nNotice below we can work with columns we’re still in the process of creating during mutate() calls.\n\nraw_data |> \n  mutate(\n    start_time = ymd_hms(start_time),\n    end_time = ymd_hms(end_time),\n    total_time = end_time - start_time\n  ) |> \n  select(contains(\"time\"))\n\n# A tibble: 29 × 4\n   start_time          end_time            `Time Since BI` total_time    \n   <dttm>              <dttm>              <chr>           <drtn>        \n 1 2018-03-22 23:06:11 2018-03-23 00:25:51 6 (2015)        1.327778 hours\n 2 2018-03-26 00:30:20 2018-03-26 02:15:52 3 (2018)        1.758889 hours\n 3 2018-03-21 11:09:38 2018-03-21 12:16:28 5 (2016)        1.113889 hours\n 4 2018-03-25 13:03:58 2018-03-25 14:45:25 23 (1998)       1.690833 hours\n 5 2018-03-24 06:46:30 2018-03-24 08:17:29 8 (2013)        1.516389 hours\n 6 2018-03-21 03:23:57 2018-03-21 04:28:01 3 (2017)        1.067778 hours\n 7 2018-03-25 21:07:24 2018-03-25 22:51:44 1 (2020)        1.738889 hours\n 8 2018-03-24 18:03:12 2018-03-24 19:38:12 29 (1992)       1.583333 hours\n 9 2018-03-22 04:16:08 2018-03-22 05:29:05 6 (2016)        1.215833 hours\n10 2018-03-23 02:48:32 2018-03-23 04:09:31 21 (2000)       1.349722 hours\n# … with 19 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf we don’t like working with decimal hours, we can change the units to minutes using e.g. units(TIBBLE$COL) <- \"mins\""
  },
  {
    "objectID": "03_transforming-data.html#improving-multi-column-operations",
    "href": "03_transforming-data.html#improving-multi-column-operations",
    "title": "3  Transforming Data",
    "section": "8.1 Improving Multi-Column Operations",
    "text": "8.1 Improving Multi-Column Operations\nListing each of the columns we’d like to use when performing operations on the columns can be tedious. Let’s say we want to sum up every value in the DJG columns. How might we do this? You might assume we can use the : operator to select only the columns between DJG_1 and DJG_6 as follows:\n\nraw_data |> \n  mutate(DJG_sum = sum(DJG_1:DJG_6))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `DJG_sum = sum(DJG_1:DJG_6)`.\nCaused by warning in `DJG_1:DJG_6`:\n! numerical expression has 29 elements: only the first used\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n# A tibble: 29 × 48\n   PID   start_time  end_t…¹ progr…² Gender   Age Region Marit…³ Livin…⁴ Prima…⁵\n   <chr> <chr>       <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n 1 26    2018-03-22… 2018-0… FINISH  M         54 East   Married With w… None   \n 2 27    2018-03-26… 2018-0… no      M         54 Scotl… Married With w… None   \n 3 28    2018-03-21… 2018-0… END     F         37 South… Married With h… None   \n 4 29    2018-03-25… 2018-0… ethics  M         51 West … Single  Alone   None   \n 5 30    2018-03-24… 2018-0… ethics  M         52 Scotl… Married With w… Wife   \n 6 A2    2018-03-21… 2018-0… END     male      39 North… Married Wife w… None   \n 7 A3    2018-03-25… 2018-0… ethics  M         47 North… Married Wife w… Wife i…\n 8 C     2018-03-24… 2018-0… ethics  M         53 Norfo… With P… Partner None   \n 9 D     2018-03-22… 2018-0… <NA>    M         68 Scotl… Widower With B… Brother\n10 I     2018-03-23… 2018-0… END     M         50 Scotl… In rel… Live w… None   \n# … with 19 more rows, 38 more variables: Employment <chr>,\n#   `Time Since BI` <chr>, `BI Severity` <chr>, Vision <chr>, Speech <chr>,\n#   Motor <chr>, Memory <chr>, Cognitive <chr>, Pain <chr>, BRS1 <dbl>,\n#   BRS2 <dbl>, BRS3 <dbl>, BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>,\n#   DJG_2 <dbl>, DJG_3 <dbl>, DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>,\n#   `WEMWBS 1` <chr>, `WEMWBS 2` <chr>, `WEMWBS 3` <chr>, `WEMWBS 4` <chr>,\n#   `WEMWBS 5` <chr>, `WEMWBS 6` <chr>, `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, …\n\n\nHowever, we got a warning and very unexpected output. That’s because the sum() function (and others) aren’t set up for row-wise operations. Do to this, we can use the across() function to tell mutate() that we want to perform a row-wise operation. However, c() also doesn’t work as expected here as it doesn’t like the bare or quoted column names we pass to the function. So, we update this to use c_across() to work with the bare column names:\n\nraw_data |> \n  rowwise() |> \n  mutate(DJG_sum = sum(c_across(DJG_1:DJG_6)))\n\n# A tibble: 29 × 48\n# Rowwise: \n   PID   start_time  end_t…¹ progr…² Gender   Age Region Marit…³ Livin…⁴ Prima…⁵\n   <chr> <chr>       <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n 1 26    2018-03-22… 2018-0… FINISH  M         54 East   Married With w… None   \n 2 27    2018-03-26… 2018-0… no      M         54 Scotl… Married With w… None   \n 3 28    2018-03-21… 2018-0… END     F         37 South… Married With h… None   \n 4 29    2018-03-25… 2018-0… ethics  M         51 West … Single  Alone   None   \n 5 30    2018-03-24… 2018-0… ethics  M         52 Scotl… Married With w… Wife   \n 6 A2    2018-03-21… 2018-0… END     male      39 North… Married Wife w… None   \n 7 A3    2018-03-25… 2018-0… ethics  M         47 North… Married Wife w… Wife i…\n 8 C     2018-03-24… 2018-0… ethics  M         53 Norfo… With P… Partner None   \n 9 D     2018-03-22… 2018-0… <NA>    M         68 Scotl… Widower With B… Brother\n10 I     2018-03-23… 2018-0… END     M         50 Scotl… In rel… Live w… None   \n# … with 19 more rows, 38 more variables: Employment <chr>,\n#   `Time Since BI` <chr>, `BI Severity` <chr>, Vision <chr>, Speech <chr>,\n#   Motor <chr>, Memory <chr>, Cognitive <chr>, Pain <chr>, BRS1 <dbl>,\n#   BRS2 <dbl>, BRS3 <dbl>, BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>,\n#   DJG_2 <dbl>, DJG_3 <dbl>, DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>,\n#   `WEMWBS 1` <chr>, `WEMWBS 2` <chr>, `WEMWBS 3` <chr>, `WEMWBS 4` <chr>,\n#   `WEMWBS 5` <chr>, `WEMWBS 6` <chr>, `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, …\n\n\nWe can also use the select() helpers with c_across() to make this even cleaner.\n\nraw_data |> \n  rowwise() |> \n  mutate(DJG_sum = sum(c_across(contains(\"DJG\"))))\n\n# A tibble: 29 × 48\n# Rowwise: \n   PID   start_time  end_t…¹ progr…² Gender   Age Region Marit…³ Livin…⁴ Prima…⁵\n   <chr> <chr>       <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n 1 26    2018-03-22… 2018-0… FINISH  M         54 East   Married With w… None   \n 2 27    2018-03-26… 2018-0… no      M         54 Scotl… Married With w… None   \n 3 28    2018-03-21… 2018-0… END     F         37 South… Married With h… None   \n 4 29    2018-03-25… 2018-0… ethics  M         51 West … Single  Alone   None   \n 5 30    2018-03-24… 2018-0… ethics  M         52 Scotl… Married With w… Wife   \n 6 A2    2018-03-21… 2018-0… END     male      39 North… Married Wife w… None   \n 7 A3    2018-03-25… 2018-0… ethics  M         47 North… Married Wife w… Wife i…\n 8 C     2018-03-24… 2018-0… ethics  M         53 Norfo… With P… Partner None   \n 9 D     2018-03-22… 2018-0… <NA>    M         68 Scotl… Widower With B… Brother\n10 I     2018-03-23… 2018-0… END     M         50 Scotl… In rel… Live w… None   \n# … with 19 more rows, 38 more variables: Employment <chr>,\n#   `Time Since BI` <chr>, `BI Severity` <chr>, Vision <chr>, Speech <chr>,\n#   Motor <chr>, Memory <chr>, Cognitive <chr>, Pain <chr>, BRS1 <dbl>,\n#   BRS2 <dbl>, BRS3 <dbl>, BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>,\n#   DJG_2 <dbl>, DJG_3 <dbl>, DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>,\n#   `WEMWBS 1` <chr>, `WEMWBS 2` <chr>, `WEMWBS 3` <chr>, `WEMWBS 4` <chr>,\n#   `WEMWBS 5` <chr>, `WEMWBS 6` <chr>, `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, …\n\n\n\n8.1.1 Using Conditionals\nFinally, we can change only certain values within a column with conditionals.\nHere we create a new column z.\n\nIf the value in y is equal to 10, that row in z gets a 1.\nFor all other values of y (default, TRUE for everything else), give z a 0.\n\n\n\n\nCreating a new column based on conditional operations on an existing column\n\n\nNotice that in our data set we have different impairments (e.g. Vision, Speech) coded as X or NA. We can’t work with these for statistics because they aren’t numbers. But we can fix that using some conditional rules using the case_when() function.\ncase_when() looks for a logical operation, (e.g. when Vision == \"X\"), when this is TRUE you can set a value to assign to that case after the ~. We will set this to 1. We can then either specify every other case, or set a default value. In this instance we’ll set a default value where the case evaluation always results in TRUE after checking for the first case (i.e. that Vision == \"X\"). For all cases other than Vision == \"X\" we set the value to 0.\nBelow we use select() just to make the printout more manageable to view.\n\nraw_data |> \n  select(PID, Vision) |> \n  mutate(\n    Vision = case_when(\n      Vision == \"X\" ~ 1,\n      TRUE ~ 0\n    )\n  )\n\n# A tibble: 29 × 2\n   PID   Vision\n   <chr>  <dbl>\n 1 26         0\n 2 27         0\n 3 28         1\n 4 29         0\n 5 30         1\n 6 A2         0\n 7 A3         0\n 8 C          0\n 9 D          1\n10 I          0\n# … with 19 more rows"
  },
  {
    "objectID": "03_transforming-data.html#using-conditionals",
    "href": "03_transforming-data.html#using-conditionals",
    "title": "3  Transforming Data",
    "section": "8.9 Using Conditionals",
    "text": "8.9 Using Conditionals\n\nFinally, we can change only certain values within a column with conditionals.\nNotice that we have different impairments (e.g. Vision, Speech) coded as X or NA. We can’t work with these for statistics because they aren’t numbers. Let’s fix that.\ncase_when() looks for a logical operation, (e.g. when Vision == “X”), and your new value when this is true after the ~ (here, 1).\ncase_when() takes defaults, so we say “when anything else happens, i.e. TRUE set it to 0”."
  },
  {
    "objectID": "03_transforming-data.html#using-conditionals-1",
    "href": "03_transforming-data.html#using-conditionals-1",
    "title": "3  Transforming Data",
    "section": "8.10 Using Conditionals",
    "text": "8.10 Using Conditionals\nHere we create a new column z.\n\nIf the value in y is equal to 10, that row in z gets a 1.\nFor all other values of y (default, TRUE for everything else), give z a 0.\n\n\n\n\nCreating a new column based on conditional operations on an existing column"
  },
  {
    "objectID": "03_transforming-data.html#using-conditionals-2",
    "href": "03_transforming-data.html#using-conditionals-2",
    "title": "3  Transforming Data",
    "section": "8.11 Using Conditionals",
    "text": "8.11 Using Conditionals\n\nraw_data |> \n  select(PID, Vision) |> \n  mutate(\n    Vision = case_when(\n      Vision == \"X\" ~ 1,\n      TRUE ~ 0\n    )\n  )\n\n# A tibble: 29 × 2\n   PID   Vision\n   <chr>  <dbl>\n 1 26         0\n 2 27         0\n 3 28         1\n 4 29         0\n 5 30         1\n 6 A2         0\n 7 A3         0\n 8 C          0\n 9 D          1\n10 I          0\n# … with 19 more rows"
  },
  {
    "objectID": "03_transforming-data.html#filter",
    "href": "03_transforming-data.html#filter",
    "title": "3  Transforming Data",
    "section": "9.1 filter()",
    "text": "9.1 filter()\n\nfilter() allows you to subset to rows of data where certain conditions are met.\nLet’s take only those who have progress as FINISH OR END\n\n\nraw_data |> \n  filter(progress == \"FINISH\" | progress == \"END\")\n\n# A tibble: 19 × 47\n   PID   start_time  end_t…¹ progr…² Gender   Age Region Marit…³ Livin…⁴ Prima…⁵\n   <chr> <chr>       <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n 1 26    2018-03-22… 2018-0… FINISH  M         54 East   Married With w… None   \n 2 28    2018-03-21… 2018-0… END     F         37 South… Married With h… None   \n 3 A2    2018-03-21… 2018-0… END     male      39 North… Married Wife w… None   \n 4 I     2018-03-23… 2018-0… END     M         50 Scotl… In rel… Live w… None   \n 5 J     2018-03-25… 2018-0… END     M         46 Scotl… Single  By self None   \n 6 K     2018-03-22… 2018-0… END     M         55 North… Married Wife    Wife   \n 7 M1    2018-03-26… 2018-0… END     Female    53 North… Single  By self None   \n 8 M2    2018-03-26… 2018-0… END     F         49 West … Single  By self None   \n 9 P     2018-03-21… 2018-0… END     F         61 Toron… Divorc… With (… None   \n10 R1    2018-03-24… 2018-0… END     F         56 North… Married Husband Husband\n11 R2    2018-03-25… 2018-0… END     M         53 Scotl… Single  By self None   \n12 R3    2018-03-21… 2018-0… END     F         28 West … Single  By self None   \n13 S     2018-03-24… 2018-0… END     F         50 West … Married Husban… None   \n14 T1    2018-03-21… 2018-0… END     M         47 East … With p… Partne… None   \n15 T2    2018-03-26… 2018-0… END     F         24 Hertf… Single  By sel… None   \n16 V     2018-03-22… 2018-0… END     M         68 North… Married w/Wife  Wife   \n17 W1    2018-03-21… 2018-0… END     F         33 North… Married Spouse  None   \n18 W2    2018-03-24… 2018-0… END     M         39 North… In rel… Partne… Mother…\n19 Z     2018-03-25… 2018-0… END     M         64 North… Single  Sister  Sister \n# … with 37 more variables: Employment <chr>, `Time Since BI` <chr>,\n#   `BI Severity` <chr>, Vision <chr>, Speech <chr>, Motor <chr>, Memory <chr>,\n#   Cognitive <chr>, Pain <chr>, BRS1 <dbl>, BRS2 <dbl>, BRS3 <dbl>,\n#   BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>, DJG_2 <dbl>, DJG_3 <dbl>,\n#   DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>, `WEMWBS 1` <chr>, `WEMWBS 2` <chr>,\n#   `WEMWBS 3` <chr>, `WEMWBS 4` <chr>, `WEMWBS 5` <chr>, `WEMWBS 6` <chr>,\n#   `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, `WEMWBS 9` <chr>, `WEMWBS 10` <chr>, …"
  },
  {
    "objectID": "03_transforming-data.html#the-in-operator",
    "href": "03_transforming-data.html#the-in-operator",
    "title": "3  Transforming Data",
    "section": "9.2 The %in% Operator",
    "text": "9.2 The %in% Operator\n\nIf we have many conditions we want to meet, many OR (|) statements are verbose. We can instead use %in%\nThis is TRUE if an observation is in a value that you provide.\n\n\nraw_data |> \n  filter(progress %in% c(\"FINISH\", \"END\"))\n\n# A tibble: 19 × 47\n   PID   start_time  end_t…¹ progr…² Gender   Age Region Marit…³ Livin…⁴ Prima…⁵\n   <chr> <chr>       <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n 1 26    2018-03-22… 2018-0… FINISH  M         54 East   Married With w… None   \n 2 28    2018-03-21… 2018-0… END     F         37 South… Married With h… None   \n 3 A2    2018-03-21… 2018-0… END     male      39 North… Married Wife w… None   \n 4 I     2018-03-23… 2018-0… END     M         50 Scotl… In rel… Live w… None   \n 5 J     2018-03-25… 2018-0… END     M         46 Scotl… Single  By self None   \n 6 K     2018-03-22… 2018-0… END     M         55 North… Married Wife    Wife   \n 7 M1    2018-03-26… 2018-0… END     Female    53 North… Single  By self None   \n 8 M2    2018-03-26… 2018-0… END     F         49 West … Single  By self None   \n 9 P     2018-03-21… 2018-0… END     F         61 Toron… Divorc… With (… None   \n10 R1    2018-03-24… 2018-0… END     F         56 North… Married Husband Husband\n11 R2    2018-03-25… 2018-0… END     M         53 Scotl… Single  By self None   \n12 R3    2018-03-21… 2018-0… END     F         28 West … Single  By self None   \n13 S     2018-03-24… 2018-0… END     F         50 West … Married Husban… None   \n14 T1    2018-03-21… 2018-0… END     M         47 East … With p… Partne… None   \n15 T2    2018-03-26… 2018-0… END     F         24 Hertf… Single  By sel… None   \n16 V     2018-03-22… 2018-0… END     M         68 North… Married w/Wife  Wife   \n17 W1    2018-03-21… 2018-0… END     F         33 North… Married Spouse  None   \n18 W2    2018-03-24… 2018-0… END     M         39 North… In rel… Partne… Mother…\n19 Z     2018-03-25… 2018-0… END     M         64 North… Single  Sister  Sister \n# … with 37 more variables: Employment <chr>, `Time Since BI` <chr>,\n#   `BI Severity` <chr>, Vision <chr>, Speech <chr>, Motor <chr>, Memory <chr>,\n#   Cognitive <chr>, Pain <chr>, BRS1 <dbl>, BRS2 <dbl>, BRS3 <dbl>,\n#   BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>, DJG_2 <dbl>, DJG_3 <dbl>,\n#   DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>, `WEMWBS 1` <chr>, `WEMWBS 2` <chr>,\n#   `WEMWBS 3` <chr>, `WEMWBS 4` <chr>, `WEMWBS 5` <chr>, `WEMWBS 6` <chr>,\n#   `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, `WEMWBS 9` <chr>, `WEMWBS 10` <chr>, …"
  },
  {
    "objectID": "03_transforming-data.html#combining-filtering-operations",
    "href": "03_transforming-data.html#combining-filtering-operations",
    "title": "3  Transforming Data",
    "section": "9.3 Combining Filtering Operations",
    "text": "9.3 Combining Filtering Operations\nWe can combine these with criteria on other columns.\n\nraw_data |> \n  filter(\n    progress %in% c(\"FINISH\", \"END\"),\n    Age > 50\n  )\n\n# A tibble: 8 × 47\n  PID   start_time   end_t…¹ progr…² Gender   Age Region Marit…³ Livin…⁴ Prima…⁵\n  <chr> <chr>        <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n1 26    2018-03-22 … 2018-0… FINISH  M         54 East   Married With w… None   \n2 K     2018-03-22 … 2018-0… END     M         55 North… Married Wife    Wife   \n3 M1    2018-03-26 … 2018-0… END     Female    53 North… Single  By self None   \n4 P     2018-03-21 … 2018-0… END     F         61 Toron… Divorc… With (… None   \n5 R1    2018-03-24 … 2018-0… END     F         56 North… Married Husband Husband\n6 R2    2018-03-25 … 2018-0… END     M         53 Scotl… Single  By self None   \n7 V     2018-03-22 … 2018-0… END     M         68 North… Married w/Wife  Wife   \n8 Z     2018-03-25 … 2018-0… END     M         64 North… Single  Sister  Sister \n# … with 37 more variables: Employment <chr>, `Time Since BI` <chr>,\n#   `BI Severity` <chr>, Vision <chr>, Speech <chr>, Motor <chr>, Memory <chr>,\n#   Cognitive <chr>, Pain <chr>, BRS1 <dbl>, BRS2 <dbl>, BRS3 <dbl>,\n#   BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>, DJG_2 <dbl>, DJG_3 <dbl>,\n#   DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>, `WEMWBS 1` <chr>, `WEMWBS 2` <chr>,\n#   `WEMWBS 3` <chr>, `WEMWBS 4` <chr>, `WEMWBS 5` <chr>, `WEMWBS 6` <chr>,\n#   `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, `WEMWBS 9` <chr>, `WEMWBS 10` <chr>, …"
  },
  {
    "objectID": "03_transforming-data.html#removing-by-criteria",
    "href": "03_transforming-data.html#removing-by-criteria",
    "title": "3  Transforming Data",
    "section": "9.4 Removing by Criteria",
    "text": "9.4 Removing by Criteria\nFinally, we can do the inverse of these operations using !.\n\nHere we ask for those whose progress ISN’T “FINISH” or “END” but who are over 50.\n\n\nraw_data |> \n  filter(\n    !progress %in% c(\"FINISH\", \"END\"),\n    Age > 50\n  )\n\n# A tibble: 5 × 47\n  PID   start_time   end_t…¹ progr…² Gender   Age Region Marit…³ Livin…⁴ Prima…⁵\n  <chr> <chr>        <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n1 27    2018-03-26 … 2018-0… no      M         54 Scotl… Married With w… None   \n2 29    2018-03-25 … 2018-0… ethics  M         51 West … Single  Alone   None   \n3 30    2018-03-24 … 2018-0… ethics  M         52 Scotl… Married With w… Wife   \n4 C     2018-03-24 … 2018-0… ethics  M         53 Norfo… With P… Partner None   \n5 D     2018-03-22 … 2018-0… <NA>    M         68 Scotl… Widower With B… Brother\n# … with 37 more variables: Employment <chr>, `Time Since BI` <chr>,\n#   `BI Severity` <chr>, Vision <chr>, Speech <chr>, Motor <chr>, Memory <chr>,\n#   Cognitive <chr>, Pain <chr>, BRS1 <dbl>, BRS2 <dbl>, BRS3 <dbl>,\n#   BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>, DJG_2 <dbl>, DJG_3 <dbl>,\n#   DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>, `WEMWBS 1` <chr>, `WEMWBS 2` <chr>,\n#   `WEMWBS 3` <chr>, `WEMWBS 4` <chr>, `WEMWBS 5` <chr>, `WEMWBS 6` <chr>,\n#   `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, `WEMWBS 9` <chr>, `WEMWBS 10` <chr>, …"
  },
  {
    "objectID": "03_transforming-data.html#handling-nas",
    "href": "03_transforming-data.html#handling-nas",
    "title": "3  Transforming Data",
    "section": "9.5 Handling NAs",
    "text": "9.5 Handling NAs\n\nIf you ask R if a value is equal to an NA (an unknown value) it is very literal in that it tells us it can’t know.\n\nSo, this doesn’t work:\n\nraw_data |> filter(progress == NA)\n\nInstead, we have to use is.na():\n\nraw_data |> filter(is.na(progress))\n\n# A tibble: 5 × 47\n  PID   start_time   end_t…¹ progr…² Gender   Age Region Marit…³ Livin…⁴ Prima…⁵\n  <chr> <chr>        <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n1 D     2018-03-22 … 2018-0… <NA>    M         68 Scotl… Widower With B… Brother\n2 <NA>  <NA>         <NA>    <NA>    <NA>      NA <NA>   <NA>    <NA>    <NA>   \n3 <NA>  <NA>         <NA>    <NA>    <NA>      NA <NA>   <NA>    <NA>    <NA>   \n4 <NA>  <NA>         <NA>    <NA>    <NA>      NA <NA>   <NA>    <NA>    <NA>   \n5 <NA>  <NA>         <NA>    <NA>    <NA>      NA <NA>   <NA>    <NA>    <NA>   \n# … with 37 more variables: Employment <chr>, `Time Since BI` <chr>,\n#   `BI Severity` <chr>, Vision <chr>, Speech <chr>, Motor <chr>, Memory <chr>,\n#   Cognitive <chr>, Pain <chr>, BRS1 <dbl>, BRS2 <dbl>, BRS3 <dbl>,\n#   BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>, DJG_2 <dbl>, DJG_3 <dbl>,\n#   DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>, `WEMWBS 1` <chr>, `WEMWBS 2` <chr>,\n#   `WEMWBS 3` <chr>, `WEMWBS 4` <chr>, `WEMWBS 5` <chr>, `WEMWBS 6` <chr>,\n#   `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, `WEMWBS 9` <chr>, `WEMWBS 10` <chr>, …"
  },
  {
    "objectID": "03_transforming-data.html#inverting-nas",
    "href": "03_transforming-data.html#inverting-nas",
    "title": "3  Transforming Data",
    "section": "9.6 Inverting NAs",
    "text": "9.6 Inverting NAs\nIf we want values that AREN’T NAs, then we combine is.na() and the NOT (!) operator.\n\nraw_data |> filter(!is.na(progress))\n\n# A tibble: 24 × 47\n   PID   start_time  end_t…¹ progr…² Gender   Age Region Marit…³ Livin…⁴ Prima…⁵\n   <chr> <chr>       <chr>   <chr>   <chr>  <dbl> <chr>  <chr>   <chr>   <chr>  \n 1 26    2018-03-22… 2018-0… FINISH  M         54 East   Married With w… None   \n 2 27    2018-03-26… 2018-0… no      M         54 Scotl… Married With w… None   \n 3 28    2018-03-21… 2018-0… END     F         37 South… Married With h… None   \n 4 29    2018-03-25… 2018-0… ethics  M         51 West … Single  Alone   None   \n 5 30    2018-03-24… 2018-0… ethics  M         52 Scotl… Married With w… Wife   \n 6 A2    2018-03-21… 2018-0… END     male      39 North… Married Wife w… None   \n 7 A3    2018-03-25… 2018-0… ethics  M         47 North… Married Wife w… Wife i…\n 8 C     2018-03-24… 2018-0… ethics  M         53 Norfo… With P… Partner None   \n 9 I     2018-03-23… 2018-0… END     M         50 Scotl… In rel… Live w… None   \n10 J     2018-03-25… 2018-0… END     M         46 Scotl… Single  By self None   \n# … with 14 more rows, 37 more variables: Employment <chr>,\n#   `Time Since BI` <chr>, `BI Severity` <chr>, Vision <chr>, Speech <chr>,\n#   Motor <chr>, Memory <chr>, Cognitive <chr>, Pain <chr>, BRS1 <dbl>,\n#   BRS2 <dbl>, BRS3 <dbl>, BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>,\n#   DJG_2 <dbl>, DJG_3 <dbl>, DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>,\n#   `WEMWBS 1` <chr>, `WEMWBS 2` <chr>, `WEMWBS 3` <chr>, `WEMWBS 4` <chr>,\n#   `WEMWBS 5` <chr>, `WEMWBS 6` <chr>, `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, …"
  },
  {
    "objectID": "03_transforming-data.html#arranging-data-1",
    "href": "03_transforming-data.html#arranging-data-1",
    "title": "3  Transforming Data",
    "section": "10.1 Arranging Data",
    "text": "10.1 Arranging Data\nThis also works with desc() so you can sort by descending order.\n\n\n\nraw_data |> \n  select(Age) |> \n  arrange(Age)\n\n# A tibble: 29 × 1\n     Age\n   <dbl>\n 1    24\n 2    28\n 3    33\n 4    37\n 5    39\n 6    39\n 7    46\n 8    47\n 9    47\n10    49\n# … with 19 more rows\n\n\n\n\nraw_data |> \n  select(Age) |> \n  arrange(desc(Age))\n\n# A tibble: 29 × 1\n     Age\n   <dbl>\n 1    68\n 2    68\n 3    64\n 4    61\n 5    56\n 6    55\n 7    54\n 8    54\n 9    53\n10    53\n# … with 19 more rows"
  },
  {
    "objectID": "03_transforming-data.html#summarising-data-1",
    "href": "03_transforming-data.html#summarising-data-1",
    "title": "3  Transforming Data",
    "section": "11.1 Summarising Data",
    "text": "11.1 Summarising Data\n\n\nEither within each operation.\n\nraw_data |> \n  summarise(\n    age_mean = \n      mean(Age, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 1\n  age_mean\n     <dbl>\n1     49.2\n\n\n\nOr within the chain of functions. Note this only keeps complete rows if several columns are provided.\n\nraw_data |> \n  drop_na(Age) |> \n  summarise(\n    age_mean = mean(Age)\n  )\n\n# A tibble: 1 × 1\n  age_mean\n     <dbl>\n1     49.2\n\n\n\n\nWe can create many columns within our new summary table made up of operations on our original data. Here, we will create a mean and standard deviation for age along with a count of the number of observations we have in the data set (excluding NAs) using n().\n\nraw_data |> \n  drop_na(Age) |> \n  summarise(\n    age_mean = mean(Age),\n    age_sd = sd(Age),\n    n = n()\n  )\n\n# A tibble: 1 × 3\n  age_mean age_sd     n\n     <dbl>  <dbl> <int>\n1     49.2   11.1    25"
  },
  {
    "objectID": "03_transforming-data.html#summarising-data-2",
    "href": "03_transforming-data.html#summarising-data-2",
    "title": "3  Transforming Data",
    "section": "11.2 Summarising Data",
    "text": "11.2 Summarising Data\n\n\nEither within each operation.\n\nraw_data |> \n  summarise(\n    age_mean = \n      mean(Age, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 1\n  age_mean\n     <dbl>\n1     49.2\n\n\n\nOr within the chain of functions. Note this only keeps complete rows if several columns are provided.\n\nraw_data |> \n  drop_na(Age) |> \n  summarise(\n    age_mean = mean(Age)\n  )\n\n# A tibble: 1 × 1\n  age_mean\n     <dbl>\n1     49.2"
  },
  {
    "objectID": "03_transforming-data.html#many-summaries",
    "href": "03_transforming-data.html#many-summaries",
    "title": "3  Transforming Data",
    "section": "11.3 Many Summaries",
    "text": "11.3 Many Summaries\nWe can add many summaries to our table and even count the observations with n()\n\nraw_data |> \n  drop_na(Age) |> \n  summarise(\n    age_mean = mean(Age),\n    age_sd = sd(Age),\n    n = n()\n  )\n\n# A tibble: 1 × 3\n  age_mean age_sd     n\n     <dbl>  <dbl> <int>\n1     49.2   11.1    25"
  },
  {
    "objectID": "03_transforming-data.html#group_by",
    "href": "03_transforming-data.html#group_by",
    "title": "3  Transforming Data",
    "section": "12.1 group_by()",
    "text": "12.1 group_by()\nOne of the most powerful functions of dplyr is group_by(). It will give us grouped operations.\nThis nicely highlights some coding problems we need to fix…\n\nraw_data |> \n  group_by(Gender) |> \n  summarise(age_mean = mean(Age, na.rm = TRUE))\n\n# A tibble: 5 × 2\n  Gender age_mean\n  <chr>     <dbl>\n1 F          42.2\n2 Female     53  \n3 M          53.4\n4 male       39  \n5 <NA>      NaN"
  },
  {
    "objectID": "03_transforming-data.html#group_by-with-other-dplyr-functions",
    "href": "03_transforming-data.html#group_by-with-other-dplyr-functions",
    "title": "3  Transforming Data",
    "section": "12.2 group_by() with other dplyr functions",
    "text": "12.2 group_by() with other dplyr functions\n\ngroup_by() works with a whole host of functions from dplyr including filter(), mutate() and others.\nLet’s filter to the earliest observation in start_time for the levels of Gender.\n\n\nraw_data |> \n  mutate(start_time = ymd_hms(start_time)) |> \n  group_by(Gender) |> \n  filter(rank(start_time) == 1)\n\n# A tibble: 5 × 47\n# Groups:   Gender [5]\n  PID   start_time          end_time progr…¹ Gender   Age Region Marit…² Livin…³\n  <chr> <dttm>              <chr>    <chr>   <chr>  <dbl> <chr>  <chr>   <chr>  \n1 A2    2018-03-21 03:23:57 2018-03… END     male      39 North… Married Wife w…\n2 M1    2018-03-26 18:28:33 2018-03… END     Female    53 North… Single  By self\n3 T1    2018-03-21 19:23:38 2018-03… END     M         47 East … With p… Partne…\n4 W1    2018-03-21 04:04:38 2018-03… END     F         33 North… Married Spouse \n5 <NA>  NA                  <NA>     <NA>    <NA>      NA <NA>   <NA>    <NA>   \n# … with 38 more variables: `Primary Carer` <chr>, Employment <chr>,\n#   `Time Since BI` <chr>, `BI Severity` <chr>, Vision <chr>, Speech <chr>,\n#   Motor <chr>, Memory <chr>, Cognitive <chr>, Pain <chr>, BRS1 <dbl>,\n#   BRS2 <dbl>, BRS3 <dbl>, BRS4 <dbl>, BRS5 <dbl>, BRS6 <dbl>, DJG_1 <dbl>,\n#   DJG_2 <dbl>, DJG_3 <dbl>, DJG_4 <dbl>, DJG_5 <dbl>, DJG_6 <dbl>,\n#   `WEMWBS 1` <chr>, `WEMWBS 2` <chr>, `WEMWBS 3` <chr>, `WEMWBS 4` <chr>,\n#   `WEMWBS 5` <chr>, `WEMWBS 6` <chr>, `WEMWBS 7` <chr>, `WEMWBS 8` <chr>, …"
  },
  {
    "objectID": "03_transforming-data.html#ungrouping",
    "href": "03_transforming-data.html#ungrouping",
    "title": "3  Transforming Data",
    "section": "12.3 Ungrouping",
    "text": "12.3 Ungrouping\nIf you want to perform further operations on the whole data set after your grouped operations, you need to remember to ungroup!\n\n\nWithout ungrouping.\n\nraw_data |> \n  mutate(start_time = ymd_hms(start_time)) |> \n  group_by(Gender) |> \n  filter(rank(start_time) == 1) |> \n  summarise(mean_age = mean(Age, na.rm = TRUE))\n\n# A tibble: 5 × 2\n  Gender mean_age\n  <chr>     <dbl>\n1 F            33\n2 Female       53\n3 M            47\n4 male         39\n5 <NA>        NaN\n\n\n\nWith ungrouping.\n\nraw_data |> \n  mutate(start_time = ymd_hms(start_time)) |> \n  group_by(Gender) |> \n  filter(rank(start_time) == 1) |> \n  ungroup() |> \n  summarise(mean_age = mean(Age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  mean_age\n     <dbl>\n1       43"
  },
  {
    "objectID": "03_transforming-data.html#chaining-functions",
    "href": "03_transforming-data.html#chaining-functions",
    "title": "3  Transforming Data",
    "section": "3.11 Chaining Functions",
    "text": "3.11 Chaining Functions\nWe’ve seen already how we can chain many functions together using the pipe. We can combine all functions we’ve covered to do all our data transformation. We can even pipe the result of these chains into ggplot() calls. In the Section 3.13 we’ll do this, fixing all of the problems with our data before we produce any summaries or plots using the functions you’ve learned about in this chapter."
  },
  {
    "objectID": "03_transforming-data.html#saving-files",
    "href": "03_transforming-data.html#saving-files",
    "title": "3  Transforming Data",
    "section": "3.12 Saving Files",
    "text": "3.12 Saving Files\nFinally, once you’re done cleaning your data and making summaries you can save it using the write_*() family of functions. There are the same number of write_*() functions corresponding to the read_*() functions listed above for each package. I suggest saving to a .csv file with write_csv() as any program can open .csv files. As mentioned earlier, it’s a good idea to keep your raw and cleaned data apart from one another, even in separate sub-folders. Here we save our data to the “cleaned_data” sub-folder. We should probably do this on actually cleaned data, but this is just an example. You will do this properly in Section 3.13.\n\nwrite_csv(raw_data, here(\"cleaned_data\", \"bi_loneliness.csv\"))"
  },
  {
    "objectID": "03_transforming-data.html#exercises",
    "href": "03_transforming-data.html#exercises",
    "title": "3  Transforming Data",
    "section": "12.6 Exercises",
    "text": "12.6 Exercises\nPlease complete the exercises at https://github.com/gpwilliams/ds-psych_course."
  },
  {
    "objectID": "03_transforming-data.html#reading-data-into-r-from-an-external-source",
    "href": "03_transforming-data.html#reading-data-into-r-from-an-external-source",
    "title": "3  Transforming Data",
    "section": "4.1 Reading Data into R from an External Source",
    "text": "4.1 Reading Data into R from an External Source\nThe readr package in the tidyverse comes with several functions using the read_*() prefix. These are focused on reading in files that aren’t saved using a proprietary format. Generally, it’s a good idea to use one of these formats as you can be sure anyone, anywhere with access to a computer can work with these files. While many have access to software like Microsoft Excel, you’re still excluding those who don’t have access to this paid software if you share data in this format. For that reason, to engage properly in Open Science it’s best to use a file format like .csv to allow the greatest number of people to access your data. Here are the open file formats supported by readr:\nThe readr package allows you to read rectangular data into R from delimited files (e.g. .csv, .tsv). Here are just a few of the main functions available to you in readr:\n\nread_csv(): comma-separated values (CSV).\nread_tsv(): tab-separated values (TSV).\nread_csv2(): semicolon-separated values with , as the decimal mark.\nread_delim(): delimited files (CSV and TSV are important special cases).\nread_table(): whitespace-separated files.\n\nSometimes, however, working with proprietary data formats can’t be avoided. In this case, we’re not out of luck. We can use the haven package to read these files into R. From here, it’s good practice to save the file in an open format. haven gives you access to:\nIf you’re working proprietary data formats, use haven which has:\n\nread_sav(): SPSS data files.\nread_dta: Stata data files.\nread_sas(): SAS data files.\n\nIf working with Microsoft Excel files, you can use readxl which has the read_excel() function and others for working with older Excel file types.\nAll of these functions expect the path to your file as the input to the function. If using an RStudio Project we can specify a file path to the file in our project folder using relative file paths to increase the computational reproducibility of our reports.\nFirst we need some data. I made a messy version of real data from Dunne et al. (2023): Uncovering the social determinants of brain injury rehabilitation1. This data is stored as a .csv file in https://github.com/gpwilliams/ds-psych_course. To follow along, download the repository from GitHub and open the exercises file in 03_transforming-data or make your own Quarto document within the folder.\n\n4.1.1 Reading Our Data\nLoad the packages once per session. Ensure that you have already installed each package once on your computer using install.packages() before you try to load the packages using library().\n\nlibrary(tidyverse) # readr, dplyr and other packages\nlibrary(here) # working with file paths\nlibrary(janitor) # cleaning column names\nlibrary(lubridate) # working with datetimes\n\nAssuming you’re working from the ds-psych_course folder, read the data from the data sub-folder. When we assign this data to the object raw_data we have created a tibble within R that we can work with.\n\n\n\n\n\n\nNote\n\n\n\nNote that when working with your data set within R this doesn’t change the external data you’ve just read in. You can only change this data if you overwrite it using one of the write_*() functions. Avoid doing that and you’re safe to break the data in any way and still have a recoverable copy of your data. For this reason it’s encouraged to leave your raw data untouched and only save your transformed, cleaned, or filtered data to new files.\n\n\n\nraw_data <- read_csv(here(\"data\", \"bi-loneliness.csv\"))\n\n\n\nRows: 29 Columns: 45\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (32): PID, time, progress, Gender, Region, Marital Status, Living Arrang...\ndbl (13): Age, BRS1, BRS2, BRS3, BRS4, BRS5, BRS6, DJG_1, DJG_2, DJG_3, DJG_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou will get a message telling you about the data types that have been detected for each column. Nicely, readr looks up each column and determines a data type for the columns. If a column contains numbers it’s saved as a double (numeric) column. If a column contains even one character, it will be read in as a character column. This can be changed manually if you need to change data types. We will look at this later in this chapter."
  },
  {
    "objectID": "03_transforming-data.html#grouped-operations",
    "href": "03_transforming-data.html#grouped-operations",
    "title": "3  Transforming Data",
    "section": "3.10 Grouped Operations",
    "text": "3.10 Grouped Operations\nOften we might want to have these summaries made up for specific groups within our data set. We can perform grouped operations using many of the tidyverse functions. For example, we can sum up values for one column for each unique value in another column using group_by().\n\n\n\nGrouping by unique values in one column to produce a summary from another column\n\n\nWe pass to group_by() the name of the column containing our groups, and then proceed with our summary() function (or others) to perform the grouped operation. Let’s get the mean ages for each gender in our study.\n\nraw_data |> \n  group_by(Gender) |> \n  summarise(age_mean = mean(Age, na.rm = TRUE))\n\n\n\n  \n\n\n\nWe have the grouped ages, but we have some coding issues in our data. We probably wan’t consistent labels for men and women, rather than splitting them across male/M and Female/F. We should fix this ahead of time using a conditional in mutate().\n\n3.10.1 group_by() with other dplyr functions\ngroup_by() works with a whole host of functions from dplyr including filter(), mutate() and others. To see how this might work, let’s filter our data to the earliest observation in start_time within each level of Gender. We’ll use the rank() function to rank order the start_time variable within each gender, and keep only the observations where rank is equal to 1.\n\nraw_data |> \n  mutate(start_time = ymd_hms(start_time)) |> \n  group_by(Gender) |> \n  filter(rank(start_time) == 1)\n\n\n\n  \n\n\n\nGreat, we pulled out one value for each label in our data!\n\n\n3.10.2 Ungrouping\nIf you want to perform further operations on the whole data set after your grouped operations, you need to remember to ungroup. Otherwise, your results might not be as expected. Compare the two outputs below.\n\n\nWithout ungrouping.\n\nraw_data |> \n  mutate(\n    start_time = \n      ymd_hms(start_time)\n  ) |> \n  group_by(Gender) |> \n  filter(rank(start_time) == 1) |> \n  summarise(\n    mean_age = mean(\n      Age, na.rm = TRUE\n    )\n  )\n\n\n\n  \n\n\n\n\nWith ungrouping.\n\nraw_data |> \n  mutate(\n    start_time = \n      ymd_hms(start_time)\n  ) |> \n  group_by(Gender) |> \n  filter(rank(start_time) == 1) |> \n  ungroup() |> \n  summarise(\n    mean_age = mean(\n      Age, na.rm = TRUE\n    )\n  )"
  },
  {
    "objectID": "03_transforming-data.html#sec-exercises",
    "href": "03_transforming-data.html#sec-exercises",
    "title": "3  Transforming Data",
    "section": "3.13 Exercises",
    "text": "3.13 Exercises\nPlease complete the exercises at https://github.com/gpwilliams/ds-psych_course."
  },
  {
    "objectID": "03_transforming-data.html#reading-data",
    "href": "03_transforming-data.html#reading-data",
    "title": "3  Transforming Data",
    "section": "3.1 Reading Data",
    "text": "3.1 Reading Data\nPrior to this chapter, we’ve used inbuilt data sets or created data within R to show off the functionality of R. However, in a real research context you’ll often be required to work with data from an external file. Thankfully, several packages allow us to read in and write to a wide range of files.\n\n3.1.1 Reading Data into R from an External Source\nThe readr package in the tidyverse comes with several functions using the read_*() prefix. These are focused on reading in files that aren’t saved using a proprietary format. Generally, it’s a good idea to use one of these formats as you can be sure anyone, anywhere with access to a computer can work with these files. While many have access to software like Microsoft Excel, you’re still excluding those who don’t have access to this paid software if you share data in this format. For that reason, to engage properly in Open Science it’s best to use a file format like .csv to allow the greatest number of people to access your data. Here are the open file formats supported by readr:\nThe readr package allows you to read rectangular data into R from delimited files (e.g. .csv, .tsv). Here are just a few of the main functions available to you in readr:\n\nread_csv(): comma-separated values (CSV).\nread_tsv(): tab-separated values (TSV).\nread_csv2(): semicolon-separated values with , as the decimal mark.\nread_delim(): delimited files (CSV and TSV are important special cases).\nread_table(): whitespace-separated files.\n\nSometimes, however, working with proprietary data formats can’t be avoided. In this case, we’re not out of luck. We can use the haven package to read these files into R. From here, it’s good practice to save the file in an open format. haven gives you access to:\nIf you’re working proprietary data formats, use haven which has:\n\nread_sav(): SPSS data files.\nread_dta: Stata data files.\nread_sas(): SAS data files.\n\nIf working with Microsoft Excel files, you can use readxl which has the read_excel() function and others for working with older Excel file types.\nAll of these functions expect the path to your file as the input to the function. If using an RStudio Project we can specify a file path to the file in our project folder using relative file paths to increase the computational reproducibility of our reports.\nFirst we need some data. I made a messy version of real data from Dunne et al. (2023): Uncovering the social determinants of brain injury rehabilitation1. This data is stored as a .csv file in https://github.com/gpwilliams/ds-psych_course. To follow along, download the repository from GitHub and open the exercises file in 03_transforming-data or make your own Quarto document within the folder.\n\n3.1.1.1 Reading Our Data\nLoad the packages once per session. Ensure that you have already installed each package once on your computer using install.packages() before you try to load the packages using library().\n\nlibrary(tidyverse) # readr, dplyr and other packages\nlibrary(here) # working with file paths\nlibrary(janitor) # cleaning column names\nlibrary(lubridate) # working with datetimes\n\nAssuming you’re working from the ds-psych_course folder, read the data from the data sub-folder. When we assign this data to the object raw_data we have created a tibble within R that we can work with.\n\n\n\n\n\n\nNote\n\n\n\nNote that when working with your data set within R this doesn’t change the external data you’ve just read in. You can only change this data if you overwrite it using one of the write_*() functions. Avoid doing that and you’re safe to break the data in any way and still have a recoverable copy of your data. For this reason it’s encouraged to leave your raw data untouched and only save your transformed, cleaned, or filtered data to new files.\n\n\n\nraw_data <- read_csv(here(\"data\", \"bi-loneliness.csv\"))\n\n\n\nRows: 29 Columns: 45\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (32): PID, time, progress, Gender, Region, Marital Status, Living Arrang...\ndbl (13): Age, BRS1, BRS2, BRS3, BRS4, BRS5, BRS6, DJG_1, DJG_2, DJG_3, DJG_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou will get a message telling you about the data types that have been detected for each column. Nicely, readr looks up each column and determines a data type for the columns. If a column contains numbers it’s saved as a double (numeric) column. If a column contains even one character, it will be read in as a character column. This can be changed manually if you need to change data types. We will look at this later in this chapter.\n\n\n\n3.1.2 Inspecting Your Data\nWith any data set, it’s important to understand the columns. Either print it out by typing the name of the data set, here raw_data in the console, or print the transposed data set with glimpse()\n\nglimpse(raw_data)\n\nRows: 29\nColumns: 45\n$ PID                   <chr> \"26\", \"27\", \"28\", \"29\", \"30\", \"A2\", \"A3\", \"C\", \"…\n$ time                  <chr> \"2018-03-22 23:06:11_2018-03-23 00:25:51\", \"2018…\n$ progress              <chr> \"FINISH\", \"no\", \"END\", \"ethics\", \"ethics\", \"END\"…\n$ Gender                <chr> \"M\", \"M\", \"F\", \"M\", \"M\", \"male\", \"M\", \"M\", \"M\", …\n$ Age                   <dbl> 54, 54, 37, 51, 52, 39, 47, 53, 68, 50, 46, 55, …\n$ Region                <chr> \"East\", \"Scotland\", \"South East\", \"West Yorkshir…\n$ `Marital Status`      <chr> \"Married\", \"Married\", \"Married\", \"Single\", \"Marr…\n$ `Living Arrangements` <chr> \"With wife\", \"With wife\", \"With husband and todd…\n$ `Primary Carer`       <chr> \"None\", \"None\", \"None\", \"None\", \"Wife\", \"None\", …\n$ Employment            <chr> \"Medically Retired\", \"Medically Retired\", \"Long …\n$ `Time Since BI`       <chr> \"6 (2015)\", \"3 (2018)\", \"5 (2016)\", \"23 (1998)\",…\n$ `BI Severity`         <chr> \"Mild\", \"Severe\", \"Mild\", \"Moderate\", \"Severe\", …\n$ Vision                <chr> NA, NA, \"X\", NA, \"X\", \"x\", NA, NA, \"X\", NA, NA, …\n$ Speech                <chr> \"X\", \"X\", NA, NA, NA, \"x\", NA, NA, \"X\", \"X\", \"X\"…\n$ Motor                 <chr> NA, \"X\", \"X\", NA, NA, \"x\", \"X\", NA, \"X\", \"X\", \"X…\n$ Memory                <chr> NA, NA, NA, NA, \"X\", NA, \"X\", NA, \"X\", \"X\", NA, …\n$ Cognitive             <chr> \"X\", \"X\", NA, NA, \"X\", NA, NA, \"X\", NA, \"X\", NA,…\n$ Pain                  <chr> NA, NA, \"X\", NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ BRS1                  <dbl> 5, 4, 3, 4, 2, 5, 5, 4, 5, 5, 4, 4, 2, 4, NA, 4,…\n$ BRS2                  <dbl> 4, 4, 2, 4, 1, 5, 5, 1, 4, 4, 2, 2, 2, 1, NA, 2,…\n$ BRS3                  <dbl> 3, 4, 1, 4, 5, 2, 5, 4, 4, 5, 2, 2, 2, 2, NA, 5,…\n$ BRS4                  <dbl> 4, 4, 2, 4, 2, 5, 5, 5, 4, 4, 4, 4, 2, 4, NA, 4,…\n$ BRS5                  <dbl> 4, 2, 2, 3, 1, 4, 5, 4, 4, 3, 4, 4, 2, 2, NA, 4,…\n$ BRS6                  <dbl> 5, 2, 1, 3, 2, 4, 5, 5, 4, 4, 2, 5, 3, 5, NA, 4,…\n$ DJG_1                 <dbl> 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, NA, 1,…\n$ DJG_2                 <dbl> 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, NA, 1,…\n$ DJG_3                 <dbl> 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1,…\n$ DJG_4                 <dbl> 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, NA, 0,…\n$ DJG_5                 <dbl> 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, NA, 1,…\n$ DJG_6                 <dbl> 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, NA, 0,…\n$ `WEMWBS 1`            <chr> \"2 - Rarely\", \"3 - Some of the time\", \"3 - Some …\n$ `WEMWBS 2`            <chr> \"4 - Often\", \"2 - Rarely\", \"2 - Rarely\", \"2 - Ra…\n$ `WEMWBS 3`            <chr> \"4 - Often\", \"3 - Some of the time\", \"2 - Rarely…\n$ `WEMWBS 4`            <chr> \"3 - Some of the time\", \"4 - Often\", \"1 - None o…\n$ `WEMWBS 5`            <chr> \"1 - None of the time\", \"2 - Rarely\", \"1 - None …\n$ `WEMWBS 6`            <chr> \"5 - All of the time\", \"3 - Some of the time\", \"…\n$ `WEMWBS 7`            <chr> \"4 - Often\", \"3 - Some of the time\", \"4 - Often\"…\n$ `WEMWBS 8`            <chr> \"3 - Some of the time\", \"2 - Rarely\", \"2 - Rarel…\n$ `WEMWBS 9`            <chr> \"2 - Rarely\", \"3 - Some of the time\", \"3 - Some …\n$ `WEMWBS 10`           <chr> \"3 - Some of the time\", \"2 - Rarely\", \"2 - Rarel…\n$ `WEMWBS 11`           <chr> \"5 - All of the time\", \"3 - Some of the time\", \"…\n$ `WEMWBS 12`           <chr> \"3 - Some of the time\", \"3 - Some of the time\", …\n$ `WEMWBS 13`           <chr> \"1 - None of the time\", \"2 - Rarely\", \"1 - None …\n$ `WEMWBS 14`           <chr> \"3 - Some of the time\", \"2 - Rarely\", \"3 - Some …\n$ enjoymentFOLLOW       <chr> \"7-no\", \"6-yes\", \"4-yes\", \"0-no\", \"4-no\", \"5-no\"…\n\n\nThat’s a lot of columns! Broadly, this data set has one row of data for each participant and tracks some of the demographics of the participants. As the data set concerns brain injury survivors, we also have information on the traumatic brain injuries of the participants, including the time since the brain injury and severity of brain injury. Along with this we have columns coding for types of impairments or disturbances as a result of brain injury (i.e. visual, speech, motor, memory, or cognitive, pain). Finally, we have ratings for two questionnaires, the BRS (Brief Resilience Scale), DJG (De Jong Gierveld scale for emotional and social loneliness), and WEMWBS (The Warwick-Edinburgh Mental Wellbeing Scales). We also have a closing question in one column that encodes two variables: whether they enjoyed the study and if they would be contactable in a follow up. We need to fix a number of issues with this data.\nAt this stage it is a good idea to take some time to really understand our data. What are the unique values in each column? What are the issues with the presentation that might make working with the data difficult? Using plots is a good way to visually inspect each variable.\nTo follow along with these examples, you might want to subset the data to remove some columns that aren’t needed. We’ll remove the WEMWBS and BRS columns along with some demographic columns just to make the print out easier to parse. At the end of this chapter, the following code will make sense:\n\nraw_data <- raw_data |> \n  select(\n    -c(contains(\"WEMWBS\"), contains(\"BRS\")), \n    -c(Region:`BI Severity`)\n  )"
  },
  {
    "objectID": "03_transforming-data.html#separating-columns",
    "href": "03_transforming-data.html#separating-columns",
    "title": "3  Transforming Data",
    "section": "3.3 Separating Columns",
    "text": "3.3 Separating Columns\nOne of the glaring issues with this data set is that we have one column, enjoymentFOLLOW which encodes two variables, whether people enjoyed the study and if they’d be contactable at a follow up, and time that encodes the start and end times for participants in the study. We can use the separate() function to split a column into two or more columns, depending on how many separate sources of data the function detects or is instructed to detect.\n\n\n\nSeparating a column which consists of two variables\n\n\nseparate() looks for separators between data points, e.g. _ or -, and splits columns there. So, if we have many separators within our data, but we only want to split the columns at one of them, we need to be explicit in where to split the data.\nLet’s fix the messy data, starting with splitting columns that contain multiple variables. First, we’ll start with enjoymentFOLLOW which has only one separator, _ between the values in the column. Using the function without specifying the separator works fine in this instance. We just need to specify the names of the new columns.\n\n\n\n\n\n\nImportant\n\n\n\nUse the arrow on the table to see all columns and the pages to see all rows.\n\n\n\nraw_data |> \n  separate(enjoymentFOLLOW, into = c(\"enjoyment\", \"follow_up\"))\n\n\n\n  \n\n\n\nWe now have separate columns for each variable. Next, we need to fix time. What happens if we don’t specify a separator?\n\nraw_data |> \n  separate(time, into = c(\"start_time\", \"end_time\"))\n\nWarning: Expected 2 pieces. Additional pieces discarded in 25 rows [1, 2, 3, 4, 5, 6, 7,\n8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].\n\n\n\n\n  \n\n\n\nWe have start and end times, but these don’t look correct. We also have a warning saying that due to our names separate() expected two rows but created (and dropped) 25! That’s because datetimes are made up of many separators, e.g. “2018-03-22 23:06:11”. In this instance, the start and end times are separated by an underscore, e.g. “2018-03-22 23:06:11_2018-03-23 00:25:51”. We can use this to tell separate() to only split the data at this separator.\n\nraw_data |> \n  separate(col = time, into = c(\"start_time\", \"end_time\"), sep = \"_\")\n\n\n\n  \n\n\n\nNow we have the expected output.\nTo put all of this together, we can chain multiple separation calls together with the pipe. Notice that unless we assign the result back to the variable we haven’t actually changed our data set. So, we’ll do this now.\nAgain, we use glimpse() to view all columns.\n\nraw_data <- raw_data |> \n  separate(col = enjoymentFOLLOW, into = c(\"enjoyment\", \"follow_up\")) |> \n  separate(col = time, into = c(\"start_time\", \"end_time\"), sep = \"_\")"
  },
  {
    "objectID": "03_transforming-data.html#selecting-columns",
    "href": "03_transforming-data.html#selecting-columns",
    "title": "3  Transforming Data",
    "section": "3.4 Selecting Columns",
    "text": "3.4 Selecting Columns\nOften your data set has lots of columns you won’t use. This can make working with the data more difficult and takes up memory on our computer which can slow things down. We can choose which columns to keep by listing them by name or by position within the select() function.\n\n\n\nSelecting specific columns from the data set\n\n\n\n3.4.1 Selecting Columns to Keep\nBy default, select() works by keeping the columns that you list within it. Here are two examples where we use the bare column names and the index to keep the columns we’d like.\n\nraw_data |> \n  select(PID, Gender, Age)\n\n\n\n  \n\n\n\n\nraw_data |> \n  select(c(1, 5, 6))\n\n\n\n  \n\n\n\n\n\n3.4.2 Selecting Columns to Remove\nWe can also select columns to remove by using - before the names or index. We can list these as between a range, e.g. 6:47 or c(Age:follow_up), or as a specific vector of indices, e.g. c(1, 2, 3) or c(PID, start_time, end_time). An example is shown below.\n\nraw_data |> \n  select(-c(Age:follow_up))\n\n\n\n  \n\n\n\n\n\n3.4.3 Select Helpers\nFor simple cases, stating the name or index of the column is fine. However, if we’re working with many columns that have a consistent naming pattern, we can make our code shorter and perhaps avoid more mistakes by using select helper functions. There are a number of select() helpers to make working with it easier:\n\nstarts_with(): keep columns starting with a specific string.\nends_with(): keep columns ending with a specific string.\ncontains(): keep columns containing a specific string.\nmatches(): Keeps columns matching a regular expression. This is useful for complex matching.\nnum_range(): Keeps columns with a matching prefix and a following range of numbers.\n\nContains is a very common select helper that you might need to use. Here, we want to select all columns that contain the string “DJG”, corresponding to the De Jong Gierveld scale for emotional and social loneliness. These columns correspond to specific questions from this scale.\n\nraw_data |> \n  select(contains(\"DJG\"))\n\n\n\n  \n\n\n\nHowever, if we want to select only a subset of those which match a prefix but end in only specific values, we can use the num_range() function. Let’s keep columns that begin with “DJG_” and end with the values 1, 2, or 3.\n\nraw_data |> \n  select(num_range(\"DJG_\", 1:3))"
  },
  {
    "objectID": "03_transforming-data.html#renaming-and-reordering-columns",
    "href": "03_transforming-data.html#renaming-and-reordering-columns",
    "title": "3  Transforming Data",
    "section": "3.5 Renaming and Reordering Columns",
    "text": "3.5 Renaming and Reordering Columns\n\n3.5.1 Renaming Columns\nOften when reading data into R we might want to change the column names, either because they are specified so that working with them is more difficult (e.g. those containing spaces such as `Time Since BI` which requires us to use back ticks around names to access them, e.g. raw_data$`Time Since BI`) or because they are excessively long or uninformative.\nWe can use the rename() function to change the name of columns. This takes the format of new = old.\n\n\n\nRenaming columns\n\n\nHere we will make the PID column more informative by making it participant_id.\n\nraw_data |> \n  rename(participant_id = PID)\n\n\n\n  \n\n\n\n\n\n3.5.2 Renaming Columns Automatically\nThe janitor package has a function, clean_names(), which fixes irregularities in names. Mainly, it’s good practice to keep one format such as snake_case, to avoid spaces in names, and to avoid numbers at the start of column names.\n\nraw_data |> \n  janitor::clean_names()\n\n\n\n  \n\n\n\n\n\n3.5.3 Reordering Columns\nWe can also use select() to reorder columns. Either specify the names or indices of every column you’d like in the order you’d like, or you can use the everything() helper function to make this easier. To use this function, we simply state the order of columns we’d like in a specific order, then use everything() to keep every other column in the data set after those specified in the specific order.\n\nraw_data |> \n  select(PID, Gender, Age, start_time, end_time, everything())"
  },
  {
    "objectID": "03_transforming-data.html#creating-and-changing-columns",
    "href": "03_transforming-data.html#creating-and-changing-columns",
    "title": "3  Transforming Data",
    "section": "3.6 Creating and Changing Columns",
    "text": "3.6 Creating and Changing Columns\nIf we want to create or change a column, we have two options: mutate() and transmute():\n\nmutate(): changes or creates a column, keeping all existing columns.\ntransmute(): changes or creates a column, dropping columns not defined in transmute().\n\nThroughout this book we’ll mainly use mutate() as it’s rare to only want to work with columns you’ve just created or changed. mutate() can consist of a simple operation on one column, using many columns to create another, or doing conditional operations within a column.\nHere we create a new column by adding together values from columns 1 and y.\n\n\n\nUsing mutate to produce a new column based on operations on existing columns\n\n\nLet’s look at working out the birth years of the participants in the data set. The study concluded in 2022 (despite what the start_date and end_date columns tell us), so we can take the year 2022 and subtract participant ages to find their birth year.\n\n\n\n\n\n\nNote\n\n\n\nBelow we use select() only to restrict the printed output for ease of viewing.\n\n\n\nraw_data |> \n  mutate(\n    birth_year = 2022 - Age\n  ) |> \n  select(PID, birth_year)\n\n\n\n  \n\n\n\nWe can also create columns by combining operations across multiple existing columns. Here, we create a total score for the De Jong Gierveld scale by adding each of the scores on the individual items.\nBelow we use select() only to restrict the printed output for ease of viewing.\n\nraw_data |> \n  mutate(\n    djg_total = DJG_1 + DJG_2 + DJG_3 + DJG_4 + DJG_5 + DJG_6\n  ) |> \n  select(contains(\"DJG\"))\n\n\n\n  \n\n\n\nOften, however, we might want to update columns by transforming values within them.\n\n\n\nUsing mutate to overwrite or update values in a column\n\n\nIn our data set we have the problem that date times are stored as character vectors rather than date times. This means that we can’t perform any mathematical operations on the values in these columns.\nIf we try to get the total time participants took in the study like so, it won’t work.\n\nraw_data |> \n  mutate(total_time = end_time - start_time) |> \n  select(contains(\"time\"))\n\nThis would also be the case if we wanted to perform a mathematical operation using a numeric and non-numeric column. To fix this, we change the data type.\nLet’s update the start_time and end_time columns, converting them to date time data types. We’ll use the lubridate package for this which has the function ymd_hms() which parses character vectors in the format of year-month-day_hours-minutes-seconds to a proper date time data type.\nNotice that we can also create a new column, total_time by subtracting start_time from end_time within the same mutate() call that we use to convert their data types. This column goes to the end of our data set.\n\nraw_data |> \n  mutate(\n    start_time = ymd_hms(start_time),\n    end_time = ymd_hms(end_time),\n    total_time = end_time - start_time\n  ) |> \n  select(contains(\"time\"))\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf we don’t like working with decimal hours, we can change the units to minutes using e.g. units(TIBBLE$COL) <- \"mins\"\n\n\n\n3.6.1 Improving Multi-Column Operations\nListing each of the columns we’d like to use when performing operations on the columns can be tedious. Let’s say we want to sum up every value in the DJG columns. How might we do this? You might assume we can use the : operator to select only the columns between DJG_1 and DJG_6 as follows:\n\nraw_data |> \n  mutate(DJG_sum = sum(DJG_1:DJG_6)) |> \n  select(contains(\"DJG\"))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `DJG_sum = sum(DJG_1:DJG_6)`.\nCaused by warning in `DJG_1:DJG_6`:\n! numerical expression has 29 elements: only the first used\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\n\n  \n\n\n\nHowever, we got a warning and very unexpected output. That’s because the sum() function (and others) aren’t set up for row-wise operations. Do to this, we can use the across() function to tell mutate() that we want to perform a row-wise operation. However, c() also doesn’t work as expected here as it doesn’t like the bare or quoted column names we pass to the function. So, we update this to use c_across() to work with the bare column names.\nAgain, we use select() only to restrict the print output for ease of viewing.\n\nraw_data |> \n  rowwise() |> \n  mutate(DJG_sum = sum(c_across(DJG_1:DJG_6))) |> \n  select(contains(\"DJG\"))\n\n\n\n  \n\n\n\nWe can also use the select() helpers with c_across() to make this even cleaner.\n\nraw_data |> \n  rowwise() |> \n  mutate(DJG_sum = sum(c_across(contains(\"DJG\")))) |> \n  select(contains(\"DJG\"))\n\n\n\n  \n\n\n\n\n\n3.6.2 Using Conditionals\nFinally, we can change only certain values within a column with conditionals.\nHere we create a new column z.\n\nIf the value in y is equal to 10, that row in z gets a 1.\nFor all other values of y (default, TRUE for everything else), give z a 0.\n\n\n\n\nCreating a new column based on conditional operations on an existing column\n\n\nNotice that in our data set we have different impairments (e.g. Vision, Speech) coded as X or NA. We can’t work with these for statistics because they aren’t numbers. But we can fix that using some conditional rules using the case_when() function.\ncase_when() looks for a logical operation, (e.g. when Vision == \"X\"), when this is TRUE you can set a value to assign to that case after the ~. We will set this to 1. We can then either specify every other case, or set a default value. In this instance we’ll set a default value where the case evaluation always results in TRUE after checking for the first case (i.e. that Vision == \"X\"). For all cases other than Vision == \"X\" we set the value to 0.\nAgain, we use select() only to restrict the print output for ease of viewing.\n\nraw_data |> \n  select(PID, Vision) |> \n  mutate(\n    Vision = case_when(\n      Vision == \"X\" ~ 1,\n      TRUE ~ 0\n    )\n  )"
  },
  {
    "objectID": "03_transforming-data.html#filtering-observations",
    "href": "03_transforming-data.html#filtering-observations",
    "title": "3  Transforming Data",
    "section": "3.7 Filtering Observations",
    "text": "3.7 Filtering Observations\n\n3.7.1 Filtering to Keep\nWe can subset our data sets by filtering it out to only contain rows where specific conditions are met using the filter() function. In the example below we filter the data set down to rows where x is greater than 1.\n\n\n\nFiltering a data set to specific rows\n\n\nIn our data set, we might subset it to only those who completed the study. Due to inconsistent coding, we have two values for having completed the study in the progress column, FINISH and END. We want to filter the data to people who have progress as FINISH OR END (they can’t be both), so we use the | OR operator.\n\nraw_data |> \n  filter(progress == \"FINISH\" | progress == \"END\")\n\n\n\n  \n\n\n\nIf we have many conditions we want to meet, many OR (|) statements are verbose and can be unwieldy. We can instead use %in% to define the values in progess that we want to keep. This is TRUE if an observation is in a value that you provide.\n\nraw_data |> \n  filter(progress %in% c(\"FINISH\", \"END\"))\n\n\n\n  \n\n\n\nWe can combine these with criteria on other columns to do additional sub-setting in one filter() call.\n\nraw_data |> \n  filter(\n    progress %in% c(\"FINISH\", \"END\"),\n    Age > 50\n  )\n\n\n\n  \n\n\n\n\n\n3.7.2 Filtering to Exclude\nWhat if we have conditions we’d like to exclude from our data set rather than keep? We can do the inverse of these operations using !. Here we ask for those whose progress ISN’T FINISH or END but who are over 50.\n\nraw_data |> \n  filter(\n    !progress %in% c(\"FINISH\", \"END\"),\n    Age > 50\n  )\n\n\n\n  \n\n\n\n\n\n3.7.3 A Note on Missing Values\nOne thing to bear in mind If you ask R if a value is equal to an NA (an unknown value) it is very literal in that it tells us it can’t know. So, this doesn’t work:\n\nraw_data |> filter(progress == NA)\n\nInstead, we have to use is.na():\n\nraw_data |> filter(is.na(progress))\n\n\n\n  \n\n\n\nIf we want values that AREN’T NAs, then we can combine is.na() and the NOT (!) operator.\n\nraw_data |> filter(!is.na(progress))"
  },
  {
    "objectID": "03_transforming-data.html#arranging-data",
    "href": "03_transforming-data.html#arranging-data",
    "title": "3  Transforming Data",
    "section": "3.8 Arranging Data",
    "text": "3.8 Arranging Data\nWhile arguable one of the less important aspects of data transformation, you will sometimes want or need to arrange your data in a specific order. To do this, we can arrange rows sorted by value in a column using arrange().\n\n\n\nArranging data by sorting all rows by values in a specific column\n\n\nThis defaults to an ascending order, but we can use desc() to enforce a descending order. Compare the default and desc() versions below.\n\n\n\nraw_data |> \n  select(Age) |> \n  arrange(Age)\n\n\n\n  \n\n\n\n\n\nraw_data |> \n  select(Age) |> \n  arrange(desc(Age))"
  },
  {
    "objectID": "03_transforming-data.html#summarising-data",
    "href": "03_transforming-data.html#summarising-data",
    "title": "3  Transforming Data",
    "section": "3.9 Summarising Data",
    "text": "3.9 Summarising Data\nOne very important aspect of data analysis is presenting descriptive statistics, or summaries of your data. Often this will be counts, measures of central tendency (such as the mean, median, or mode) and measures of dispersion (such as standard deviation, interquartile range, etc.). To produce these summaries we can use the summarise() function.\nHere, we create a new table with a new column which is the result of a mathematical operation on every value in the original table of data.\n\n\n\nSummarising values in a data set\n\n\nLet’s look at getting an idea of some very basic descriptive statistics for the demographics in our data set. We’ll create a new table which contains the mean of ages in our data set.\n\nraw_data |> \n  summarise(age_mean = mean(Age))\n\n\n\n  \n\n\n\nHowever, by default this results in an error. That’s because mean() can’t compute a mean of numeric values and something that doesn’t exist NA. Instead, we can tell mean() to explicitly ignore NAs in the computation. We can do this as follows.\n\n3.9.1 Handling Missing Values\n\n\nEither within each operation.\n\nraw_data |> \n  summarise(\n    age_mean = \n      mean(Age, na.rm = TRUE)\n  )\n\n\n\n  \n\n\n\n\nOr within the chain of functions.\n\nraw_data |> \n  drop_na(Age) |> \n  summarise(\n    age_mean = mean(Age)\n  )\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote drop_na() only keeps complete rows if several columns are provided.\n\n\nWe can create many columns within our new summary table made up of operations on our original data. Here, we will create a mean and standard deviation for age along with a count of the number of observations we have in the data set (excluding NAs) using n().\n\nraw_data |> \n  drop_na(Age) |> \n  summarise(\n    age_mean = mean(Age),\n    age_sd = sd(Age),\n    n = n()\n  )"
  }
]