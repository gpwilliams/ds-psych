[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Psychologists",
    "section": "",
    "text": "This book is an introduction to Data Science for Psychologists (and others in the social sciences). Primarily using the R programming language, you will learn a general workflow that helps you to process, plot, analyse, and present data to your audience. It is expected that readers are familiar with introductory statistics typically taught at the Undergraduate level in Psychology programmes across the UK.\nThroughout, concepts will be taught using examples from real and simulated data from studies in Psychology. R will be taught using a tidyverse-first approach, using a suite of packages that are designed to make programming quick, easy, and highly readable."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.” Journal of Open Source\nSoftware 4 (43): 1686."
  },
  {
    "objectID": "01_getting-started.html",
    "href": "01_getting-started.html",
    "title": "1  Getting Started",
    "section": "",
    "text": "2 References"
  },
  {
    "objectID": "10_understanding-null-results.html",
    "href": "10_understanding-null-results.html",
    "title": "10  Understanding Null Results",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "02_creating-graphs.html",
    "href": "02_creating-graphs.html",
    "title": "2  Creating Graphs",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "03_transforming-data.html",
    "href": "03_transforming-data.html",
    "title": "3  Transforming Data",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "04_tidying-and-merging-data.html",
    "href": "04_tidying-and-merging-data.html",
    "title": "4  Tidying and Merging Data",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "05_core-statistics.html",
    "href": "05_core-statistics.html",
    "title": "5  Core Statistics",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "06_open-science-practices.html",
    "href": "06_open-science-practices.html",
    "title": "6  Open Science Practices",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "07_multilevel-modelling.html",
    "href": "07_multilevel-modelling.html",
    "title": "7  Multilevel Modelling",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "08_data-simulation-and-power.html",
    "href": "08_data-simulation-and-power.html",
    "title": "8  Data Simulation and Power",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "09_bayesian-estimation-and-model-comparison.html",
    "href": "09_bayesian-estimation-and-model-comparison.html",
    "title": "9  Bayesian Estimation and Model Comparison",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "99_installing-r.html",
    "href": "99_installing-r.html",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "",
    "text": "B Installing R\nTo install R, go to the Comprehensive R Archive Network (CRAN). In the heading Download and Install R click the download link for your operating system.\nSelect the correct distribution for your operating system."
  },
  {
    "objectID": "99_installing-r.html#downloading-r-for-mac",
    "href": "99_installing-r.html#downloading-r-for-mac",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "B.1 Downloading R for Mac",
    "text": "B.1 Downloading R for Mac\nFor Mac users, click Download R for macOS. Always download the latest release of R. At the time of writing this is R version 4.2.2. Click the link to the left of the release notes to download R to your system.\n\n\n\nDownloading R for Mac"
  },
  {
    "objectID": "99_installing-r.html#downloading-r-for-windows",
    "href": "99_installing-r.html#downloading-r-for-windows",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "B.2 Downloading R for Windows",
    "text": "B.2 Downloading R for Windows\nFor Windows users, click Install R for the first time.\n\n\n\nDownloading R for Windows\n\n\nThis will take you to another web page. At the top is your download link in the format Download R-[version number] for windows. At the time of writing this is Download R-4.2.2 for Windows.\nIf you have a 64 bit system, install the 64 bit version of R as you'll be able to take advantage of having more than 4Gb RAM. (This is useful in instances where you're working with very large data sets.)"
  },
  {
    "objectID": "99_installing-r.html#downloading-r-for-linux",
    "href": "99_installing-r.html#downloading-r-for-linux",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "B.3 Downloading R for Linux",
    "text": "B.3 Downloading R for Linux\nFor Linux users, detailed instructions are given for different distributions. Follow the relevant links and details there to download and install R."
  },
  {
    "objectID": "99_installing-r.html#installing-r",
    "href": "99_installing-r.html#installing-r",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "A.1 Installing R",
    "text": "A.1 Installing R\nTo install R, go to the Comprehensive R Archive Network (CRAN). In the heading Download and Install R click the download link for your operating system.\nSelect the correct distribution for your operating system.\n\n\n\nDownloading R from CRAN\n\n\n\nA.1.1 Downloading R for Mac\nFor Mac users, click Download R for macOS. Always download the latest release of R. At the time of writing this is R version 4.2.2. Click the link to the left of the release notes to download R to your system.\n\n\n\nDownloading R for Mac\n\n\n\n\nA.1.2 Downloading R for Windows\nFor Windows users, click Install R for the first time.\n\n\n\nDownloading R for Windows\n\n\nThis will take you to another web page. At the top is your download link in the format Download R-[version number] for windows. At the time of writing this is Download R-4.2.2 for Windows.\nIf you have a 64 bit system, install the 64 bit version of R as you'll be able to take advantage of having more than 4Gb RAM. (This is useful in instances where you're working with very large data sets.)\n\n\nA.1.3 Downloading R for Linux\nFor Linux users, detailed instructions are given for different distributions. Follow the relevant links and details there to download and install R."
  },
  {
    "objectID": "99_installing-r.html#installing-rstudio",
    "href": "99_installing-r.html#installing-rstudio",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "A.2 Installing RStudio",
    "text": "A.2 Installing RStudio"
  },
  {
    "objectID": "99_installing-r.html#installing-quarto",
    "href": "99_installing-r.html#installing-quarto",
    "title": "Appendix A — Installing R and Dependencies",
    "section": "A.3 Installing Quarto",
    "text": "A.3 Installing Quarto"
  },
  {
    "objectID": "99a_installing-r.html#downloading-r-for-mac",
    "href": "99a_installing-r.html#downloading-r-for-mac",
    "title": "Appendix A — Installing R",
    "section": "A.1 Downloading R for Mac",
    "text": "A.1 Downloading R for Mac\nFor Mac users, click Download R for macOS. Always download the latest release of R. At the time of writing this is R version 4.2.2. Click the link to the left of the release notes to download R to your system.\n\n\n\nDownloading R for Mac\n\n\nIf you have a Mac with an M1 chip, ensure that you download and install the arm64 build. Otherwise, for Macs with an Intel chip, download and install the Intel 64-bit build."
  },
  {
    "objectID": "99a_installing-r.html#downloading-r-for-windows",
    "href": "99a_installing-r.html#downloading-r-for-windows",
    "title": "Appendix A — Installing R",
    "section": "A.2 Downloading R for Windows",
    "text": "A.2 Downloading R for Windows\nFor Windows users, click Install R for the first time.\n\n\n\nDownloading R for Windows\n\n\nThis will take you to another web page. At the top is your download link in the format Download R-[version number] for windows. At the time of writing this is Download R-4.2.2 for Windows.\nIf you have a 64 bit system, install the 64 bit version of R as you’ll be able to take advantage of having more than 4Gb RAM. (This is useful in instances where you’re working with very large data sets.)"
  },
  {
    "objectID": "99a_installing-r.html#downloading-r-for-linux",
    "href": "99a_installing-r.html#downloading-r-for-linux",
    "title": "Appendix A — Installing R",
    "section": "A.3 Downloading R for Linux",
    "text": "A.3 Downloading R for Linux\nFor Linux users, detailed instructions are given for different distributions. Follow the relevant links and details there to download and install R."
  },
  {
    "objectID": "99a_installing-r.html#installing-rstudio",
    "href": "99a_installing-r.html#installing-rstudio",
    "title": "Appendix A — Installing R",
    "section": "A.4 Installing RStudio",
    "text": "A.4 Installing RStudio"
  },
  {
    "objectID": "99a_installing-r.html#installing-quarto",
    "href": "99a_installing-r.html#installing-quarto",
    "title": "Appendix A — Installing R",
    "section": "A.5 Installing Quarto",
    "text": "A.5 Installing Quarto"
  },
  {
    "objectID": "99d_installing-bayesian-software.html#mac",
    "href": "99d_installing-bayesian-software.html#mac",
    "title": "Appendix D — Installing Bayesian Software",
    "section": "D.1 Mac",
    "text": "D.1 Mac\n\nD.1.1 Configure the C++ Toolchain\nIf you have previously used R on your Mac, you may have the files ~/.R/Makevars and/or ~/.Renviron. If you have any important settings in here that you’ve defined personally, make a backup of these files in another location. Then, delete the originals. If you can’t find these files on your system, go to Finder, click on Home (i.e. the icon with the house), and type Shift + CMD + . to show the hidden files on your system. After completing the installation instructions you can go back to the new Makevars file in this location and add back in any settings you previously had.\nThere are two steps to configuring the C++ toolchain on Mac:\n\nInstalling the Xcode Command Line Tools\nInstalling gfortran.\n\nFirst, install the Xcode Command Line Tools by opening the terminal (use the spotlight search and type Terminal). In the terminal type the following and then press Enter:\nxcode-select --install.\nInstalling the Xcode Command Line Tools may take a while. Once done install gfortran. The version you install will differ whether you have a Mac with Apple or Intel chips.\nInstall the latest version of gfortran for your operating system at https://github.com/fxcoudert/gfortran-for-macOS/releases.\nCheck your macOS by clicking the Apple logo in the dock and selecting About this Mac. On the releases page, select the appropriate release for your operating system. Please install the latest version of gfortran for your system. Specific instructions for Apple Silicon and Intel Silicon Macs are provided below.\n\nD.1.1.1 Macs with Apple Silicon (i.e. M1 or M2 chips)\nAt the time of writing for up to date Macs with Apple Silicon this is gfortran 12.2 for Ventura (macOS 13). Click gfortran-ARM-12.2-Ventura.dmg to download the software and install this on your system.\n\n\nD.1.1.2 Macs with Intel Silicon\nAt the time of writing for up to date Macs with Apple Silicon this is gfortran 12.1 for Monterey (macOS 12). Click gfortran-ARM-12.1-Monterey.dmg to download the software and install this on your system.\n\n\n\nD.1.2 Installing RStan\nIn case you previously tried to install RStan and it didn’t work, copy and paste the following code into the RStudio console and press Enter. This will clean up R to remove any failed installations.\n\nremove.packages(\"rstan\")\nif (file.exists(\".RData\")) file.remove(\".RData\")\n\nRestart R by clicking Session, Restart R.\nCopy and paste the following code into the console and press Enter.\n\ninstall.packages(\"rstan\")\n\nFinally, we’ll check everything works by again copying and pasting the following code into the console and pressing Enter.\n\nexample(stan_model, package = \"rstan\", run.dontrun = TRUE)\n\nThis might take some time to run, but if you see a big wall of text and it ends by sampling a model then your install works.\n\n\n\nA working installation of RStan\n\n\nFinally, install brms by typing the following into the RStudio console:\n\ninstall.packages(\"brms\")\n\nYou’re now ready to work with Bayesian models in R!"
  },
  {
    "objectID": "99d_installing-bayesian-software.html#windows",
    "href": "99d_installing-bayesian-software.html#windows",
    "title": "Appendix D — Installing Bayesian Software",
    "section": "D.2 Windows",
    "text": "D.2 Windows\n\nD.2.1 Configure the C++ Toolchain\nFollow the relevant instructions for your R version here: https://github.com/stan-dev/rstan/wiki/Configuring-C—Toolchain-for-Windows. It is strongly advised that you use the latest version of R for this. At the time of writing this is R version 4.2. This necessitates installing RTools42. To do this, follow this link: https://cran.r-project.org/bin/windows/Rtools/rtools42/rtools.html and download the Rtools42 installer. Click the executable and follow the instructions to install RTools.\nAs the current version of RStan on CRAN isn’t compatible with R 4.2, install the preview versions of StanHeaders and rstan by copying and pasting the following code into your RStudio console and pressing Enter:\n\ninstall.packages(\"StanHeaders\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\ninstall.packages(\"rstan\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\n\nEnsure that your installation works by running the following in the RStudio console:\n\nexample(stan_model, package = \"rstan\", run.dontrun = TRUE)\n\nIf this runs and samples (see the example above in the Mac instructions) then your RStan installation works.\nFinally, install brms by typing the following into the RStudio console:\n\ninstall.packages(\"brms\")\n\nYou’re now ready to work with Bayesian models in R!"
  },
  {
    "objectID": "99d_installing-bayesian-software.html#linux",
    "href": "99d_installing-bayesian-software.html#linux",
    "title": "Appendix D — Installing Bayesian Software",
    "section": "D.3 Linux",
    "text": "D.3 Linux\n\nD.3.1 Configure the C++ Toolchain\nFollow the instructions at https://github.com/stan-dev/rstan/wiki/Configuring-C-Toolchain-for-Linux to install a pre-built RStan binary.\nFinally, install brms by typing the following into the RStudio console:\n\ninstall.packages(\"brms\")\n\nYou’re now ready to work with Bayesian models in R!"
  },
  {
    "objectID": "99b_installing-rstudio.html",
    "href": "99b_installing-rstudio.html",
    "title": "Appendix B — Installing RStudio",
    "section": "",
    "text": "A popular way to work with R is to use an Integrated Development Environment (IDE). This IDE makes it easier to work with R when integrating version control, producing codebooks, and creating project files.\nThe most popular IDE for R is RStudio by Posit, which you can download from the Posit website. We will use this IDE in this course.\nClick Download RStudio in the top right of the screen. On the new page ensure you are on the tab for RStudio Desktop and click the Download RStudio button.\n\n\n\nDownloading RStudio\n\n\nYou should have already downloaded and installed R at this point. If you haven’t, follow the instructions on the Posit website or return to Appendix A. Next, download and install RStudio using the download link on the Posit website. Open the executable and follow the instructions to install R on your system."
  },
  {
    "objectID": "99c_installing-quarto.html",
    "href": "99c_installing-quarto.html",
    "title": "Appendix C — Installing Quarto",
    "section": "",
    "text": "Quarto is a new open-source program for publishing technical documents. This allows you to create articles, presentations, websites, books, and various other outputs. Crucially, Quarto allows you to dynamically create content within these outputs using R, Python, Julia, or Observable. (In fact, this book was written in Quarto!)\nWe will use Quarto to create documents and to complete exercises for this course. On the Quarto web page, click Get Started.\n\n\n\nA summary of Quarto’s capabilities\n\n\nTo download Quarto, click Get Started. This will take you to the download page. Click the button labelled Download Quarto CLI. Next, run the executable and install this on your system. You should now be able to create and view Quarto documents in RStudio."
  },
  {
    "objectID": "99e_installing-r-packages.html",
    "href": "99e_installing-r-packages.html",
    "title": "Appendix E — Installing R Packages",
    "section": "",
    "text": "For this course, we will use various packages which can be downloaded directly from CRAN or via GitHub within R. Packages are just functions that authors have created and shared with other R users to make certain processes easier. These can range from rather hard core data processing, plotting, and analysis functions to a set of beautiful palettes to make your plots more attractive.\nPlease copy and paste the following code into your RStudio Console to install these packages in R. In each session we will load several of these packages up to make working with our data easier.\n\ninstall.packages(\"tidyverse\") # various data wrangling and plotting packages bundled\ninstall.packages(\"here\") # working with file paths\ninstall.packages(\"easystats\") # various stats packages bundled\ninstall.packages(\"emmeans\") # calculating marginal means and pairwise tests\ninstall.packages(\"lme4\") # mixed effects modelling\ninstall.packages(\"afex\") # ANOVA and improvements on lme4\ninstall.packages(\"brms\") # bayesian regression models using Stan\ninstall.packages(\"tidybayes\") # working with draws from bayesian models\ninstall.packages(\"ggdist\") # visualising distributions and uncertainty"
  },
  {
    "objectID": "01_getting-started.html#understanding-rstudio",
    "href": "01_getting-started.html#understanding-rstudio",
    "title": "1  Getting Started",
    "section": "1.1 Understanding RStudio",
    "text": "1.1 Understanding RStudio\nNow that you have R installed, you could jump straight into opening the R Graphical User Interface (GUI). But, as you’ll see in Figure 1.1, Funny-Looking Kid isn’t just the name of this R distribution, but perfectly captures the look of this interface.\n\n\n\nFigure 1.1: The R GUI\n\n\nYou can still get a lot done in the R GUI, but there’s a lot of quality of life improvements we can get from RStudio, including access to syntax highlighting, code completion, a graphical git interface, RStudio Projects, templates for Quarto documents, and a rich markdown editor. None of these things should mean anything to you at this point, but you’ll see how they can be very helpful later. For this reason, we’ll start with RStudio from the beginning.\nThe RStudio pane can be broken down into a few different sections, as shown in Figure 1.2.\n\n\n\nFigure 1.2: The RStudio GUI\n\n\n\nThe editor: Type, edit, and save your R code in R files or Quarto documents.\nThe console: Execute your R code here by typing or copying your code here and pressing Enter. You can also highlight sections of code in the editor and press Shift + Enter to execute that code in the console.\nThe environment and history: View objects stored in memory for this working session (e.g. values, objects, and user-defined functions etc.). You can also see a history of your commands in the History tab and use the Git interface to save records of your code using the Git version control system.\nThe viewer: view any files in your working directory, see your last plot from this session, view installed packages on your machine (you can also load them here), view the help documentation for any R commands, and view plots and documents you’ve created in the viewer.\n\nYou should always aim to write your code in the editor because you can save it, edit it, and reuse it later. Only write your code in the console if you’re happy for it to be lost as soon as you type it out.\nBefore we get started writing code in RStudio, please take the time to change some of the defaults in RStudio.\nRStudio defaults to saving your workspace to an .RData file when you exit RStudio and to restoring your workspace once you reopen this. This means anything you’ve created will be restored when you start it back up. This is a bad idea because it impedes reproducibility: Imagine you mess around in the console and create or edit an object that changes the results of your analyses. This change will still be there when you restart RStudio, but you’d have no record of it. Instead, it’s better to ensure your code works from scratch in case you move it to a new computer or share it with others.\nRemove these defaults by going to Tools, Global Options and deselecting Restore .RData into workspace at startup and from the drop-down menu on Save workspace to .RData on exit to Never."
  },
  {
    "objectID": "01_getting-started.html#packages",
    "href": "01_getting-started.html#packages",
    "title": "1  Getting Started",
    "section": "1.4 Packages",
    "text": "1.4 Packages\nWhile you can get a lot done in R out of the box, many developers have created packages that add additional functionality to R (e.g. new analytical techniques) or make working with R more convenient.\nOne of the most successful packages is the tidyverse Wickham et al. (2019) suite of packages which bundles together several packages containing functions that make working with your data much easier and make your code more readable. We will primarily focus on using functions from the tidyverse in this course.\nTo install a package on your computer, simply type install.packages() in your console with the package name in quotes within this function call. Press Enter, and your package will be installed directly in R. You only need to do this once per computer.\nTry installing the tidyverse on your computer now.\n\ninstall.packages(\"tidyverse\")\n\nEvery time you open RStudio, be sure to load up the packages you need for your code. Do this now with the tidyverse. Here, you’re asking R to load the package library.\n\nlibrary(\"tidyverse\")"
  },
  {
    "objectID": "01_getting-started.html#file-systems-and-projects",
    "href": "01_getting-started.html#file-systems-and-projects",
    "title": "1  Getting Started",
    "section": "1.4 File Systems and Projects",
    "text": "1.4 File Systems and Projects"
  },
  {
    "objectID": "01_getting-started.html#objects-and-functions",
    "href": "01_getting-started.html#objects-and-functions",
    "title": "1  Getting Started",
    "section": "1.5 Objects and Functions",
    "text": "1.5 Objects and Functions\nYou can work directly with data in R, for example using it like a calculator.\n\n4 + 2\n\n[1] 6\n\n\nR will be patient and wait for you to finish an expression before executing code. So, if your line of text ends with a mathematical operator, R waits to receive the next number. This can be useful for splitting long expressions across multiple lines:\n\n1 + 2 + 3 + 4 + 5 + 6 + 7 +\n  8 + 9\n\n[1] 45\n\n\nTry typing this out in the console. You’ll notice the > that is normally there when you type changed to a +, indicating that R is waiting for more code.\nR also parses text if included in quotes.\n\n\"Hello World!\"\n\n[1] \"Hello World!\"\n\n\nThe same rule applies about finishing expressions here; if you don’t close your quote, then R will wait for you to do so. This means you can spread your text over several lines (by pressing Enter) and R will parse that as one expression. Note with our output we get \\n which indicates that a new line follows the comma.\n\n\"Hello world, isn't this book taking longer to write than Glenn expected\ndespite Glenn having alredy done this before and \nswearing he learned to not ovedo it?\"\n\n[1] \"Hello world, isn't this book taking longer to write than Glenn expected\\ndespite Glenn having alredy done this before and \\nswearing he learned to not ovedo it?\"\n\n\nBut, repeatedly typing data out or referring back to this data is going to be very tedious if we can’t use a shorthand to refer to it. This is where objects come in. Objects are used to store information in R. Crucially, we can perform operations on them by simply using the name of the object.\nWe assign values to a object using the assignment operator <-. Using this, we give the object its values.\n\nsummed_numbers <- 4 + 2\n\nBy default, R will not return the result of this operation from summed_numbers unless you ask it to do so. To get the result, simply type the name of the object.\n\nsummed_numbers\n\n[1] 6\n\n\nWe can perform operations on these objects after they’ve been created.\n\nsummed_numbers * 5\n\n[1] 30\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can’t start objects with a number, you can’t use special characters (e.g. %!*), and you can’t include spaces in your object name.\nAlso, capitalisation matters, so Summed_numbers is not summed_numbers.\n\n\nR has many mathematical operations built in.\n\nMathematical Operations in R\n\n\n\n\n\n\nOperation\nCode Example\n\n\n\n\nAdd\nx + y\n\n\nSubtract\nx - y\n\n\nMultiply\nx * y\n\n\nDivide\nx / y\n\n\nExponentiate\nx ^ y\n\n\nModulus\nx %% y (e.g. 5 mod 2 = 1, the remainder of how many times 2 goes into 5.)\n\n\nInteger Division\nx %/% y (e.g. 5 int div 2 = 2)\n\n\nMatrix Multiplication\n%*%\n\n\n\nIt also has many logical operations built in.\n\nLogical Operations in R\n\n\nOperation\nCode Example\n\n\n\n\nLess than\nx < y\n\n\nLess than or equal to\nx <= y\n\n\nGreater than\nx > y\n\n\nGreater than or equal to\nx >= y\n\n\nExactly equal to\n==\n\n\nNot equal to\nx != y\n\n\nNot x\n!x\n\n\nx OR y\nx | y\n\n\nx AND y\nx & y\n\n\nIs x TRUE?\nisTRUE(x)\n\n\n\nThese come in pretty handy for performing most operations on our data. If you’re unfamiliar with these, don’t worry. We’ll cover how you might use some of these in a staggered format as you progress through this course. Nicely, R also has a number of functions built in.\nFunctions in R always end in parentheses, indicating that they take an argument. For example, one of the most basic and important functions in R is c() for concatenate. This allows you to combine many values into a vector or list of values.\nWhen we combine values into an object, this object is stored in our global environment. This means that we can perform operations on the object later on, without the worry of typing our the values again. This is particularly useful if you want to store values from one function (say a statistical test) that you cannot pre-define but that you want to use later on.\nLet’s see how this works.\n\nmy_values <- c(1, 10, 4, 5)\nmy_values\n\n[1]  1 10  4  5\n\n\nWe now have a vector of values stored in one object.\nR has other convenient functions built in. For example, we can sum this vector, or get its mean.\n\nsum(my_values)\n\n[1] 20\n\nmean(my_values)\n\n[1] 5\n\n\nNotice how you also don’t have to output things one at a time. R remembers the order of operations.\nFunctions can have default values or not (requiring you to specify the argument). Above, we passed the the values in my_values to each function as an argument. Later, we’ll look at functions that ask for arguments from separate data types (e.g. numbers and characters) or even multiple arguments.\nIf you’re unsure what an argument does, you can always ask R what it does, how it does it, and what to pass to it by using ?, e.g. ?mean(). This will bring up a document in the Help window of RStudio.\nUsing objects allows our code to be flexible, as we can write a script that performs operations on objects that can take any range of values. This, to me, is one of the nicest things about doing your analyses in R. While you may spend more time getting your script up and running in the first place when compared to using point-and-click methods (e.g. in SPSS), if you gain new data or run a new experiment, it’s likely that your script can simply be re-run with no (or few) changes at very little cost to your time.\nNow, this part is pretty important but may only be obvious if you’ve programmed in other languages. R is a vectorised language, which means that, as with the sum() function above, R can perform operations on the entire object. So, if you want to increment all values in your object by 1, you can simply tell R to do so in one line of code, without the need for loops or other complex methods.\n\nmy_values + 1\n\n[1]  2 11  5  6\n\n\n\n1.5.1 Namespace Conflicts\nMost of the time, you won’t have any trouble using functions from a loaded package. However, there can be cases when you have two packages installed that use the same function name. To tell R exactly which version of a function to use, we can specify both the package and function name in the form package::function_name(). For example, we can use the group_by() function from the package dplyr by typing dplyr::group_by(). You won’t come across this in this course, as we’ll be using packages that have functions with unique names, but it’s worth bearing in mind if you come across problems with functions you know should work in the future.\n\n\n1.5.2 Data Types\nWhile we’ve seen that we can create objects containing integers (whole numbers), we can also create objects of other data types. There are 4 main data types that you’ll come across regularly in R:\n\nCharacters: Strings of text, e.g. \"My cats, Bear and Penny\"\nNumeric: Numbers stored as floats (decimals), e.g. 1, 1.5, 1.576.\nInteger: Numbers stored explicitly as whole numbers using L notation, e.g. 1L, 2L\nLogical: Boolean operators, i.e. TRUE and FALSE. (Avoid T and F as these can be overwritten.)\n\nOnly values of the same data type can be stored together in an object. If you try to concatenate values of different data types you get type coercion. Let’s see the difference in output when concatenating two numbers versus a number and a character.\n\nc(2, 2)\n\n[1] 2 2\n\nc(2, \"Cat\")\n\n[1] \"2\"   \"Cat\"\n\n\nNotice that in the first example the two values are unquoted. In the second, the two values are quoted. That’s because since you can only store values of a single data type within an object, when you try to combine a number with a character, R converts all values to character.\nSimilarly, certain operations only work on specific data types. For example, if you try to perform mathematical operations on invalid data types (e.g. trying to add two characters), R will give you an error.\n\n2 + \"Cat\"\n2 + \"2\"\n\nIf you try to run this code, you get a similar error to this: Error in 2 + \"2\" : non-numeric argument to binary operator. This basically says you can’t add a character to a number.\n\n\n1.5.3 Data Structures\n\n1.5.3.1 Vectors\nWe’ve seen already how we can combine values of different data types into one object. In R, these objects are called vectors. We’ve seen already how vectors can be useful in that operations can be applied to every element in the vector using simple mathematical operations. For example, when we want to add 1 to every element of the vector my_numbers, we just type my_numbers + 1. In other, non-vectorised languages, we need to have a way to apply this addition to every element of the vector. This brings us nicely to the idea of indexing values in a vector. How do we get an value back from a vector at a specific location?\nHere, we’ll create a vector with the values 3 through 7, and extract the third value from the vector. We can use the : operator to get values between 3 and 7 without explicitly writing them out. We can then extract a value at a specific place in our vector using []. Since R is a 1 indexed language, when we want the third value from a vector, we make this [3]\n\nmy_numbers <- 3:7 # values are: 3, 4, 5, 6, 7\nmy_numbers[3]\n\n[1] 5\n\n\nR returns the value in the third position, 5.\nWhat if we want to change the value in position 2 to 189? We use indexing to access this value, [2], and assignment to make the value at that position 189, <- 189.\n\nmy_numbers[2] <- 189\nmy_numbers\n\n[1]   3 189   5   6   7\n\n\nFinally, we can create vectors from a range of complex in-built (and additional) functions. Let’s look at creating scores from a sequence of numbers, sampling from a set of numbers, and even drawing scores from a normal distribution. These functions all take on multiple named arguments. Remember, you can find out about these functions by using ?, e.g. ?seq().\nFor this, imagine we want to perform a quick simulation of what IQ looks like for cat and dog owners (assuming people only have one or the other).\nLet’s first create some participant IDs ranging from 1 to 100.\n\nparticipant_id <- seq(from = 1, to = 100, by = 1)\nparticipant_id\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\n\nBefore we get with sampling further data, we’ll set the random seed in R. Since computers are deterministic, nothing is ever actually random with them. Instead, processes that we imagine are random (avoiding heavy philosophical issues here) are created by pseudorandom number generators that take a seed. This is a number used to get the pseudorandom number generator started. We often use these in our code to ensure computational reproducibility. Though the process itself is hopefully random, you might still want to recreate my exact random numbers. Let’s do this.\n\nset.seed(1892)\n\nThen lets make some pet ownership codes and sample from this 100 times with replacement.\n\npets <- c(\"cat\", \"dog\")\npet_owned <- sample(pets, size = 100, replace = TRUE)\npet_owned\n\n  [1] \"dog\" \"cat\" \"dog\" \"cat\" \"dog\" \"cat\" \"cat\" \"dog\" \"cat\" \"dog\" \"dog\" \"cat\"\n [13] \"cat\" \"dog\" \"dog\" \"cat\" \"dog\" \"dog\" \"cat\" \"dog\" \"dog\" \"cat\" \"cat\" \"cat\"\n [25] \"cat\" \"dog\" \"cat\" \"cat\" \"cat\" \"cat\" \"cat\" \"cat\" \"dog\" \"dog\" \"cat\" \"dog\"\n [37] \"cat\" \"cat\" \"cat\" \"dog\" \"dog\" \"dog\" \"cat\" \"dog\" \"cat\" \"cat\" \"dog\" \"dog\"\n [49] \"cat\" \"dog\" \"cat\" \"dog\" \"cat\" \"dog\" \"dog\" \"dog\" \"dog\" \"cat\" \"cat\" \"cat\"\n [61] \"cat\" \"dog\" \"dog\" \"dog\" \"dog\" \"cat\" \"dog\" \"cat\" \"dog\" \"cat\" \"dog\" \"dog\"\n [73] \"dog\" \"cat\" \"dog\" \"dog\" \"dog\" \"cat\" \"cat\" \"dog\" \"cat\" \"dog\" \"dog\" \"cat\"\n [85] \"dog\" \"dog\" \"cat\" \"cat\" \"dog\" \"cat\" \"cat\" \"dog\" \"cat\" \"dog\" \"cat\" \"dog\"\n [97] \"cat\" \"cat\" \"cat\" \"dog\"\n\n\nFinally, let’s create the IQ scores, sampling from a normal distribution with mean 150 and with a standard deviation of 15.\n\niq_score <- rnorm(n = 100, mean = 100, sd = 15)\niq_score\n\n  [1] 119.90419 100.31550 106.28567 100.32896  99.40381 105.60851  69.25938\n  [8]  88.32001 101.35022 108.49565  79.78903 106.13353 102.67474  97.29907\n [15]  88.10597 109.67761 109.34948 102.41409 115.91469 126.64919  80.55541\n [22]  93.51404 103.97598  75.78122  97.44319 107.84348  91.55643  81.11721\n [29] 108.46863 113.81963  96.25558  94.15728 122.85215  86.35401 109.98973\n [36] 104.70575  95.83989  62.79169  90.88445 100.07760  82.95541  95.33197\n [43] 114.02527 112.32369  81.45885 111.64807  87.57530  94.64206  99.15243\n [50]  61.11503 106.62990  79.72260 100.15081  83.41594 114.31351  99.14609\n [57]  84.50973  81.42513 114.89340 124.02161  89.06398  93.27161  94.83063\n [64] 106.89741 111.33804  84.21109 126.75998 106.26074 120.37403  77.79786\n [71] 128.42155 112.19440  92.08103 104.15516 127.38645 105.28515  90.39646\n [78]  96.97260  99.75440 124.81151 107.16399 106.19192  70.66848  78.42079\n [85]  94.77819  92.42836  96.64234  91.71500 133.46106  84.11078  88.70980\n [92]  80.65210  94.50641  93.65194  87.23135 109.73530 128.65171  91.49707\n [99]  76.43452 108.62265\n\n\nNow, we could index these values, change them, or perform operations on them to our heart’s content. Let’s see some useful things we can do if we had data stored in this way.\nFirst, we might want to know how many participants are in our sample. Assume we don’t already know it’s 100, we could do this by asking for the length of the participant ID vector.\n\nlength(participant_id)\n\n[1] 100\n\n\nWe have 100 IDs! But what if someone took the study again? How would we find out the number of unique people in the sample? First, assign participant 100 again to the object, in the 101st place. Then we’ll get the length of this to see that we have 101 values. If we want the unique values, we’ll take the length of the unique values.\n\nparticipant_id[101] <- 100\nlength(participant_id)\n\n[1] 101\n\nlength(unique(participant_id))\n\n[1] 100\n\n\nNotice that by chaining together two functions we were able to get the length of the unique participants in the object. This function chaining is an important concept in any programming work.\n\n\n1.5.3.2 Lists\nLists are a way to store multiple vectors of different data types together in one object. Think of it as nesting vectors within vectors (very meta).\nYou can make these from existing objects or from scratch by defining values within the list. Lists can either have named elements, where we explicitly state the name of each object to be stored in the list, or they can be unnamed. Here, we’ll make a named list from our simulated data.\n\nsimulated_data <- list(\n  participant = participant_id,\n  pet = pet_owned,\n  iq = iq_score\n)\n\nAs before, we could print out each element of this list, but it’d produce a lot of output for the console. Instead, let’s check out a new list based on data we create within the list. Let’s look at the qualities of people named Glenn and not Glenn.\n\nperson_quality <- list(\n  glenn = c(\"handsome\", \"smart\", \"modest\"),\n  not_glenn = c(\"less_handsome\", \"less_smart\", \"less_modest\")\n)\nperson_quality\n\n$glenn\n[1] \"handsome\" \"smart\"    \"modest\"  \n\n$not_glenn\n[1] \"less_handsome\" \"less_smart\"    \"less_modest\"  \n\n\nIf we want just Glenn (which most people do, I’m sure) along with the name of the vector, use the same notation as before to access the element in the first location.\n\nperson_quality[1]\n\n$glenn\n[1] \"handsome\" \"smart\"    \"modest\"  \n\n\nOr we could access it by name:\n\nperson_quality[\"glenn\"]\n\n$glenn\n[1] \"handsome\" \"smart\"    \"modest\"  \n\n\nIf we just want the values in this element (which we often do), we need to use the double bracket notation, [[]].\n\nperson_quality[[\"glenn\"]]\n\n[1] \"handsome\" \"smart\"    \"modest\"  \n\n\nIn doing this we can edit values at specific locations or add elements to the vector just as we did before. However, this requires indexing the values of the correct element in the list, and then accessing the position of the correct value. This requires using a combination of double and single bracket notation.\n\nperson_quality[[\"glenn\"]][4] <- \"liar\"\nperson_quality[[\"glenn\"]]\n\n[1] \"handsome\" \"smart\"    \"modest\"   \"liar\"    \n\n\nAs you’ll notice in adding a fourth element to this entry in the list, the data needn’t be square. There are 4 elements in one of the entries of the list, but only 3 in the other.\n\nperson_quality\n\n$glenn\n[1] \"handsome\" \"smart\"    \"modest\"   \"liar\"    \n\n$not_glenn\n[1] \"less_handsome\" \"less_smart\"    \"less_modest\"  \n\n\nThis isn’t the case for the more commonly encountered data structure you’ll use in the course, data frames. We’ll often work with data frames because they’re easy to manage and follow a logical structure that’s analogous to working with a spreadsheet.\n\n\n1.5.3.3 Data Frames (and Tibbles)\nIn the real world, if you tested IQs you’d typically have this data stored in a table somewhere prior to reading it into R. So let’s pair the data together into a table in R. One way to do this is to create a data frame. However, if you use the tidyverse set of packages, which we do here, you have access to tibbles. These are just data frames with some sensible defaults like ensuring that R doesn’t convert vectors to different data structures when you subset your table.\nLet’s make a tibble from our IQ data from earlier. Notice that we have to subset the participant_id object to be the same length (100 items) as the other objects.\n\niq_data <- tibble(\n  participant = participant_id[1:100],\n  pet = pet_owned,\n  iq = iq_score\n)\niq_data\n\n# A tibble: 100 × 3\n   participant pet      iq\n         <dbl> <chr> <dbl>\n 1           1 dog   120. \n 2           2 cat   100. \n 3           3 dog   106. \n 4           4 cat   100. \n 5           5 dog    99.4\n 6           6 cat   106. \n 7           7 cat    69.3\n 8           8 dog    88.3\n 9           9 cat   101. \n10          10 dog   108. \n# … with 90 more rows\n\n\nTibbles are nice to use in that they show you the data type of each column, and by default print the first 10 rows of data only when you print the table. if you want more rows of data, you can ask for it explicitly. Here, n defines the number of rows, while width defines the number of columns. We can set this to Inf or infinity, to ensure all columns are printed.\n\nprint(iq_data, n = 12, width = Inf)\n\n# A tibble: 100 × 3\n   participant pet      iq\n         <dbl> <chr> <dbl>\n 1           1 dog   120. \n 2           2 cat   100. \n 3           3 dog   106. \n 4           4 cat   100. \n 5           5 dog    99.4\n 6           6 cat   106. \n 7           7 cat    69.3\n 8           8 dog    88.3\n 9           9 cat   101. \n10          10 dog   108. \n11          11 dog    79.8\n12          12 cat   106. \n# … with 88 more rows\n\n\nUnfortunately, some older functions in R won’t allow you to use a tibble. If this is the case, simply convert your tibble to a data.frame using the as.data.frame() function. Note, we use head() to see the head of our data frame, or the first 6 values. This is necessary here to avoid printing out each row, as we’re not in using a tibble any more. Notice that We’ve assigned the data.frame version of our IQ data to a new object, rather than overwriting the previous object. This is good practice when testing your code, as you never know what might break, resulting in data loss. (Although this wasn’t strictly necessary here.)\n\niq_data_df <- as.data.frame(iq_data)\nhead(iq_data_df)\n\n  participant pet        iq\n1           1 dog 119.90419\n2           2 cat 100.31550\n3           3 dog 106.28567\n4           4 cat 100.32896\n5           5 dog  99.40381\n6           6 cat 105.60851\n\n\nThere are multiple ways to access data from a tibble or data frame.\n\n1.5.3.3.1 Working with Columns\nWe can access columns through dollar indexing for a object, or by name or position as we did with lists. (In fact, this works because tibbles and data frames are just square lists!)\n\niq_data$iq\niq_data[[\"iq\"]]\niq_data[[3]]\n\n\n\n  [1] 119.90419 100.31550 106.28567 100.32896  99.40381 105.60851  69.25938\n  [8]  88.32001 101.35022 108.49565  79.78903 106.13353 102.67474  97.29907\n [15]  88.10597 109.67761 109.34948 102.41409 115.91469 126.64919  80.55541\n [22]  93.51404 103.97598  75.78122  97.44319 107.84348  91.55643  81.11721\n [29] 108.46863 113.81963  96.25558  94.15728 122.85215  86.35401 109.98973\n [36] 104.70575  95.83989  62.79169  90.88445 100.07760  82.95541  95.33197\n [43] 114.02527 112.32369  81.45885 111.64807  87.57530  94.64206  99.15243\n [50]  61.11503 106.62990  79.72260 100.15081  83.41594 114.31351  99.14609\n [57]  84.50973  81.42513 114.89340 124.02161  89.06398  93.27161  94.83063\n [64] 106.89741 111.33804  84.21109 126.75998 106.26074 120.37403  77.79786\n [71] 128.42155 112.19440  92.08103 104.15516 127.38645 105.28515  90.39646\n [78]  96.97260  99.75440 124.81151 107.16399 106.19192  70.66848  78.42079\n [85]  94.77819  92.42836  96.64234  91.71500 133.46106  84.11078  88.70980\n [92]  80.65210  94.50641  93.65194  87.23135 109.73530 128.65171  91.49707\n [99]  76.43452 108.62265\n\n\nAll three methods pull out every value from the IQ column as a basic vector. Just like with lists, by using the double bracket method you get just the values, and not the name of the object.\n\n1.5.3.3.1.1 Adding or Removing Columns\nTo add a row to a data frame, we simply need to specify what we want to add and assign it a new name. Let’s say that we want to add a column that indicates the operating system used by each participant.\nWe may have this because we made assumptions that people who use Windows, macOS, or the Linux families of operating systems differ in their IQ. This is a silly example for several reasons, not only because you can use more than one system; but we’ll stick with this for now.\nImagine we already have a sample of operating systems to draw from. You don’t need to understand how this works, but briefly I’ve used the inbuilt sample() function to pick from the three names with replacement, skewing the probabilities to select Windows most often, followed by Mac, then Linux. All that matters is that we’re assigning 100 names to an object.\n\nset.seed(1892) # set the random seed\n\noperating_system <- sample(\n  c(\"windows\", \"mac\", \"linux\"), \n  size = 100, \n  replace = TRUE,\n  prob = c(0.5, 0.3, 0.2)\n)\n\nIn the iq_data data set, we can add the new column using the usual assignment operator.\n\niq_data$operating_system <- operating_system # add new column\niq_data\n\n# A tibble: 100 × 4\n   participant pet      iq operating_system\n         <dbl> <chr> <dbl> <chr>           \n 1           1 dog   120.  linux           \n 2           2 cat   100.  mac             \n 3           3 dog   106.  mac             \n 4           4 cat   100.  windows         \n 5           5 dog    99.4 windows         \n 6           6 cat   106.  windows         \n 7           7 cat    69.3 mac             \n 8           8 dog    88.3 mac             \n 9           9 cat   101.  mac             \n10          10 dog   108.  linux           \n# … with 90 more rows\n\n\nNote that you can rename the column to anything you like. But, for consistency, I like to keep the same name as the object which acts as the data source.\nFinally, we can remove the new column (and any column) by setting the entire column to nothing (NULL), like so:\n\niq_data$operating_system <- NULL # remove the column\niq_data\n\n# A tibble: 100 × 3\n   participant pet      iq\n         <dbl> <chr> <dbl>\n 1           1 dog   120. \n 2           2 cat   100. \n 3           3 dog   106. \n 4           4 cat   100. \n 5           5 dog    99.4\n 6           6 cat   106. \n 7           7 cat    69.3\n 8           8 dog    88.3\n 9           9 cat   101. \n10          10 dog   108. \n# … with 90 more rows\n\n\nNow the data is back to its original format.\n\n\n\n1.5.3.3.2 Working with Rows\nWe can access rows again using the name or position indexing as above. However, since we’re accessing multiple columns we won’t be able to pull them out as a single vector. This means we can’t use the double bracket notation. Instead, we use single bracket notation and use a comma to specify what we want from rows, and what we want from columns. Remember, rows first, then columns.\nLet’s get the first two rows from the pet column. There’s a few ways we could do this:\n\niq_data[1:2, \"pet\"]\niq_data[c(1, 2), \"pet\"]\niq_data[1:2, 2]\n\n\n\n# A tibble: 2 × 1\n  pet  \n  <chr>\n1 dog  \n2 cat  \n\n\nYou’ll notice that R returns a tibble even when we subset this time. That’s because we’ve asked for specific rows from the entire table. If we want just these rows from a column, we can combine the notation with list-style subsetting as we used before.\n\niq_data[1:2, \"pet\"]$pet\niq_data[1:2, \"pet\"][[\"pet\"]]\niq_data[1:2, \"pet\"][[1]]\n\n\n\n[1] \"dog\" \"cat\"\n\n\nWe can include multiple columns in the first column index to get specific rows for a subset of columns.\n\niq_data[1:2, c(\"pet\", \"iq\")]\n\n# A tibble: 2 × 2\n  pet      iq\n  <chr> <dbl>\n1 dog    120.\n2 cat    100.\n\n\nOnce you know how to index these values, assigning new values to them is just as easy as before. Simply use the assignment operator <-.\nThis can all be a little unintuitive, so in future chapters we’ll look at how to use the tidyverse functions to subset rows and columns more easily.\n\n1.5.3.3.2.1 Adding or Removing Rows\nWhat if we want to add a new row to our data? This may be less common than adding a new column for data processing purposes, but it’s good to know anyway.\nFirst, we need to know what should go in each cell. Remember that we have to keep the data square, so you can’t have missing values when you add a row. If you don’t have any data, you can just put NA (with no quotations) to keep the data square but to show that you don’t have any value for a given cell.\nLet’s assume we want to add a new participant, 101, who has a dog but an unknown IQ. We must define a list of data where we assign values to the columns that match up with our IQ data column headings.\nHere, we have to define all our values to be added in parentheses, using the list() function:\n\nparticipant number is 101\npet_id is “dog”\niq is NA (i.e. unknown)\n\nThen we assign this list of values to the data frame in the 101st row.\nWe do this like so:\n\niq_data[101, ] <- list(\n  participant = 101, \n  pet = \"dog\",\n  iq = NA\n)\n\niq_data\n\n# A tibble: 101 × 3\n   participant pet      iq\n         <dbl> <chr> <dbl>\n 1           1 dog   120. \n 2           2 cat   100. \n 3           3 dog   106. \n 4           4 cat   100. \n 5           5 dog    99.4\n 6           6 cat   106. \n 7           7 cat    69.3\n 8           8 dog    88.3\n 9           9 cat   101. \n10          10 dog   108. \n# … with 91 more rows\n\n\nRemember that data frames and tibbles have to be square (i.e. with data in every column). This means if we just assign a participant ID to a column, all remaining rows are completed with NA.\n\niq_data[102, \"participant\"] <- 102\n\ntail(iq_data)\n\n# A tibble: 6 × 3\n  participant pet      iq\n        <dbl> <chr> <dbl>\n1          97 cat   129. \n2          98 cat    91.5\n3          99 cat    76.4\n4         100 dog   109. \n5         101 dog    NA  \n6         102 <NA>   NA  \n\n\n\n\n\n\n1.5.3.4 Matrices\nMatrices work very similarly to data frames and tibbles, but they’re even stricter. They can only contain the same data type throughout, so we can’t mix columns containing characters and numbers without converting them all to the same data type. Here’s how you’d make a matrix. However, we won’t go into any other details here. You’ll mainly come across matrices only when doing more advanced statistics by hand or developing your own statistical packages. For most of your data work, this isn’t necessary.\n\nmatrix_example <- matrix(\n  rep(1:25),\n  nrow = 5,\n  ncol = 5\n)\nmatrix_example\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    6   11   16   21\n[2,]    2    7   12   17   22\n[3,]    3    8   13   18   23\n[4,]    4    9   14   19   24\n[5,]    5   10   15   20   25"
  },
  {
    "objectID": "01_getting-started.html#quarto-documents",
    "href": "01_getting-started.html#quarto-documents",
    "title": "1  Getting Started",
    "section": "1.3 Quarto Documents",
    "text": "1.3 Quarto Documents\nBefore we start coding, it’s worthwhile explaining how we can create scripts or documents for our code that allow us to perform our data processing tasks.\nTraditionally, R users often wrote their code in R Scripts. You can try this now by going to File > New File > R Script. This will open a pane in your editor where you can write your R code. Crucially, you can save this file to your computer, allowing you to return to your work at a later date or to rerun your code. Notice that at the moment the pane is labelled Untitled1. That’s because you haven’t saved your work yet. Go to File, Save or click the floppy disk icon just under the tab for this pane to save your work.\n\n1.3.1 Literate Programming\nScripts are a great way to work with R, but they can be difficult to manage (especially for beginners) and even if these scripts produce files or graphs, you’re still left with the prospect of putting together your scientific outputs in a separate word processor, which often involves a lot of copying and pasting.\nInstead, one option is to produce your outputs with your R code embedded within them. This has the advantage of cutting down on transcription errors and time tweaking the output of your documents every time you update something. Let’s imagine you write a paper based on a project with several analyses and containing several plots. Unfortunately, you didn’t realise that one participant should have been excluded from the analyses all along (we all make mistakes). If you made your report manually, you’ve got a lot of manual edits to make. If you let R populate your outputs in the document, it’s as simple as updating a single line of code to remove that participant and rerunning the analyses by pressing a big play button. This should cut down on further human error.\nFor these reasons, I’m a fan of Quarto: open-source scientific and technical publishing system that allows you to create dynamic documents with content using R, Python, Julia, or Observable code. The advantage of Quarto over other literate programming systems in R (such as RMarkdown) is that if you ever change your programming language to one of the other 3, your workflow stays the same. This reduces overheads to being multilingual.\n\n\n1.3.2 Creating a Quarto Document\nCreate a Quarto document by going to File > New File > Quarto Document. You can set the name and author of the document here or define those later on. Keep the default options for the output type and engine for rendering and click Create. This creates a Quarto document with some boilerplate code and text to show you how it works.\n\n\n\nThe default Quarto template\n\n\nSave this document somewhere where you’ll have access to it again if you’d like to keep a record of the code used in the remaining sections of this chapter.\n\n\n1.3.3 Understanding Quarto Documents\nBy default, RStudio opens the document using the visual editor. This allows you to type text directly into RStudio in a similar way to how you would with other word processing software. Notice the headings below the tab that allow you to apply different styles to the text, to create lists, links, and include images and tables.\n\n1.3.3.1 YAML Headers\nAt the top of the document is the YAML header. This stands for YAML Ain’t Markup Language (illuminating, I know). Essentially, this is a highly-readable format for configuring your Quarto document. Options are presented as bare text with a colon and can define the content, look, or behaviour of your document. Here, if we change title: \"Untitled\" to title: \"My First Quarto Document\" when you render the file your title will be updated. That’s all we need to know about the heading for now.\n\n\n1.3.3.2 Markdown\nText is authored using Markdown. (Specifically, the Pandoc flavour of Markdown.) This is a plain text syntax and tool for converting your text to HTML without having to know much, if any, HTML. The idea behind this system is that you can write your content and use some simple syntax to control the formatting of the text. The exact look of it is then handled by a template, several of which come with Quarto. This is different to a WYSIWIG (What You See Is What You Get) editor such as Word where you must define the content, formatting, and presentation at the same time. One advantage of Markdown is that you can create a document based on e.g. the APA manuscript template and then immediately change it to a specific journal’s formatting by swapping out your template. No more manual edits.\nFor an introduction to Markdown formatting, see https://quarto.org/docs/authoring/markdown-basics.html. But, since we’re using the visual editor this won’t be necessary right now.\n\n\n1.3.3.3 Embedding Code\nFinally, you’ll notice that the Quarto boilerplate includes some code chunks. These can be inserted by clicking the green +C in the header of RStudio, by Cmd + Option + I on a Mac or Ctrl + Alt + I on PC, or by typing:\n```{r}\n```\nYour code then goes in the empty space between the backticks.\nCode chunks allow you to write long expressions in R that span multiple lines. The output of that code is then presented in your document immediately below the code that produced it (unless you set options for your code to not show up). Press play on the code chunk or highlight the code and press Cmd + Enter or Ctrl + Enter to run it.\nFinally, you can embed R code within text by using surrounding your code with backticks with the inclusion of the language (R) at the header of this. Here’s an example: `r 1 + 1 `. This is useful when including in-text statistics in a document.\n\n\n1.3.3.4 Rendering Your Document\nTo create an output file which renders your Markdown and code chunks into a nicely formatted output (like this ebook you’re viewing now), simply click the Render button at the top of your editor. This will create a rendered document in the same location as your Quarto file, rendered in the format you specify in the YAML header. By default, this is an html file. Figure 1.3 shows how the source code and rendered html file compare.\n\n\n\n\n\n\n\n(a) Quarto source code\n\n\n\n\n\n\n\n(b) Quarto rendered html output\n\n\n\n\nFigure 1.3: Rendering Quarto documents"
  },
  {
    "objectID": "01_getting-started.html#section",
    "href": "01_getting-started.html#section",
    "title": "1  Getting Started",
    "section": "1.6 ",
    "text": "1.6 \n\n\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686."
  },
  {
    "objectID": "01_getting-started.html#project-management",
    "href": "01_getting-started.html#project-management",
    "title": "1  Getting Started",
    "section": "1.2 Project Management",
    "text": "1.2 Project Management\nA large part of creating an effective workflow for your data processing needs revolves around having an effective system for managing the inputs, processing scripts, and outputs associated with your project. By using a logical folder structure and consistent naming conventions you can make working with and managing updates/changes to your project much easier.\n\n1.2.1 File Systems\nWhen working with R, its important to know where the working directory of your project is based. The working directory is essentially the home base for R: where it looks by default when you try to read into R or save data from R to file.\nBy default in RStudio this depends on how you open RStudio:\n\nBy opening RStudio: the working directory is wherever you installed R on your computer.\nBy opening a .R file: the working directory is the location of the file.\nBy opening an RStudio Project: the working directory is the location of the project. More on this later!\n\nTo find out where your working directory is in RStudio right now, type in the console getwd() and press Enter to get the working directory. (Henceforth, if you see code like this, try it out by either copying and pasting into the console or typing it out and pressing Enter.)\nAfter using getwd() you should see something like \"/Users/glenn\", with glenn replaced with whatever your home folder is. This means that by default if I want to read some data into R or write it to a file then R will look in this folder to do so. But, you probably want to have a better organisational system than having many files floating around in your home directory. One way around this is to create a specific folder for your project and set your working directory to that folder. Imagine I have a folder in my home directory called “DS-Psych”. I might set the working directory to this location by using setwd(\"/Users/glenn/DS-Psych\"). Now, R will by default read/write files at this location.\nBut, if you share this with script with someone (or even yourself on say, a computer at work), it won’t work on their machine because you can guarantee they won’t have the same file structure as you. For example, they might have a home directory called rachel or george. Now using setwd(\"/Users/glenn/DS-Psych\") will result in an error.\nSo, how do we solve the problem of (a) knowing exactly where our working directory is, and (b) ensuring that directory works across different machines? RStudio Projects are the answer.\n\n\n1.2.2 RStudio Projects\nAn RStudio Project is a file that can sit in a folder which allows you to open an instance of RStudio in the root (top level) of the folder.\nYou can create the folder from scratch within RStudio with an associated Project file, or add the Project file to an existing folder within RStudio. To do this click File > New Project and select the relevant option for your use case.\n\n\n\nCreating an RStudio Project\n\n\nNow, whenever you want to start up RStudio and ensure that your working directory is in the project folder, you simply need to open the RStudio project file that sits in the folder. This will work across different computers for different people, meaning that simply sharing the folder with other people is enough to ensure that R looks in the correct place for files when they run your scripts.\nTo further ensure that your script runs when, for example, reading in or writing data to file, its good practice to ensure that you way you direct R to files avoids specifying locations that are only present on your computer.\n\n\n1.2.3 Absolute and Relative File Paths\nImagine you have a folder called analysis on your desktop containing the following items:\n\ninputs/my_data.csv\noutputs/\nmy_script.R\nanalysis.RProj\n\nThe data you want to analyse, my_data.csv, sits in a sub-folder called inputs. You’d like to write a script that reads this data in, creates a graph, and saves that graph in the outputs folder.\nYou open up RStudio by double clicking on the analysis.RProj file. This ensures that R knows the working directory is set to the root of the analysis folder (i.e. wherever the .RProj file sits). You can read the data into R in one of two ways, using:\n\nAbsolute file paths: You specify the exact location of the file on your computer using the full location from your root directory. For me, this would be /Users/glenn/Desktop/analysis/inputs/my_data.csv\nRelative file paths: You specify the location of the file relative to your working directory. For all of us, this would be: /inputs/my_data.csv\n\nNot only are relative file paths shorter and easier to manage, but they will work on anyone’s computer as long as they have your project folder. This makes reproducibility and collaboration much easier.\n\n\n1.2.4 Naming Conventions\nSince you’ll often work with files and folders, it’s a good idea to establish a consistent naming convention. Jenny Bryan has a great presentation on why this matters, based around 3 principles. Make names:\n\nMachine readable: File names should be easily read by computers. Avoid spaces, punctuation, accented characters, and case sensitivity.\nHuman readable: File names should be easily read by humans. Include a slug to define what a file is or does (e.g. 01_read-files.R). Separate words with a dash to meet the machine readable and human readable criteria. Use underscores to separate slugs (e.g. concepts).\nPlay well with default ordering: Numerics come first. Left pad numbers to ensure proper numeric ordering. Use the ISO 8601 standard (YYYY-MM-DD) for dates.\n\nExamples of poor and good naming conventions are provided below:\n\nExamples of poor and good file naming conventions\n\n\nPoor Naming Conventions\nGood Naming Conventions\n\n\n\n\n26012022_rp1.csv\n2022-01-26_reading-data_participant-01.csv\n\n\n26012023_wp1.csv\n2023-01-27_writing-data_participant-02.csv\n\n\n10.R\n01_read-data.R\n\n\n1.R\n02_plot-data.R\n\n\n…\n…\n\n\n2.R\n10_fit-models.R\n\n\n\nNotice that with poor naming conventions not using the correct date changes the order of the files for both the .csv files (based on date) and by not left-padding the R files? Also, it’s clear that using too many abbreviations or simply naming files with inscrutable titles will make working with this a nightmare."
  },
  {
    "objectID": "01_getting-started.html#some-final-tips",
    "href": "01_getting-started.html#some-final-tips",
    "title": "1  Getting Started",
    "section": "1.6 Some Final Tips",
    "text": "1.6 Some Final Tips\nFinally, a few tips on checking your data before you manipulate your data:\n\nIf you’re unsure what objects you’ve created in a session, either check the environment pane in RStudio or type ls() to list everything in the global environment.\nIf you want to know the class of data for some object, use the class() function (e.g. class(iq_data)).\nIf you want to know the structure (including object classes) for some object, use the str() function (e.g. str(iq_data). Nicely, str() also tells you how many arrays are in the object, and how many observations you have in total.\n\nI strongly recommend that you choose a style guide and stick to it throughout when you write your R code. This will make it easier to notice any errors in your code, and increases readability for you and others. Consistency is key here. Since we’re using a tidyverse first approach to teaching R in this course, I recommend the following one by Hadley Wickham, a core developer of the tidyverse.\n\n1.6.1 R Style Guide by Hadley Wickham\nThe important things to take home are that:\n\nUse sensible object names: if a column shows, e.g. participant weight, call it participant_weight.\nUse verbs to describe user-defined functions: if you write a function to make all the descriptive statistics you could ever want, call it something like make_descriptives().\nUse a consistent style, like snake_case, or even camelCase, but don’t mix_snake_and_camelCase.\nComment your code with descriptions of why you’ve done something using #: you can often work out how you did it by following your code, but the why is easily lost.\n\n\n\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686."
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Data Science for Psychologists",
    "section": "Course Content",
    "text": "Course Content\nTo download the course content, including workshop slides, videos, and exercise workbooks, including instructions for using and downloading the content, please follow this link: https://github.com/gpwilliams/ds-psych_course"
  },
  {
    "objectID": "00a_part-core.html",
    "href": "00a_part-core.html",
    "title": "Core",
    "section": "",
    "text": "This section is dedicated to getting you from no experience with R or programming whatsoever to being able to use R for your entire data processing pipeline.\nAt the end of this section you will be able to perform complex data processing operations, including combining and cleaning multiple data sets, create beautiful, publication-ready graphs, compute descriptive and inferential statistics with a deep understanding of the general linear model family of analyses, create beautiful dynamic reports that can generate content from code, and do all of this while adhering to best practices in terms of open science practices.\nOnce you understand the core content, you can optionally move on to the Advanced topics. This section consists of modular chapters that introduce you to advanced topics in R for which R excels."
  },
  {
    "objectID": "00b_part-advanced.html",
    "href": "00b_part-advanced.html",
    "title": "Advanced",
    "section": "",
    "text": "This section consists of modular chapters that introduce you to advanced topics in R for which R excels. Here, the focus is on introducing the conceptual and philosophical basis for these methods, learning how to implement them, but also crucially on how to communicate these results and tackle any potential issues encountered when executing these methods.\nIt is expected that you are at least comfortable with the Core topics before attempting these chapters. The structure of this section is such that you can read each chapter in any order you like, so feel free to focus on chapters that are only useful to your research."
  },
  {
    "objectID": "index.html#core-content",
    "href": "index.html#core-content",
    "title": "Data Science for Psychologists",
    "section": "Core Content",
    "text": "Core Content\nThe first section of the book is written for those with no experience with R or programming in general. The focus here is on using R across the data processing and analysis pipeline so that you can automate data processing. This has the benefit of documenting your work, automating tedious manual processes, and avoiding user error. Along the way you will learn about best practices in terms of project structure and workflows, version control to track and manage updates to your code, and how to share your work online with the broader scientific community. By the end of this section you should be able to do all your data analysis work with R."
  },
  {
    "objectID": "index.html#advanced-content",
    "href": "index.html#advanced-content",
    "title": "Data Science for Psychologists",
    "section": "Advanced Content",
    "text": "Advanced Content\nThe second section of the book is written for those with a background in R who want to take advantage of the advanced data analysis methods available to this language. This section has a heavier focus on theory with the goal of understanding not just how to use advanced methods, but also how these methods work."
  },
  {
    "objectID": "index.html#approach-to-coding",
    "href": "index.html#approach-to-coding",
    "title": "Data Science for Psychologists",
    "section": "Approach to Coding",
    "text": "Approach to Coding\nThroughout, concepts will be taught using examples from real and simulated data from studies in Psychology. R will be taught using a tidyverse-first approach, using a suite of packages that are designed to make programming quick, easy, and highly readable."
  },
  {
    "objectID": "index.html#sections",
    "href": "index.html#sections",
    "title": "Data Science for Psychologists",
    "section": "Sections",
    "text": "Sections\n\nCore\nThe first section of the book is written for those with no experience with R or programming in general. The focus here is on using R across the data processing and analysis pipeline so that you can automate data processing. This has the benefit of documenting your work, automating tedious manual processes, and avoiding user error. Along the way, you will learn about best practices in terms of project structure and workflows, version control to track and manage updates to your code, and how to share your work online with the broader scientific community. By the end of this section you should be able to do all your data analysis work with R.\n\n\nAdvanced\nThe second section of the book is written for those with a background in R or who have completed the core content who want to take advantage of the advanced data analysis methods available to this language. This section has a heavier focus on theory with the goal of understanding not just how to use advanced methods, but also how these methods work, how to diagnose and solve problems, and how to communicate your findings."
  },
  {
    "objectID": "02_creating-graphs.html#the-grammar-of-graphics",
    "href": "02_creating-graphs.html#the-grammar-of-graphics",
    "title": "2  Creating Graphs",
    "section": "2.1 The Grammar of Graphics",
    "text": "2.1 The Grammar of Graphics\nIn R, you can build plots using the base plotting system or lattice graphics. The former is very low level, giving you a lot of granular control over plotting at the cost to specifying everything manually. The latter is more high level but makes customisation difficult. Thankfully, ggplot2, a package in the tidyverse strikes a good balance between the two. Most notably, however, it relies on a grammar of graphics to make creating bespoke plots consistent across many types of visualisations.\nBroadly, a grammar of graphics is a set of rules we can use to describe the components of a graphic. In the context of creating plots in R, the approach moves away from focusing on specific types of plots (e.g. a scatter plot, a box plot, a bar plot), instead focusing on the elements of the plot that can be build up to create different visualisations (Wickham 2010).\nIn ggplot2, this means every plot must define a data set from which to map onto a canvas. From here, we add layers to the plot. Some key concepts that form the basis of every plot are:\n\ngeoms: these are geometric objects that you map your data to. For example, points on a scatter plot.\naes: these are the aesthetics you define which map certain parts of your data to the plot. For example, the which parts of your data should be mapped onto the x and y coordinates of a scatter plot.\nscale: the scale you want to have for your axes. This is often calculated using sensible defaults in ggplot2, but sometimes you’d like control over this.\nannotations: Labels used for axes, titles, captions, etc. to help with communicating your results. The functions lab() and annotate() are key here.\n\nIn addition to these key ideas, ggplot2 allows you to create faceted plots, where we can produce many panels of a plot across variables, allowing you to create many sub plots with a simple, one-line command.\nBeyond these basic ways to define your plot, ggplot2 comes with various themes that set sensible defaults for many parameters that define the look of the plot. We will explore these and how to customise them."
  },
  {
    "objectID": "02_creating-graphs.html#building-a-plot-layer-by-layer",
    "href": "02_creating-graphs.html#building-a-plot-layer-by-layer",
    "title": "2  Creating Graphs",
    "section": "2.2 Building a Plot, Layer by Layer",
    "text": "2.2 Building a Plot, Layer by Layer\nTo get started, we’ll need a data set. R itself comes with a few data sets build in that we can use for examples. However, there are more fun data sets to use in the packages we’ve already installed. We’ll use the starwars data set from dplyr.\nAs always, we’ll first load the tidyverse.\n\nlibrary(tidyverse)\n\nOnce loaded, we can load up the starwars data set from the dplyr library (one of the libraries loaded up when using library(tidyverse). Let’s take a look at it.\n\nstarwars\n\n# A tibble: 87 × 14\n   name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n   <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n 1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n 2 C-3PO          167    75 <NA>    gold    yellow    112   none  mascu… Tatooi…\n 3 R2-D2           96    32 <NA>    white,… red        33   none  mascu… Naboo  \n 4 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n 5 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n 6 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n 7 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n 8 R5-D4           97    32 <NA>    white,… red        NA   none  mascu… Tatooi…\n 9 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n10 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n# … with 77 more rows, 4 more variables: species <chr>, films <list>,\n#   vehicles <list>, starships <list>, and abbreviated variable names\n#   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\nWe have 87 rows of data with 14 columns containing information about different characters from Star Wars.\nLet’s build our plot up, one layer at a time. Every plot made in ggplot2 must define the dataset within the ggplot() function.\n\nggplot(data = starwars)\n\n\n\n\nThat hasn’t done much. All we did was create a canvas for our data. But, this is the first step to any plot.\nNext, we’ll add our geometric objects to the plot. We’ll make a scatter plot of the mass and height of each character. To do this, we use the geom_point() function. But, we’re not done yet. The following code won’t run because we’re missing some key information. Try this yourself to see what’s wrong.\n\nggplot(data = starwars) +\n  geom_point()\n\nWe have to specify how ggplot2 should map the data we have onto the aesthetics of the plot to create the points. To do this, we need to tell it which parts of our data set should be mapped onto the relevant aesthetics of the plot. For simple points, this means we must define the key aesthetics of a point, the x and y values on the axis and where to get these in our data set.\nTry this code below, which should now work properly.\n\nggplot(data = starwars) +\n  geom_point(mapping = aes(x = mass, y = height))\n\nWarning: Removed 28 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWe have a plot, but we also got a warning stating that we have 28 rows in our data set that contain missing values. These rows with missing data are thus missing from the plot. ggplot() tells us about this missing data because it’s generally good practice to explicitly remove values with missing data before you try to plot them. Take this as a reminder that either (1) you need to remove these points prior to plotting, or (2) you can confirm that the number of missing values ggplot() tells you about matches what you’d expect from your understanding of the data.\n\n\n\n\n\n\nNote\n\n\n\nWarnings in R are there to tell you that the output of your code may not contain what you wanted it to contain. In the previous plot, ggplot dropped those with missing heights and masses, even though we didn’t explicitly tell it to do so. If you remove these ahead of plotting you won’t get a warning.\nWarnings are different to errors in that your code will still work, but you need to check out whether it did what you wanted it to do. On the other hand, errors indicate you did something wrong and your code will fail to run.\n\n\nA further thing you might notice here is the big outlier. One character has a middling height but a huge mass. We might choose to highlight this or do further investigations later. Regardless, this is a good reminder that the best thing you can do to understand your data prior to analysis is plot it.\n\n2.2.1 Aesthetics\nWhile for geom_point() the mandatory aesthetics are x and y values for plotting the points, you can also define additional aesthetics based on variables within your dataset. Let’s pick out the gender of each character.\n\n2.2.1.1 Colour\n\nggplot(data = starwars) +\n  geom_point(mapping = aes(x = mass, y = height, colour = gender))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may have noticed my plots have stopped outputting a warning. I’ve suppressed this in Quarto by using the code chunk options. You can set many options for chunks. Here, I’ve set messages such as warnings to not display. You do this by inserting your options using the hash-pipe (not the Weezer song) as follows:\n#| warning: false\n\n\n\n\n2.2.1.2 Coordinates\nWhile we’ve now picked out each dot with the colour representing gender, it’s difficult to see any real trends here due to the outlier. We can tackle this in two ways:\n\nFilter out the outlier prior to plotting the data. We’ll cover this in Transforming Data.\nChange the coordinates of the plot to only include a limited range.\n\nWe’ll go with the latter option now to show the capabilities of ggplot2 and to highlight how we can build plots up layer by layer by just adding more commands.\nTo limit the x-axis to a restricted range we need to provide two concatenated values, the start and end of the axis, to the xlim argument in the coord_cartesian() function. This function also takes ylim arguments if you’d like to limit that too.\n\nggplot(data = starwars) +\n  geom_point(mapping = aes(x = mass, y = height, colour = gender)) +\n  coord_cartesian(xlim = c(0, 180))\n\n\n\n\nNow we have a better understanding of our data and any general trends across those within a more restricted range of masses.\n\n\n\n\n\n\nWarning\n\n\n\nWe can alternatively remove data from our plot by setting limits on the x-axis using scale_x_continuous(limits = c(0, 180)). This may seem more intuitive than coord_cartesian(), but it throws out the data points outside the limits prior to plotting. This isn’t a problem in this instance, but if we want to, for example, draw a line of best fit through our data, scale_x_continuous() will ignore the outlier, while coord_catesian() will still include it. Of course, the option to include or exclude this in your estimates is a decision you have to make informed by your domain knowledge. But, being aware of what is going on in each option is crucial to make sure you make the correct inferences.\n\n\nWhat if we want to change the look of our points, but we want to apply this change to every point (i.e. not making it vary by some column in the data set). We simply have to specify some options outside the aes() call.\n\nggplot(data = starwars) +\n  geom_point(\n    mapping = \n      aes(\n        x = mass, \n        y = height, \n        colour = gender\n      ),\n    alpha = 0.7, # opacity\n    shape = \"triangle\", # triangles\n    size = 4 # bigger points\n  ) +\n  coord_cartesian(xlim = c(0, 180))\n\n\n\n\nNow we’ve set fixed values for some aesthetics, specifically setting the alpha (opacity), shape, and size of the points. Here’s a good list of the aesthetic specifications which provides a good cheat sheet to all the aesthetics you can change and their options.\nSetting an aesthetic within aes() and mapping it to a column in your data allows it to vary. Alternatively, you can define the colour to be set outside of the aesthetic to make it consistent across all data points. If you set both at the same time, the fixed aesthetic takes precedence. Give it a go.\n\nggplot(data = starwars) +\n  geom_point(\n    mapping = \n      aes(\n        x = mass, \n        y = height, \n        colour = gender\n      ),\n    alpha = 0.7, # opacity\n    shape = \"triangle\", # triangles\n    size = 4, # bigger points\n    colour = \"red\"\n  ) +\n  coord_cartesian(xlim = c(0, 180))\n\n\n\n\n\n\n2.2.1.3 Fill\nA final thing to bear in mind is that colour and fill are different properties within ggplot2. Below, we’ll change the triangles to filled circles (i.e. a with a border) and we’ll map gender to the fill aesthetic instead of colour.\n\nggplot(data = starwars) +\n  geom_point(\n    mapping = \n      aes(\n        x = mass, \n        y = height, \n        fill = gender\n      ),\n    alpha = 0.7, # opacity\n    shape = \"circle filled\", # triangles\n    size = 4, # bigger points\n    colour = \"red\"\n  ) +\n  coord_cartesian(xlim = c(0, 180))\n\n\n\n\nNotice how the colour of the circle varies, but the fill of the border is fixed?\nFinally, we can see how smart ggplot2 can be in terms of setting legends. If we use a continuous, rather than a categorical variable to define the colour of points you’ll notice that we get a gradient of colours.\n\nggplot(data = starwars) +\n  geom_point(\n    mapping = \n      aes(\n        x = mass, \n        y = height, \n        colour = birth_year\n      )\n  ) +\n  coord_cartesian(xlim = c(0, 180))\n\n\n\n\nNotice the very short and light character highlighted in light blue, indicating they are very old? Who could that be?"
  },
  {
    "objectID": "02_creating-graphs.html#geoms",
    "href": "02_creating-graphs.html#geoms",
    "title": "2  Creating Graphs",
    "section": "2.3 Geoms",
    "text": "2.3 Geoms"
  },
  {
    "objectID": "02_creating-graphs.html#facets",
    "href": "02_creating-graphs.html#facets",
    "title": "2  Creating Graphs",
    "section": "2.6 Facets",
    "text": "2.6 Facets\nAnother useful part of plotting in ggplot2 is that you can make facets of plots, or subplots. This is a good way to display your data if you have multiple categorical variables. Essentially, you’ll get a plot for each category in your data. There are two approaches to faceting with ggplot2:\n\nfacet_wrap(): Let ggplot figure out the rows and columns for you, wrapping your plot around to make the most use of the plotting area.\nfacet_grid(): Manually define variables which should be mapped onto the columns or rows of the plot.\n\nIn each case, you have some control over the number of rows and columns you’d like in the plot. Of course, with facet_wrap() this can override some of the behaviour that maximises the use of the plotting space.\n\n2.6.1 Facet Wrap\nFor facet_wrap(), we define the variables that are mapped onto columns using R’s formula notation, like so: facet_grid(. ~ variable). The dot signifies that we’re not plotting anything onto separate rows, while the variable name to the right of the ~ (read: tilde) denotes the variable you’d like to split your plots by, plotting onto separate columns.\n\nggplot(data = starwars, mapping = aes(x = height)) +\n  geom_histogram(fill = \"white\", colour = \"black\") +\n  facet_wrap(. ~ gender)\n\n\n\n\n\n\n2.6.2 Facet Grid\nFor facet_grid(), we define the variables that are mapped onto columns or rows using R’s formula notation, like so: facet_grid(rows ~ columns).\n\nggplot(data = starwars, mapping = aes(x = height)) +\n  geom_histogram(fill = \"white\", colour = \"black\") +\n  facet_grid(eye_color ~ gender)\n\n\n\n\nIn this case, we have limited data for each combination of the variables, so many panels are empty. However, this still gives you a good idea of both the data set and the functionality of facet_grid().\nBy default ggplot2 sets a consistent y-axis range across all facets. However, if you’d like the scales to vary within each facet you have some flexibility:\n\nscales = \"fixed\": Both x and y axis have fixed scales across all facets.\nscales = \"free\": Both x and y axis have different scales defined by the range of the data in each facet.\nscales = \"free_x\" or scales = \"free_y\": Either the x or y axis have free scales respectively, with the other scale set to fixed."
  },
  {
    "objectID": "02_creating-graphs.html#customisation",
    "href": "02_creating-graphs.html#customisation",
    "title": "2  Creating Graphs",
    "section": "2.5 Customisation",
    "text": "2.5 Customisation"
  },
  {
    "objectID": "02_creating-graphs.html#combining-plots",
    "href": "02_creating-graphs.html#combining-plots",
    "title": "2  Creating Graphs",
    "section": "2.7 Combining Plots",
    "text": "2.7 Combining Plots\nOften, you might several plots that serve different data visualisation needs, but you want to display these together in one place. If using Quatro, you can choose to save your plots externally (e.g. to a .png file) and then load them back into Quarto, using the options to display subfigures to display plots side by side. However, you may want to just display these plots in your document from R code without saving the plots (but still retaining this option if you choose). For this, we can use the patchwork package. To use this package, we have to load it up like any other package prior to using it.\n\nlibrary(patchwork)\n\nWe can assign our plots to variables just like any other object in R. Let’s create two plots, my_histogram and my_dotplot using the following code:\n\nmy_histogram <- ggplot(data = starwars, mapping = aes(x = height)) +\n  geom_histogram(fill = \"white\", colour = \"black\")\n\nmy_dotplot <- ggplot(data = starwars, mapping = aes(x = height, y = mass)) +\n  geom_point()\n\nThen we can simply add the plots together to display them side by side.\n\nmy_histogram + my_dotplot\n\n\n\n\nWe can make relatively complicated layouts by nesting plots together using () and defining separate rows using the /. First, let’s make one final before we make a 3 plot output with 2 rows.\n\nmy_boxplot <- ggplot(data = starwars, mapping = aes(x = gender, y = mass, colour = gender)) +\n  geom_boxplot() +\n  coord_cartesian(ylim = c(0, 200))\n\nNow, let’s add all of this together:\n\n(my_histogram | my_dotplot) / my_boxplot"
  },
  {
    "objectID": "02_creating-graphs.html#saving-plots",
    "href": "02_creating-graphs.html#saving-plots",
    "title": "2  Creating Graphs",
    "section": "2.8 Saving Plots",
    "text": "2.8 Saving Plots\nFinally, once you’ve created these plots you might want to save them for further use beyond R and Quarto documents. To do this, we use the ggsave() function which expects us to define a file name (optionally set at a specific path). Note that you need to include the file type. Often, you will want to save your plot as a .png file as these can be small, lightweight, allow transparency, and compress relatively well with little artifacting (cf. .jpegs). Alternatively, you can use a vectorised format (e.g. .svg or .pdf) that scales up and down to different sizes while maintaining the integrity of the plot even better than .png files.\nNext, ggsave() expects the name of the object you saved your plot to, or the last plot you created. I suggest you’re always explicit with this: Once happy with a plot, assign it to an object and save it by specifying the name of the plot. That way, you know you’ll have saved the correct version of your plot.\n\nggsave(\n  filename = \"my_plot.png\",\n  plot = my_histogram\n)"
  },
  {
    "objectID": "02_creating-graphs.html#advanced-functionality",
    "href": "02_creating-graphs.html#advanced-functionality",
    "title": "2  Creating Graphs",
    "section": "2.9 Advanced Functionality",
    "text": "2.9 Advanced Functionality\nThere’s a lot more we can do in ggplot beyond what we’ve covered here. But these basics cover the most important use cases you’ll come across. You now understand how to build plots, what options there are for different aesthetics and geometric shapes, how to change positioning of elements, how to create subplots, and how to combine and save your plots.\nBeyond this, you might want to explore using the stat_summary() functions in ggplot, which allow you to aggregate data and create statistics such as standard errors and 95% confidence intervals from your data. However, in later sessions we will explore how to get these values from fitted model objects and create plots directly from these values, obviating the need for calculating these statistics in ggplot.\nOnce you understand that, you might want to check out modern plotting approaches such as creating raincloud plots to show the raw data, density of data, central tendency and confidence intervals together in one attractive package.\n\n\n\n\nCorrell, Michael, and Michael Gleicher. 2014. “Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error.” IEEE Transactions on Visualization and Computer Graphics 20 (12): 2142–51. https://doi.org/10.1109/TVCG.2014.2346298.\n\n\nKang, Hyunmin, Jeayeong Ji, Yeji Yun, and Kwanghee Han. 2021. “Estimating Bar Graph Averages: Overcoming Within-the-Bar Bias.” I-Perception 12 (1): 204166952098725. https://doi.org/10.1177/2041669520987254.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n\n\nXiong, Cindy, Cristina R. Ceja, Casimir J. H. Ludwig, and Steven Franconeri. 2020. “Biased Average Position Estimates in Line and Bar Graphs: Underestimation, Overestimation, and Perceptual Pull.” IEEE Transactions on Visualization and Computer Graphics 26 (1): 301–10. https://doi.org/10.1109/TVCG.2019.2934400."
  },
  {
    "objectID": "02_creating-graphs.html#exploring-geoms",
    "href": "02_creating-graphs.html#exploring-geoms",
    "title": "2  Creating Graphs",
    "section": "2.3 Exploring Geoms",
    "text": "2.3 Exploring Geoms\nThere are many geoms that you can use in ggplot2. Some common ones you might use are bars (geom_bar()), boxes (geom_boxplot()), violins (geom_violin()), densities (geom_density()), and histograms (geom_histogram()). Let’s briefly see how we might use each geom.\n\n2.3.1 Bars\nBars are best used to indicate counts. They are often used in Psychology along with error bars to indicate means and 95% confidence intervals or standard errors. However, these aren’t the best way to display continuous data as it (1) hides any information about the distribution of data (e.g. groupings) that may be masked by a bar plot with error bars, and (2) anchor people to the top of the bar such that under or overestimate the magnitude of effects and the potential for dispersion (Kang et al. 2021; Xiong et al. 2020; Correll and Gleicher 2014). So, let’s just use them for counts!\n\nggplot(data = starwars) +\n  geom_bar(mapping = aes(x = gender))\n\n\n\n\n\n\n2.3.2 Boxplots\nBoxplots do a nice job of visualising central tendency (specifically the median) and dispersion (specifically the interquartile range), so they are a good choice for looking at continuous outcomes grouped by a categorical measure. Let’s make a boxplot for each gender looking at heights.\nggplot(data = starwars) +\ngeom_boxplot(mapping = aes(x = gender, y = height))\n\nggplot(data = starwars) +\n  geom_boxplot(mapping = aes(x = gender, y = height))\n\n\n\n\nHow should you interpret this?\n\nThe middle line represents the median\nThe upper white section of the box the upper quartile: 75% of scores fall below this.\nThe lower white section the lower quartile: 25% of scores fall below this.\nTogether the quartiles represent the interquartile range: The middle 50% of scores.\nThe limits of the whiskers (black lines) for the upper and lower parts of the graph represent the smallest and largest observations that are equal to the upper or lower quartiles minus or plus 1.5 times the interquartile range. Effectively, this is most of the rest of the data, apart from outliers.\nThe dots represent outliers (i.e. those values outside of the whiskers).\n\n\n\n2.3.3 Violins\nViolin plots show you the density of the mean scores. The wider the section of the violin, the more scores around that area. We set trim to FALSE within the violin plot so that we see the full tails of the data. If we set this to TRUE, then the tails are trimmed to the range of the data.\nIt can be useful to draw quantiles on the violin plot so they can communicate similar information as a box plot. To do this, set draw_quantiles to concatenated values indicating the quartiles you’re interested in. Here, we chose the upper and lower 25% along with the median.\n\nggplot(data = starwars) +\n  geom_violin(\n    mapping = aes(x = gender, y = height),\n    trim = FALSE,\n    draw_quantiles = c(0.25, 0.5, 0.75)\n  )\n\n\n\n\n\n\n2.3.4 Histograms\nOften, you want to get a general idea of the distribution of your data with the aim of looking at skewness and kurtosis. One way to do this is with a histogram. This bins values into a specific range on your x-axis and looks at counts for observations within each bin. By default, ggplot2 selects sensible bins for us, in this case bins incrementing by 30cm in height. But, you can change this by setting binwidth to whatever value you like.\n\nggplot(data = starwars) +\n  geom_histogram(\n    mapping = aes(x = height), \n    fill = \"white\",\n    colour = \"black\"\n  )\n\n\n\n\nFrom the histogram we can see that the more common heights lie between 150 and 200cm, but we have a fair spread of scores with a few characters much shorter and larger than this.\n\n\n2.3.5 Density Plots\nDensity plots work like histograms but apply kernel smoothing to create a density line that approximates the shape of the histogram. This can often be easier to interpret than a histogram as it smooths out the noise in the histogram to make looking at general trends a little easier.\nAs with other geoms in ggplot2 we can customise our plot by picking out categories in different colours, and even changing the opacity of the plots if we’re concerned with overplotting. Let’s look at the densities of the heights by each gender.\n\nggplot(data = starwars) +\n  geom_density(\n    mapping = aes(x = height, fill = gender), \n    alpha = 0.5\n  )\n\n\n\n\nWe can see that the distribution of heights is different between genders. Due to the small number of observations where gender is NA (i.e. droids in this data set) we can see this density is very narrow with a large peak.\n\n\n2.3.6 Smooths\nIf we want to add a line of best fit to a plot, we can either define this line manually, perhaps from some values from a fitted model object, or allow ggplot to do this for us using one of many methods. This relies on ggplot calculating statistics for us based on our data and adding a line of best fit to the plot. To do this, we use the geom_smooth() function. The following methods are available to us:\n\n“lm”: the linear model fitted using stats::lm(), fitting a straight line between the values on the x and y axis to find the line of best fit.\n“loess”: locally estimated scatterplot smoothing fitted using stats::loess(), a nonparametric method to find the line of best fit in nonlinear data by fitting many local regressions within the series of data.\n“gam”: generalised additive model fitted using mgcv::gam(), a nonparametric method to find the line of best fit in nonlinear data by using a series of basis splines.\n\nFor our purposes, we’ll just rely on “lm” to draw a line of best fit through our data.\nNote, that we used scale_x_continuous() with a restricted range to remove Jabba the Hutt from our data set as an outlier. Remember, unlike coord_cartesian(), scale_x_continuous() removed the data point entirely, meaning our estimate of the line of best fit will be adjusted.\n\nggplot(data = starwars) +\n  scale_x_continuous(limits = c(0, 200)) +\n  geom_smooth(mapping = aes(x = mass, y = height), method = \"lm\")\n\n\n\n\nggplot2 will attempt to use a sensible default for the formula used to fit our model. Here, it set the formula to y ~ x as in the values on the y-axis are predicted by values on the x-axis. You can, however, specify an alternative formula by including a string to the formula argument. For example, if we wanted to predict the log of values on the y-axis, we might use formula = log(y) ~ x.\nIf none of this made sense, then don’t worry. We will turn to model fitting and estimation in later chapters."
  },
  {
    "objectID": "02_creating-graphs.html#combining-geoms",
    "href": "02_creating-graphs.html#combining-geoms",
    "title": "2  Creating Graphs",
    "section": "2.4 Combining Geoms",
    "text": "2.4 Combining Geoms\nWe can easily combine geoms in ggplot2 by simply adding another layer to our plot. Let’s create a dotplot and add the line of best fit to the data.\n\nggplot(data = starwars) +\n  scale_x_continuous(limits = c(0, 200)) +\n  geom_point(mapping = aes(x = mass, y = height)) +\n  geom_smooth(mapping = aes(x = mass, y = height), method = \"lm\")\n\n\n\n\nNote that there’s a bit of repetition here. We’ve specified the x and y elements in each geom, but they’re just the same. If this happens, we can specify them during the initial ggplot() call and allow these to be inherited by the geoms.\n\nggplot(data = starwars, mapping = aes(x = mass, y = height)) +\n  scale_x_continuous(limits = c(0, 200)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "02_creating-graphs.html#positioning",
    "href": "02_creating-graphs.html#positioning",
    "title": "2  Creating Graphs",
    "section": "2.5 Positioning",
    "text": "2.5 Positioning\nWhile ggplot2 has many sensible defaults for plotting data, often we want more granular control over the positioning of the elements of a plot. There are a few different positions we can specify. Most often, however, if you want to change the position of an element you want to jitter elements by some amount to avoid overplotting or stop bar plots from stacking and instead set them side by side. We’ll briefly explore each method here.\nOften, with scatter plots it’s a better idea to use opacity to help with overplotting as adding jitter clearly influences the interpretation of the plot. But sometimes this still doesn’t help. In these cases, adding some jitter (and making this clear to the reader) is appropriate. Compare the two plots below.\n\n2.5.1 Jitter\n\nggplot(data = starwars, mapping = aes(x = gender, y = height)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\nggplot(data = starwars, mapping = aes(x = gender, y = height)) +\n  geom_point(alpha = 0.5, position = \"jitter\")\n\n\n\n\n\n\n2.5.2 Dodge\nWith bar plots we often want to display more than just one variable. To do this, we may choose to pick out the bars on the x-axis in colour.\n\nggplot(data = starwars, mapping = aes(x = gender, fill = sex)) +\n  geom_bar()\n\n\n\n\nBy default, ggplot2 using the “identity” position, where bars are stacked on top of each other within categories. Sometimes this is what we want if we care about the breakdown within the groups on the x-axis. However, if we want to compare absolute values across all subgroups, we need to dodge the bars. We do this using the position = \"dodge\" argument.\n\nggplot(data = starwars, mapping = aes(x = gender, fill = sex), ) +\n  geom_bar(position = \"dodge\")"
  }
]